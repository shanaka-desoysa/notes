{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanaka-desoysa/notes/blob/main/docs/Python/Deep_Learning/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2FVlZ5yEmlS"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "Sentiment analysis is a natural language processing technique used to determine whether data is positive, negative or neutral. Sentiment analysis is often performed on textual data to help businesses monitor brand and product sentiment in customer feedback, and understand customer needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJwn3Z2dFR9t"
      },
      "source": [
        "## BERT Model\n",
        "- Split Data: Split your dataset into training and validation sets.\n",
        "- Training: Train the model on the training set.\n",
        "- Evaluation: Use the validation set to evaluate the model.\n",
        "Here’s how you can do it with the BERT model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRyar0EzNvo_"
      },
      "source": [
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split the data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    [d['text'] for d in data], [0, 1, 2, 3], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "# Convert to torch tensors\n",
        "train_dataset = torch.utils.data.Dataset(train_encodings, train_labels)\n",
        "val_dataset = torch.utils.data.Dataset(val_encodings, val_labels)\n",
        "\n",
        "# Train the model\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = trainer.predict(val_dataset)\n",
        "pred_labels = predictions.argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(val_labels, pred_labels, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM Model\n",
        "- Split Data: Split your dataset into training and validation sets.\n",
        "- Training: Train the model on the training set.\n",
        "- Evaluation: Use the validation set to evaluate the model.\n",
        "Here’s how you can do it with the LSTM model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split the data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    [d['text'] for d in data], [0, 1, 2, 3], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize and pad the data\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=50)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=50)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_padded, train_labels, epochs=5, batch_size=2, validation_data=(val_padded, val_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "pred_labels = model.predict(val_padded).argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(val_labels, pred_labels, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RNN Model\n",
        "- Split Data: Split your dataset into training and validation sets.\n",
        "- Training: Train the model on the training set.\n",
        "- Evaluation: Use the validation set to evaluate the model.\n",
        "Here’s how you can do it with the RNN model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split the data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    [d['text'] for d in data], [0, 1, 2, 3], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize and pad the data\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=50)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=50)\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_padded, train_labels, epochs=5, batch_size=2, validation_data=(val_padded, val_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "pred_labels = model.predict(val_padded).argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(val_labels, pred_labels, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPLb9/RLWIjyK1NN71uzITL",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Django_Pandas_DataFrame_To_jQuery_DataTable.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
