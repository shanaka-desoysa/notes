{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanaka-desoysa/notes/blob/main/docs/Python/Deep_Learning/Sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2FVlZ5yEmlS"
      },
      "source": [
        "# Sentiment Analysis\n",
        "\n",
        "Sentiment analysis is a natural language processing technique used to determine whether data is positive, negative or neutral. Sentiment analysis is often performed on textual data to help businesses monitor brand and product sentiment in customer feedback, and understand customer needs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Sample Data\n",
        "\n",
        "```python\n",
        "data = [\n",
        "    {\"text\": \"I loved working here, but I need to move to a new city.\", \"sentiment\": \"positive\"},\n",
        "    {\"text\": \"The work environment was toxic and stressful.\", \"sentiment\": \"negative\"},\n",
        "    {\"text\": \"It was an okay experience, nothing special.\", \"sentiment\": \"neutral\"},\n",
        "    {\"text\": \"I am unsure about my feelings towards this job.\", \"sentiment\": \"ambiguous\"}\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJwn3Z2dFR9t"
      },
      "source": [
        "## BERT Model\n",
        "- Split Data: Split your dataset into training and validation sets.\n",
        "- Training: Train the model on the training set.\n",
        "- Evaluation: Use the validation set to evaluate the model.\n",
        "Here’s how you can do it with the BERT model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRyar0EzNvo_"
      },
      "source": [
        "```python\n",
        "from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load pre-trained BERT tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n",
        "\n",
        "# Split the data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    [d['text'] for d in data], [0, 1, 2, 3], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize the data\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n",
        "\n",
        "# Convert to torch tensors\n",
        "class Dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
        "        item['labels'] = torch.tensor(self.labels[idx])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "train_dataset = Dataset(train_encodings, train_labels)\n",
        "val_dataset = Dataset(val_encodings, val_labels)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(output_dir='./results', num_train_epochs=3, per_device_train_batch_size=2)\n",
        "\n",
        "# Create Trainer instance\n",
        "trainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Evaluate the model\n",
        "predictions = trainer.predict(val_dataset)\n",
        "pred_labels = predictions.predictions.argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(val_labels, pred_labels, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LSTM Model\n",
        "- Split Data: Split your dataset into training and validation sets.\n",
        "- Training: Train the model on the training set.\n",
        "- Evaluation: Use the validation set to evaluate the model.\n",
        "Here’s how you can do it with the LSTM model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Tokenize the data\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts([d['text'] for d in data])\n",
        "\n",
        "# Split the data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    [d['text'] for d in data], [0, 1, 2, 3], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize and pad the data\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=50)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=50)\n",
        "\n",
        "# Define the LSTM model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=1000, output_dim=64, input_length=50))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_padded, train_labels, epochs=5, batch_size=2, validation_data=(val_padded, val_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "pred_labels = model.predict(val_padded).argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(val_labels, pred_labels, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RNN Model\n",
        "- Split Data: Split your dataset into training and validation sets.\n",
        "- Training: Train the model on the training set.\n",
        "- Evaluation: Use the validation set to evaluate the model.\n",
        "Here’s how you can do it with the RNN model:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, SimpleRNN, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Tokenize the data\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts([d['text'] for d in data])\n",
        "\n",
        "# Split the data\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    [d['text'] for d in data], [0, 1, 2, 3], test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Tokenize and pad the data\n",
        "train_sequences = tokenizer.texts_to_sequences(train_texts)\n",
        "val_sequences = tokenizer.texts_to_sequences(val_texts)\n",
        "train_padded = pad_sequences(train_sequences, maxlen=50)\n",
        "val_padded = pad_sequences(val_sequences, maxlen=50)\n",
        "\n",
        "# Define the RNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=1000, output_dim=64, input_length=50))\n",
        "model.add(SimpleRNN(64))\n",
        "model.add(Dense(4, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_padded, train_labels, epochs=5, batch_size=2, validation_data=(val_padded, val_labels))\n",
        "\n",
        "# Evaluate the model\n",
        "pred_labels = model.predict(val_padded).argmax(axis=1)\n",
        "\n",
        "# Print classification report\n",
        "print(classification_report(val_labels, pred_labels, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Traditional Machine Learning Models: Logistic Regression, Naive Bayes, Support Vector Machines (SVM)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "data = [\n",
        "    {\"text\": \"I loved working here, but I need to move to a new city.\", \"sentiment\": \"positive\"},\n",
        "    {\"text\": \"The work environment was toxic and stressful.\", \"sentiment\": \"negative\"},\n",
        "    {\"text\": \"It was an okay experience, nothing special.\", \"sentiment\": \"neutral\"},\n",
        "    {\"text\": \"I am unsure about my feelings towards this job.\", \"sentiment\": \"ambiguous\"}\n",
        "]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocessing\n",
        "First, we need to preprocess the text data and convert it into a format suitable for machine learning models.\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Convert data to DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Encode labels\n",
        "label_mapping = {'positive': 0, 'negative': 1, 'neutral': 2, 'ambiguous': 3}\n",
        "df['label'] = df['sentiment'].map(label_mapping)\n",
        "\n",
        "# Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "# Vectorize text data\n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_val_vec = vectorizer.transform(X_val)\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression\n",
        "Logistic regression is a linear model used for binary classification. It can be extended to multi-class classification using the one-vs-rest approach.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Train the model\n",
        "log_reg = LogisticRegression()\n",
        "log_reg.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = log_reg.predict(X_val_vec)\n",
        "print(\"Logistic Regression Classification Report:\")\n",
        "print(classification_report(y_val, y_pred, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Naive Bayes\n",
        "Naive Bayes is a probabilistic classifier based on Bayes' theorem with strong independence assumptions between features.\n",
        "\n",
        "```python\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "# Train the model\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = nb.predict(X_val_vec)\n",
        "print(\"Naive Bayes Classification Report:\")\n",
        "print(classification_report(y_val, y_pred, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Support Vector Machine (SVM)\n",
        "Support Vector Machines (SVM) are supervised learning models used for classification tasks. They find the hyperplane that best separates the classes in the feature space.\n",
        "\n",
        "```python\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Train the model\n",
        "svm = SVC(kernel='linear')\n",
        "svm.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = svm.predict(X_val_vec)\n",
        "print(\"SVM Classification Report:\")\n",
        "print(classification_report(y_val, y_pred, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPLb9/RLWIjyK1NN71uzITL",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Sentiment_analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
