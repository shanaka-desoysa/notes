{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"My Notes","text":"<p>Notes for my future self.</p> <p>\u00a9 2022-2026 Shanaka C. DeSoysa</p>"},{"location":"AWS/EC2_simple_web_server/","title":"Run a Simple Web Server","text":"<p>Use following script in the User Data section of the EC2 instance configuration.</p> <pre><code>#!/bin/bash\n# Use this for your user data (script from top to bottom)\n# install httpd (Linux 2 version)\nyum update -y\nyum install -y httpd\nsystemctl start httpd\nsystemctl enable httpd\necho \"&lt;h1&gt;Hello World from $(hostname -f)&lt;/h1&gt;\" &gt; /var/www/html/index.html\n</code></pre>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/","title":"Blockchain Explained in 7 Simple Functions","text":"<pre><code>import hashlib\nimport json\n\ndef hash_function(k):\n    \"\"\"Hashes our transaction.\"\"\"\n    if type(k) is not str:\n        k = json.dumps(k, sort_keys=True)\n\n    return hashlib.sha256(k.encode('utf-8')).hexdigest()\n</code></pre> <pre><code>hash_function('www.geni.ai')\n</code></pre> <pre>\n<code>'8bfae4a2d420bce8036bd69ed798765a00e103901b7389386373315d7506143f'</code>\n</pre> <pre><code>def update_state(transaction, state):\n    state = state.copy()\n\n    for key in transaction:\n        if key in state.keys():\n            state[key] += transaction[key]\n        else:\n            state[key] = transaction[key]\n\n    return state\n</code></pre> <pre><code>def valid_transaction(transaction, state):\n    \"\"\"A valid transaction must sum to 0.\"\"\"\n    if sum(transaction.values()) is not 0:\n        return False\n\n    for key in transaction.keys():\n        if key in state.keys():\n            account_balance = state[key]\n        else:\n            account_balance = 0\n\n        if account_balance + transaction[key] &amp;lt; 0:\n            return False\n\n    return True\n</code></pre> <pre><code>def make_block(transactions, chain):\n    \"\"\"Make a block to go into the chain.\"\"\"\n    parent_hash = chain[-1]['hash']\n    block_number = chain[-1]['contents']['block_number'] + 1\n\n    block_contents = {\n        'block_number': block_number,\n        'parent_hash': parent_hash,\n        'transaction_count': block_number + 1,\n        'transaction': transactions\n    }\n\n    return {'hash': hash_function(block_contents), 'contents': block_contents}\n</code></pre> <pre><code>def check_block_hash(block):\n    expected_hash = hash_function(block['contents'])\n\n    if block['hash'] is not expected_hash:\n        raise\n\n    return\n</code></pre> <pre><code>def check_block_validity(block, parent, state):\n    parent_number = parent['contents']['block_number']\n    parent_hash = parent['hash']\n    block_number = block['contents']['block_number']\n\n    for transaction in block['contents']['transaction']:\n        if valid_transaction(transaction, state):\n            state = update_state(transaction, state)\n        else:\n            raise\n\n    check_block_hash(block)  # Check hash integrity\n\n    if block_number is not parent_number + 1:\n        raise\n\n    if block['contents']['parent_hash'] is not parent_hash:\n        raise\n</code></pre> <pre><code>def check_chain(chain):\n    \"\"\"Check the chain is valid.\"\"\"\n    if type(chain) is str:\n        try:\n            chain = json.loads(chain)\n            assert (type(chain) == list)\n        except ValueError:\n            # String passed in was not valid JSON\n            return False\n    elif type(chain) is not list:\n        return False\n\n    state = {}\n\n    for transaction in chain[0]['contents']['transaction']:\n        state = update_state(transaction, state)\n\n    check_block_hash(chain[0])\n    parent = chain[0]\n\n    for block in chain[1:]:\n        state = check_block_validity(block, parent, state)\n        parent = block\n\n    return state\n</code></pre> <pre><code>def add_transaction_to_chain(transaction, state, chain):\n    if valid_transaction(transaction, state):\n        state = update_state(transaction, state)\n    else:\n        raise Exception('Invalid transaction.')\n\n    my_block = make_block(state, chain)\n    chain.append(my_block)\n\n    for transaction in chain:\n        check_chain(transaction)\n\n    return state, chain\n</code></pre> <pre><code>genesis_block = {\n    'hash': hash_function({\n        'block_number': 0,\n        'parent_hash': None,\n        'transaction_count': 1,\n        'transaction': [{'Geni AI': 100}]\n    }),\n    'contents': {\n        'block_number': 0,\n        'parent_hash': None,\n        'transaction_count': 1,\n        'transaction': [{'Geni AI': 100}]\n    },\n}\n\nblock_chain = [genesis_block]\nchain_state = {'Geni AI': 100}\n</code></pre> <p>Now, look what happens when Geni AI give some coins to user John Smith:</p> <pre><code>chain_state, block_chain = add_transaction_to_chain(transaction={'Geni AI': -5, 'John Smith': 5}, state=chain_state, chain=block_chain)\n</code></pre> <pre><code>chain_state\n</code></pre> <pre>\n<code>{'Geni AI': 95, 'John Smith': 5}</code>\n</pre> <pre><code>block_chain\n</code></pre> <pre>\n<code>[{'contents': {'block_number': 0,\n   'parent_hash': None,\n   'transaction': [{'Geni AI': 100}],\n   'transaction_count': 1},\n  'hash': 'e46fb93e96b70a86e6998cd4ba9f20cdbde1e843e5b2343667f0f46d23cee439'},\n {'contents': {'block_number': 1,\n   'parent_hash': 'e46fb93e96b70a86e6998cd4ba9f20cdbde1e843e5b2343667f0f46d23cee439',\n   'transaction': {'Geni AI': 95, 'John Smith': 5},\n   'transaction_count': 2},\n  'hash': '65af8f82cd4e55e4db67280eb1cd8c03216ea829ca533135f684524018961c6b'}]</code>\n</pre> <p>Our first new transaction has been created and inserted to the top of the stack. </p>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#blockchain-explained-in-7-simple-functions","title":"Blockchain Explained in 7 Simple Functions","text":"<p>Practical hands-on guide to implement your own blockchain with 7 simple Python functions.</p>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#hashing-function","title":"Hashing Function","text":"<p>At the heart of the blockchain is the hashing function. Without encryption, the blockchain will be easily manipulable and transactions will be able to be fraudulently inserted. Here we're using a simple MD5 hashing algorithm. If you're interested in what's actually being used in bitcoin, read here.</p>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#state-management","title":"State Management","text":"<p>The \u2018state\u2019 is the record of who owns what. For example, Geni AI have 100 coins and give 5 to John Smith, then the state will be the value of the dictionary below.</p> <p><code>{'transaction': {'Geni AI': 95, 'John Smith': 5}}</code></p>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#transaction-validation","title":"Transaction Validation","text":"<p>The important thing to note is that overdrafts cannot exist. If there are only 10 coins in existence, then I cannot give 11 coins to someone. The below function verifies that the transaction we attempt to make is indeed valid. Also, a transaction must balance. I cannot give 5 coins and have the recipient receive 4 coins, since that would allow the destruction and creation of coins.</p>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#make-block","title":"Make Block","text":"<p>Now, we can make our block. The information from the previous block is read, and used to link it to the new block. This, too, is central to the idea of blockchain. Seemingly valid transactions can be attempted to fraudulently be inserted into the blockchain, but decrypting all the previous blocks is computationally (nearly) impossible, which preserves the integrity of the blockchain.</p>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#check-block-hash","title":"Check Block Hash","text":"<p>Below is a small helper function to check the hash of the previous block:</p>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#block-validity","title":"Block Validity","text":"<p>Once we have assembled everything together, its time to create our block. We will now update the blockchain.</p>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#check-blockchain","title":"Check Blockchain","text":"<p>Before we are finished, the chain must be verified:</p>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#add-transaction","title":"Add transaction","text":"<p>Finally, need a transaction function, which hangs all of the above together:</p>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#example","title":"Example","text":"<p>So, now we have our 7 functions. How do we interact with it? Well, first we need to start our chain with a Genesis Block. This is the inception of our new coin (or stock inventory, etc). </p> <p>For the purposes of this article, I will say that I, Tom, will start off with 10 coins. Let's say we start off with 100 coins for Geni AI.</p>"},{"location":"BlockChain/Blockchain_Explained_in_7_Simple_Functions/#references","title":"References","text":"<p>https://towardsdatascience.com/blockchain-explained-in-7-python-functions-c49c84f34ba5</p>"},{"location":"Copilot_Studio/EX_Analyzer/","title":"Instructions","text":""},{"location":"Copilot_Studio/EX_Analyzer/#copilot-studio-agent-prompt-employee-survey-comments-analysis","title":"\ud83d\udd0d Copilot Studio Agent Prompt: Employee Survey Comments Analysis","text":"<p>Objective: Create an AI agent that analyzes employee survey comments using a predefined taxonomy of employee experience topics and classifies each comment by topic, sub-topic, and sentiment.</p>"},{"location":"Copilot_Studio/EX_Analyzer/#agent-capabilities","title":"\ud83e\udde0 Agent Capabilities","text":"<ol> <li>Taxonomy Integration </li> <li>Access a predefined taxonomy of employee experience topics and sub-topics stored in a SharePoint location.  </li> <li> <p>Use this taxonomy to guide topic classification.</p> </li> <li> <p>Data Input </p> </li> <li>Retrieve employee survey comments from Excel files stored in SharePoint.  </li> <li> <p>Supported survey types include exit surveys, annual surveys, quarterly pulse surveys, etc.</p> </li> <li> <p>Comment Analysis </p> </li> <li> <p>For each comment:</p> <ul> <li>Identify all relevant topics and sub-topics discussed.</li> <li>Determine sentiment (positive, neutral, negative).</li> <li>Handle multi-topic and multi-sentiment comments appropriately.</li> </ul> </li> <li> <p>Output Generation </p> </li> <li>Save the analysis results in a new Excel file.  </li> <li>Include columns for:<ul> <li>Original comment</li> <li>Identified topic(s)</li> <li>Sub-topic(s)</li> <li>Sentiment</li> <li>Any additional metadata (e.g., survey type, date, employee ID if available)</li> </ul> </li> </ol>"},{"location":"Copilot_Studio/EX_Analyzer/#inputs-required","title":"\ud83d\udcc1 Inputs Required","text":"<ul> <li>SharePoint link to the taxonomy file (Excel or structured document)</li> <li>SharePoint link to the survey comments Excel file(s)</li> </ul>"},{"location":"Copilot_Studio/EX_Analyzer/#expected-output-format","title":"\u2705 Expected Output Format","text":"<p>An Excel file with structured rows like:</p> Comment Topic Sub-topic Sentiment Survey Type Date \"I felt unsupported by my manager.\" Management Support Negative Exit Survey 2025-06-01"},{"location":"Copilot_Studio/EX_Analyzer/#example-taxonomy","title":"Example Taxonomy","text":""},{"location":"Copilot_Studio/EX_Analyzer/#example-employee-experience-topics-taxonomy","title":"\ud83d\udcda Example: Employee Experience Topics Taxonomy","text":"Topic Sub-topic Leadership Vision &amp; Strategy Trust in Leadership Communication from Leadership Ethical Behavior Decision-Making Manager Effectiveness Feedback &amp; Recognition Support &amp; Coaching Fairness Communication Work Environment Physical Workspace Tools &amp; Technology Safety Remote/Hybrid Work Career Development Learning &amp; Training Opportunities Career Path Clarity Internal Mobility Mentorship Compensation &amp; Benefits Salary &amp; Pay Equity Health &amp; Wellness Benefits Retirement &amp; Financial Benefits Time Off &amp; Leave Policies Team &amp; Collaboration Team Dynamics Cross-functional Collaboration Inclusion in Decision-Making Diversity, Equity &amp; Inclusion (DEI) Belonging Representation Inclusive Culture Bias &amp; Discrimination Work-Life Balance Flexibility Workload Burnout Organizational Culture Values Alignment Trust &amp; Transparency Innovation &amp; Risk-Taking Change Management Employee Engagement Motivation Pride in Work Connection to Mission Recognition"},{"location":"Copilot_Studio/EX_Analyzer/#testing-agent","title":"Testing Agent","text":""},{"location":"Cyber_Security/ARP_Spoofing/","title":"Kali Linux ARP Spoofing","text":"<p>In computer networking, ARP spoofing, ARP cache poisoning, or ARP poison routing, is a technique by which an attacker sends (spoofed) Address Resolution Protocol (ARP) messages onto a local area network.</p>"},{"location":"Cyber_Security/ARP_Spoofing/#arp","title":"ARP","text":"<p>The Address Resolution Protocol) (ARP) is a communication protocol used for discovering the link layer address, such as a MAC address, associated with a given internet layer address, typically an IPv4 address. </p> <p>Check ARP on Linux/Mac/Win.</p> <pre><code>arp -a\n</code></pre>"},{"location":"Cyber_Security/ARP_Spoofing/#find-default-gateway","title":"Find Default Gateway","text":"<p>Check your default gateway: <pre><code>ip route\n</code></pre></p>"},{"location":"Cyber_Security/ARP_Spoofing/#scan-the-network","title":"Scan the Network","text":"<p>-r is for range</p> <pre><code>netdiscover -r 10.0.2.0/24\n</code></pre>"},{"location":"Cyber_Security/ARP_Spoofing/#arp-spoofing","title":"ARP Spoofing","text":"<pre><code>arpspoof -i INTERFACE -t VICTIM_IP GATEWAY_IP\n\narpspoof -i INTERFACE -t GATEWAY_IP VICTIM_IP\n</code></pre> <pre><code>arpspoof -i eth0 -t 10.0.2.15 10.0.2.1\n</code></pre> <pre><code>arpspoof -i eth0 -t 10.0.2.1 10.0.2.15\n</code></pre> <p>Make sure port forwarding is enabled, as described below.</p>"},{"location":"Cyber_Security/ARP_Spoofing/#check-port-forwarding","title":"Check port forwarding","text":"<pre><code>sysctl net.ipv4.ip_forward\n</code></pre>"},{"location":"Cyber_Security/ARP_Spoofing/#enable-port-forwarding","title":"Enable port forwarding","text":"<pre><code>sysctl -w net.ipv4.ip_forward=1\n</code></pre> <p>OR, if that doesn't work:</p> <pre><code>echo 1 &gt; /proc/sys/net/ipv4/ip_forward\n</code></pre>"},{"location":"Cyber_Security/Monitor_cyber_threats_using_EC2/","title":"Kali Linux ARP Spoofing","text":"<p>In computer networking, ARP spoofing, ARP cache poisoning, or ARP poison routing, is a technique by which an attacker sends (spoofed) Address Resolution Protocol (ARP) messages onto a local area network.</p>"},{"location":"Cyber_Security/Monitor_cyber_threats_using_EC2/#arp","title":"ARP","text":"<p>The Address Resolution Protocol) (ARP) is a communication protocol used for discovering the link layer address, such as a MAC address, associated with a given internet layer address, typically an IPv4 address. </p> <p>Check ARP on Linux/Mac/Win.</p> <pre><code>arp -a\n</code></pre>"},{"location":"Cyber_Security/Monitor_cyber_threats_using_EC2/#find-default-gateway","title":"Find Default Gateway","text":"<p>Check your default gateway: <pre><code>ip route\n</code></pre></p>"},{"location":"Cyber_Security/Monitor_cyber_threats_using_EC2/#scan-the-network","title":"Scan the Network","text":"<p>-r is for range</p> <pre><code>netdiscover -r 10.0.2.0/24\n</code></pre>"},{"location":"Cyber_Security/Monitor_cyber_threats_using_EC2/#arp-spoofing","title":"ARP Spoofing","text":"<pre><code>arpspoof -i INTERFACE -t VICTIM_IP GATEWAY_IP\n\narpspoof -i INTERFACE -t GATEWAY_IP VICTIM_IP\n</code></pre> <pre><code>arpspoof -i eth0 -t 10.0.2.15 10.0.2.1\n</code></pre> <pre><code>arpspoof -i eth0 -t 10.0.2.1 10.0.2.15\n</code></pre> <p>Make sure port forwarding is enabled, as described below.</p>"},{"location":"Cyber_Security/Monitor_cyber_threats_using_EC2/#check-port-forwarding","title":"Check port forwarding","text":"<pre><code>sysctl net.ipv4.ip_forward\n</code></pre>"},{"location":"Cyber_Security/Monitor_cyber_threats_using_EC2/#enable-port-forwarding","title":"Enable port forwarding","text":"<pre><code>sysctl -w net.ipv4.ip_forward=1\n</code></pre> <p>OR, if that doesn't work:</p> <pre><code>echo 1 &gt; /proc/sys/net/ipv4/ip_forward\n</code></pre>"},{"location":"Docker/Copy_and_list_files_in_named_volume/","title":"Create a volume, copy files and list files in Docker Named Volume","text":""},{"location":"Docker/Copy_and_list_files_in_named_volume/#create-a-container-with-volume","title":"Create a container with volume","text":"<pre><code>docker container create --name test_container -v test_vol:/test-data busybox;\n</code></pre>"},{"location":"Docker/Copy_and_list_files_in_named_volume/#copy-files","title":"Copy files","text":"<pre><code>echo \"Hello World\" &gt; test.txt;\ndocker cp ./test.txt test_container:/test-data;\ndocker rm test_container;\n</code></pre>"},{"location":"Docker/Copy_and_list_files_in_named_volume/#list-files","title":"List files","text":"<pre><code>docker run -it --rm -v test_vol:/vol busybox ls -l /vol\n</code></pre>"},{"location":"Docker/Copy_and_list_files_in_named_volume/#show-content-of-file","title":"Show content of file","text":"<pre><code>docker run -it --rm -v test_vol:/vol busybox cat /vol/test.txt\n</code></pre>"},{"location":"Docker/Copy_and_list_files_in_named_volume/#remove-docker-volume","title":"Remove docker volume","text":"<pre><code>docker volume rm test_vol\n</code></pre>"},{"location":"Docker/Makefile_for_docker-compose/","title":"Makefile for docker-compose","text":"<p>Makefile for basic docker-compose commands.</p> <pre><code>## Project Name\n##\n##   Parameters\n##-------------------------|-----------------------------------------------------------------\n## COMPOSE_FILE            | Name of compose file (docker-compose.yml)\n## BUILD_CACHE             | Docker build cache (or --no-cache)\n\nCOMPOSE_FILE := docker-compose.yml\nBUILD_CACHE :=\n#\"--no-cache\"\n\nTHIS_FILE := $(lastword $(MAKEFILE_LIST))\nCMD_ARGUMENTS ?= $(cmd)\n\n.PHONY: help build up start down destroy stop restart logs logs-web ps\n\n##   Commands\n##-------------------------|-----------------------------------------------------------------\n## help                    | print docs (make help) or dry run a command (make help cmd=run)\n\nhelp : Makefile\nifeq ($(CMD_ARGUMENTS),)\n    @sed -n 's/^##//p' $&lt; | less\nelse\n    ${MAKE} $(CMD_ARGUMENTS) --dry-run\nendif\n\n## build                   | docker-compose build\nbuild:\n        docker-compose -f $(COMPOSE_FILE) build $(c)\n\n## up                      | docker-compose up\nup:\n        docker-compose -f $(COMPOSE_FILE) up -d $(c)\n\n## start                   | docker-compose start\nstart:\n        docker-compose -f $(COMPOSE_FILE) start $(c)\n\n## down                    | docker-compose down\ndown:\n        docker-compose -f $(COMPOSE_FILE) down $(c)\n\n## destroy                 | docker-compose down and remove volumes\ndestroy:\n        docker-compose -f $(COMPOSE_FILE) down -v $(c)\n\n## stop                    | docker-compose stop\nstop:\n        docker-compose -f $(COMPOSE_FILE) stop $(c)\n\n## restart                 | docker-compose stop and up\nrestart:\n        docker-compose -f $(COMPOSE_FILE) stop $(c)\n        docker-compose -f $(COMPOSE_FILE) up -d $(c)\n\n## logs                    | docker-compose logs\nlogs:\n        docker-compose -f $(COMPOSE_FILE) logs --tail=100 -f $(c)\n\n## ps                      | docker-compose ps\nps:\n        docker-compose -f $(COMPOSE_FILE) ps\n</code></pre>"},{"location":"Docker/OpenAPI_Editor_and_UI/","title":"Swagger (OpenAPI) UI and Editor","text":"<p>Run following commands to start Swagger UI and Editor with Docker.</p>"},{"location":"Docker/OpenAPI_Editor_and_UI/#swagger-editor","title":"Swagger Editor","text":"<pre><code>docker run -d -p 8081:8080 -v ${PWD}/docs:/docs -e SWAGGER_FILE=/docs/swagger.json swaggerapi/swagger-editor\n</code></pre>"},{"location":"Docker/OpenAPI_Editor_and_UI/#swagger-ui-viewer","title":"Swagger UI Viewer","text":"<pre><code>docker run -d -p 8082:8080 -e SWAGGER_JSON=/docs/swagger.json -v ${PWD}/docs:/docs swaggerapi/swagger-ui\n</code></pre>"},{"location":"Docker/Setup_GitHub_Container_Registry_Access/","title":"Setup GitHub Container Registry Access","text":"<p>Generate a GitHub Token with <code>read:packages</code> scope.</p> <p>Run command to login to container registry.</p> <pre><code>docker login -u {USER} -p {TOKEN} docker.pkg.github.com\n</code></pre> <p>Then you can access the images as follows:</p> <pre><code>docker pull docker.pkg.github.com/OWNER/REPOSITORY/IMAGE_NAME:TAG_NAME\n</code></pre>"},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds/","title":"Calculate Composite Rate for Series I Bonds","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom pandas_datareader.fred import FredReader\n</code></pre> <pre><code>end_date = pd.to_datetime('today').date()\nstart_date = (end_date + pd.DateOffset(years=-5)).date()\n\nprint(f'start_date: {start_date} to end_date: {end_date}')\n</code></pre> <pre>\n<code>start_date: 2017-04-23 to end_date: 2022-04-23\n</code>\n</pre> <pre><code>df = FredReader('CPIAUCNS', start=start_date).read()\n</code></pre> <pre><code>df\n</code></pre> CPIAUCNS DATE 2017-05-01 244.733 2017-06-01 244.955 2017-07-01 244.786 2017-08-01 245.519 2017-09-01 246.819 2017-10-01 246.663 2017-11-01 246.669 2017-12-01 246.524 2018-01-01 247.867 2018-02-01 248.991 2018-03-01 249.554 2018-04-01 250.546 2018-05-01 251.588 2018-06-01 251.989 2018-07-01 252.006 2018-08-01 252.146 2018-09-01 252.439 2018-10-01 252.885 2018-11-01 252.038 2018-12-01 251.233 2019-01-01 251.712 2019-02-01 252.776 2019-03-01 254.202 2019-04-01 255.548 2019-05-01 256.092 2019-06-01 256.143 2019-07-01 256.571 2019-08-01 256.558 2019-09-01 256.759 2019-10-01 257.346 2019-11-01 257.208 2019-12-01 256.974 2020-01-01 257.971 2020-02-01 258.678 2020-03-01 258.115 2020-04-01 256.389 2020-05-01 256.394 2020-06-01 257.797 2020-07-01 259.101 2020-08-01 259.918 2020-09-01 260.280 2020-10-01 260.388 2020-11-01 260.229 2020-12-01 260.474 2021-01-01 261.582 2021-02-01 263.014 2021-03-01 264.877 2021-04-01 267.054 2021-05-01 269.195 2021-06-01 271.696 2021-07-01 273.003 2021-08-01 273.567 2021-09-01 274.310 2021-10-01 276.589 2021-11-01 277.948 2021-12-01 278.802 2022-01-01 281.148 2022-02-01 283.716 2022-03-01 287.504 <pre><code>df.plot(title='CPI - CPIAUCNS', figsize=(15,10));\n</code></pre> <pre><code>df['CPIAUCNS-1m'] = df['CPIAUCNS'].shift(periods=1)\ndf['CPIAUCNS-6m'] = df['CPIAUCNS'].shift(periods=6)\ndf['CPIAUCNS-12m'] = df['CPIAUCNS'].shift(periods=12)\ndf\n</code></pre> CPIAUCNS CPIAUCNS-1m CPIAUCNS-6m CPIAUCNS-12m DATE 2017-05-01 244.733 NaN NaN NaN 2017-06-01 244.955 244.733 NaN NaN 2017-07-01 244.786 244.955 NaN NaN 2017-08-01 245.519 244.786 NaN NaN 2017-09-01 246.819 245.519 NaN NaN 2017-10-01 246.663 246.819 NaN NaN 2017-11-01 246.669 246.663 244.733 NaN 2017-12-01 246.524 246.669 244.955 NaN 2018-01-01 247.867 246.524 244.786 NaN 2018-02-01 248.991 247.867 245.519 NaN 2018-03-01 249.554 248.991 246.819 NaN 2018-04-01 250.546 249.554 246.663 NaN 2018-05-01 251.588 250.546 246.669 244.733 2018-06-01 251.989 251.588 246.524 244.955 2018-07-01 252.006 251.989 247.867 244.786 2018-08-01 252.146 252.006 248.991 245.519 2018-09-01 252.439 252.146 249.554 246.819 2018-10-01 252.885 252.439 250.546 246.663 2018-11-01 252.038 252.885 251.588 246.669 2018-12-01 251.233 252.038 251.989 246.524 2019-01-01 251.712 251.233 252.006 247.867 2019-02-01 252.776 251.712 252.146 248.991 2019-03-01 254.202 252.776 252.439 249.554 2019-04-01 255.548 254.202 252.885 250.546 2019-05-01 256.092 255.548 252.038 251.588 2019-06-01 256.143 256.092 251.233 251.989 2019-07-01 256.571 256.143 251.712 252.006 2019-08-01 256.558 256.571 252.776 252.146 2019-09-01 256.759 256.558 254.202 252.439 2019-10-01 257.346 256.759 255.548 252.885 2019-11-01 257.208 257.346 256.092 252.038 2019-12-01 256.974 257.208 256.143 251.233 2020-01-01 257.971 256.974 256.571 251.712 2020-02-01 258.678 257.971 256.558 252.776 2020-03-01 258.115 258.678 256.759 254.202 2020-04-01 256.389 258.115 257.346 255.548 2020-05-01 256.394 256.389 257.208 256.092 2020-06-01 257.797 256.394 256.974 256.143 2020-07-01 259.101 257.797 257.971 256.571 2020-08-01 259.918 259.101 258.678 256.558 2020-09-01 260.280 259.918 258.115 256.759 2020-10-01 260.388 260.280 256.389 257.346 2020-11-01 260.229 260.388 256.394 257.208 2020-12-01 260.474 260.229 257.797 256.974 2021-01-01 261.582 260.474 259.101 257.971 2021-02-01 263.014 261.582 259.918 258.678 2021-03-01 264.877 263.014 260.280 258.115 2021-04-01 267.054 264.877 260.388 256.389 2021-05-01 269.195 267.054 260.229 256.394 2021-06-01 271.696 269.195 260.474 257.797 2021-07-01 273.003 271.696 261.582 259.101 2021-08-01 273.567 273.003 263.014 259.918 2021-09-01 274.310 273.567 264.877 260.280 2021-10-01 276.589 274.310 267.054 260.388 2021-11-01 277.948 276.589 269.195 260.229 2021-12-01 278.802 277.948 271.696 260.474 2022-01-01 281.148 278.802 273.003 261.582 2022-02-01 283.716 281.148 273.567 263.014 2022-03-01 287.504 283.716 274.310 264.877 <p>Composite Rate is 6 months percentage change multiplied by 2 (annualized).</p> <pre><code>df['percent_change_1m'] = (df['CPIAUCNS'] - df['CPIAUCNS-1m'])*100 / df['CPIAUCNS-1m']\ndf['percent_change_6m'] = (df['CPIAUCNS'] - df['CPIAUCNS-6m'])*100 / df['CPIAUCNS-6m']\ndf['percent_change_12m'] = (df['CPIAUCNS'] - df['CPIAUCNS-12m'])*100 / df['CPIAUCNS-12m']\n\n# Composite Rate\ndf['percent_change_6m_annualized'] = df['percent_change_6m'] *2\ndf\n</code></pre> CPIAUCNS CPIAUCNS-1m CPIAUCNS-6m CPIAUCNS-12m percent_change_1m percent_change_6m percent_change_12m percent_change_6m_annualized DATE 2017-05-01 244.733 NaN NaN NaN NaN NaN NaN NaN 2017-06-01 244.955 244.733 NaN NaN 0.090711 NaN NaN NaN 2017-07-01 244.786 244.955 NaN NaN -0.068992 NaN NaN NaN 2017-08-01 245.519 244.786 NaN NaN 0.299445 NaN NaN NaN 2017-09-01 246.819 245.519 NaN NaN 0.529491 NaN NaN NaN 2017-10-01 246.663 246.819 NaN NaN -0.063204 NaN NaN NaN 2017-11-01 246.669 246.663 244.733 NaN 0.002432 0.791066 NaN 1.582132 2017-12-01 246.524 246.669 244.955 NaN -0.058783 0.640526 NaN 1.281052 2018-01-01 247.867 246.524 244.786 NaN 0.544775 1.258650 NaN 2.517301 2018-02-01 248.991 247.867 245.519 NaN 0.453469 1.414147 NaN 2.828294 2018-03-01 249.554 248.991 246.819 NaN 0.226113 1.108099 NaN 2.216199 2018-04-01 250.546 249.554 246.663 NaN 0.397509 1.574213 NaN 3.148425 2018-05-01 251.588 250.546 246.669 244.733 0.415892 1.994170 2.801012 3.988341 2018-06-01 251.989 251.588 246.524 244.955 0.159388 2.216823 2.871548 4.433645 2018-07-01 252.006 251.989 247.867 244.786 0.006746 1.669847 2.949515 3.339694 2018-08-01 252.146 252.006 248.991 245.519 0.055554 1.267114 2.699180 2.534228 2018-09-01 252.439 252.146 249.554 246.819 0.116203 1.156062 2.276972 2.312125 2018-10-01 252.885 252.439 250.546 246.663 0.176676 0.933561 2.522470 1.867122 2018-11-01 252.038 252.885 251.588 246.669 -0.334935 0.178864 2.176601 0.357728 2018-12-01 251.233 252.038 251.989 246.524 -0.319396 -0.300013 1.910159 -0.600026 2019-01-01 251.712 251.233 252.006 247.867 0.190660 -0.116664 1.551235 -0.233328 2019-02-01 252.776 251.712 252.146 248.991 0.422705 0.249855 1.520135 0.499710 2019-03-01 254.202 252.776 252.439 249.554 0.564136 0.698387 1.862523 1.396773 2019-04-01 255.548 254.202 252.885 250.546 0.529500 1.053048 1.996440 2.106096 2019-05-01 256.092 255.548 252.038 251.588 0.212876 1.608488 1.790228 3.216975 2019-06-01 256.143 256.092 251.233 251.989 0.019915 1.954361 1.648485 3.908722 2019-07-01 256.571 256.143 251.712 252.006 0.167094 1.930381 1.811465 3.860762 2019-08-01 256.558 256.571 252.776 252.146 -0.005067 1.496186 1.749780 2.992373 2019-09-01 256.759 256.558 254.202 252.439 0.078345 1.005893 1.711305 2.011786 2019-10-01 257.346 256.759 255.548 252.885 0.228619 0.703586 1.764043 1.407172 2019-11-01 257.208 257.346 256.092 252.038 -0.053624 0.435781 2.051278 0.871562 2019-12-01 256.974 257.208 256.143 251.233 -0.090977 0.324428 2.285130 0.648856 2020-01-01 257.971 256.974 256.571 251.712 0.387977 0.545658 2.486572 1.091316 2020-02-01 258.678 257.971 256.558 252.776 0.274062 0.826324 2.334874 1.652648 2020-03-01 258.115 258.678 256.759 254.202 -0.217645 0.528122 1.539327 1.056243 2020-04-01 256.389 258.115 257.346 255.548 -0.668694 -0.371873 0.329097 -0.743746 2020-05-01 256.394 256.389 257.208 256.092 0.001950 -0.316475 0.117926 -0.632951 2020-06-01 257.797 256.394 256.974 256.143 0.547205 0.320266 0.645733 0.640532 2020-07-01 259.101 257.797 257.971 256.571 0.505824 0.438034 0.986082 0.876067 2020-08-01 259.918 259.101 258.678 256.558 0.315321 0.479360 1.309645 0.958721 2020-09-01 260.280 259.918 258.115 256.759 0.139275 0.838773 1.371325 1.677547 2020-10-01 260.388 260.280 256.389 257.346 0.041494 1.559739 1.182066 3.119479 2020-11-01 260.229 260.388 256.394 257.208 -0.061063 1.495745 1.174536 2.991490 2020-12-01 260.474 260.229 257.797 256.974 0.094148 1.038414 1.362005 2.076828 2021-01-01 261.582 260.474 259.101 257.971 0.425378 0.957542 1.399770 1.915083 2021-02-01 263.014 261.582 259.918 258.678 0.547438 1.191145 1.676215 2.382290 2021-03-01 264.877 263.014 260.280 258.115 0.708327 1.766175 2.619763 3.532350 2021-04-01 267.054 264.877 260.388 256.389 0.821891 2.560026 4.159695 5.120052 2021-05-01 269.195 267.054 260.229 256.394 0.801711 3.445427 4.992707 6.890854 2021-06-01 271.696 269.195 260.474 257.797 0.929066 4.308299 5.391451 8.616599 2021-07-01 273.003 271.696 261.582 259.101 0.481052 4.366126 5.365475 8.732252 2021-08-01 273.567 273.003 263.014 259.918 0.206591 4.012334 5.251272 8.024668 2021-09-01 274.310 273.567 264.877 260.280 0.271597 3.561276 5.390349 7.122551 2021-10-01 276.589 274.310 267.054 260.388 0.830812 3.570439 6.221869 7.140878 2021-11-01 277.948 276.589 269.195 260.229 0.491343 3.251546 6.809003 6.503093 2021-12-01 278.802 277.948 271.696 260.474 0.307252 2.615423 7.036403 5.230846 2022-01-01 281.148 278.802 273.003 261.582 0.841457 2.983484 7.479872 5.966967 2022-02-01 283.716 281.148 273.567 263.014 0.913398 3.709877 7.871064 7.419755 2022-03-01 287.504 283.716 274.310 264.877 1.335138 4.809887 8.542456 9.619773 <pre><code>composite_rate = df.iloc[-1]['percent_change_6m_annualized'].round(2)\nlatest_CPI_date = df.iloc[-1].name.date()\n\nprint(f'Latest CPI Date: {latest_CPI_date}, Composite Rate: {composite_rate}%')\n</code></pre> <pre>\n<code>Latest CPI Date: 2022-03-01, Composite Rate: 9.62%\n</code>\n</pre> <pre><code>pct_columns = ['percent_change_1m', 'percent_change_6m', 'percent_change_12m', 'percent_change_6m_annualized']\n</code></pre> <pre><code>df[pct_columns].tail(12)\n</code></pre> percent_change_1m percent_change_6m percent_change_12m percent_change_6m_annualized DATE 2021-04-01 0.821891 2.560026 4.159695 5.120052 2021-05-01 0.801711 3.445427 4.992707 6.890854 2021-06-01 0.929066 4.308299 5.391451 8.616599 2021-07-01 0.481052 4.366126 5.365475 8.732252 2021-08-01 0.206591 4.012334 5.251272 8.024668 2021-09-01 0.271597 3.561276 5.390349 7.122551 2021-10-01 0.830812 3.570439 6.221869 7.140878 2021-11-01 0.491343 3.251546 6.809003 6.503093 2021-12-01 0.307252 2.615423 7.036403 5.230846 2022-01-01 0.841457 2.983484 7.479872 5.966967 2022-02-01 0.913398 3.709877 7.871064 7.419755 2022-03-01 1.335138 4.809887 8.542456 9.619773 <pre><code>df[pct_columns].plot(title='CPI', figsize=(15, 10));\n</code></pre>"},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds/#calculate-composite-rate-for-series-i-savings-bonds-i-bonds-using-cpi","title":"Calculate Composite Rate for Series I Savings Bonds (I Bonds) using CPI","text":""},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds/#install-packages","title":"Install Packages","text":""},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds/#download-cpi-data-from-fred","title":"Download CPI Data from FRED","text":"<p>Download Consumer Price Index for All Urban Consumers - Not Seasonaly Adjusted CPIAUCNS.</p>"},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds/#plot-cpi-data","title":"Plot CPI Data","text":""},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds/#calculate-percent-change","title":"Calculate Percent Change","text":""},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds/#composite-rate","title":"Composite Rate","text":"<p>Following we can see the composite rate based on the latest CPI data.</p>"},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds/#plot-percent-changes-over-time","title":"Plot Percent Changes over Time","text":""},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds_Latest/","title":"Calculate Composite Rate for Series I Bonds Latest","text":"<pre><code>import numpy as np\nimport pandas as pd\nfrom pandas_datareader.fred import FredReader\n</code></pre> <pre><code>end_date = pd.to_datetime('today').date()\nstart_date = (end_date + pd.DateOffset(years=-5)).date()\n\nprint(f'start_date: {start_date} to end_date: {end_date}')\n</code></pre> <pre>\n<code>start_date: 2017-08-11 to end_date: 2022-08-11\n</code>\n</pre> <pre><code>df = FredReader('CPIAUCNS', start=start_date).read()\n</code></pre> <pre><code>df.tail(12)\n</code></pre> CPIAUCNS DATE 2021-08-01 273.567 2021-09-01 274.310 2021-10-01 276.589 2021-11-01 277.948 2021-12-01 278.802 2022-01-01 281.148 2022-02-01 283.716 2022-03-01 287.504 2022-04-01 289.109 2022-05-01 292.296 2022-06-01 296.311 2022-07-01 296.276 <pre><code>df.plot(title='CPI - CPIAUCNS', figsize=(15,10));\n</code></pre> <pre><code>df['CPIAUCNS-1m'] = df['CPIAUCNS'].shift(periods=1)\ndf['CPIAUCNS-6m'] = df['CPIAUCNS'].shift(periods=6)\ndf['CPIAUCNS-12m'] = df['CPIAUCNS'].shift(periods=12)\ndf.tail(12)\n</code></pre> CPIAUCNS CPIAUCNS-1m CPIAUCNS-6m CPIAUCNS-12m DATE 2021-08-01 273.567 273.003 263.014 259.918 2021-09-01 274.310 273.567 264.877 260.280 2021-10-01 276.589 274.310 267.054 260.388 2021-11-01 277.948 276.589 269.195 260.229 2021-12-01 278.802 277.948 271.696 260.474 2022-01-01 281.148 278.802 273.003 261.582 2022-02-01 283.716 281.148 273.567 263.014 2022-03-01 287.504 283.716 274.310 264.877 2022-04-01 289.109 287.504 276.589 267.054 2022-05-01 292.296 289.109 277.948 269.195 2022-06-01 296.311 292.296 278.802 271.696 2022-07-01 296.276 296.311 281.148 273.003 <p>Composite Rate is 6 months percentage change multiplied by 2 (annualized).</p> <pre><code>df['percent_change_1m'] = (df['CPIAUCNS'] - df['CPIAUCNS-1m'])*100 / df['CPIAUCNS-1m']\ndf['percent_change_6m'] = (df['CPIAUCNS'] - df['CPIAUCNS-6m'])*100 / df['CPIAUCNS-6m']\ndf['percent_change_12m'] = (df['CPIAUCNS'] - df['CPIAUCNS-12m'])*100 / df['CPIAUCNS-12m']\n\n# Composite Rate\ndf['percent_change_6m_annualized'] = df['percent_change_6m'] *2\ncols = ['percent_change_1m', 'percent_change_6m', 'percent_change_12m', 'percent_change_6m_annualized']\ndf[cols].tail(12)\n</code></pre> percent_change_1m percent_change_6m percent_change_12m percent_change_6m_annualized DATE 2021-08-01 0.206591 4.012334 5.251272 8.024668 2021-09-01 0.271597 3.561276 5.390349 7.122551 2021-10-01 0.830812 3.570439 6.221869 7.140878 2021-11-01 0.491343 3.251546 6.809003 6.503093 2021-12-01 0.307252 2.615423 7.036403 5.230846 2022-01-01 0.841457 2.983484 7.479872 5.966967 2022-02-01 0.913398 3.709877 7.871064 7.419755 2022-03-01 1.335138 4.809887 8.542456 9.619773 2022-04-01 0.558253 4.526572 8.258629 9.053144 2022-05-01 1.102352 5.162117 8.581512 10.324233 2022-06-01 1.373608 6.280084 9.059758 12.560168 2022-07-01 -0.011812 5.380796 8.524815 10.761592 <pre><code>composite_rate = df.iloc[-1]['percent_change_6m_annualized'].round(2)\ninflation_rate = df.iloc[-1]['percent_change_12m'].round(2)\nlatest_CPI_date = df.iloc[-1].name.date()\n\nprint(f'Latest CPI Date: {latest_CPI_date}, Inflation Rate: {inflation_rate}%, Composite Rate: {composite_rate}%')\n</code></pre> <pre>\n<code>Latest CPI Date: 2022-07-01, Inflation Rate: 8.52%, Composite Rate: 10.76%\n</code>\n</pre> <pre><code># pct_columns = ['percent_change_1m', 'percent_change_6m', 'percent_change_12m', 'percent_change_6m_annualized']\npct_columns = ['percent_change_6m_annualized']\n</code></pre> <pre><code>df[pct_columns].tail(12)\n</code></pre> percent_change_6m_annualized DATE 2021-08-01 8.024668 2021-09-01 7.122551 2021-10-01 7.140878 2021-11-01 6.503093 2021-12-01 5.230846 2022-01-01 5.966967 2022-02-01 7.419755 2022-03-01 9.619773 2022-04-01 9.053144 2022-05-01 10.324233 2022-06-01 12.560168 2022-07-01 10.761592 <pre><code>df[pct_columns].plot(title='Calculated Inflation Rate', figsize=(15, 10));\n</code></pre> <pre><code>print(f\"Last Updated {pd.to_datetime('today')}\")\n</code></pre> <pre>\n<code>Last Updated 2022-08-11 12:03:35.659083\n</code>\n</pre>"},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds_Latest/#calculate-composite-rate-for-series-i-savings-bonds-i-bonds-using-cpi","title":"Calculate Composite Rate for Series I Savings Bonds (I Bonds) using CPI","text":""},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds_Latest/#install-packages","title":"Install Packages","text":""},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds_Latest/#download-cpi-data-from-fred","title":"Download CPI Data from FRED","text":"<p>Download Consumer Price Index for All Urban Consumers - Not Seasonaly Adjusted CPIAUCNS.</p>"},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds_Latest/#plot-cpi-data","title":"Plot CPI Data","text":""},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds_Latest/#calculate-percent-change","title":"Calculate Percent Change","text":""},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds_Latest/#composite-rate","title":"Composite Rate","text":"<p>Following we can see the composite rate based on the latest CPI data.</p>"},{"location":"Finance/Calculate_Composite_Rate_for_Series_I_Bonds_Latest/#plot-percent-changes-over-time","title":"Plot Percent Changes over Time","text":""},{"location":"Finance/Precious_Metals_Spot_Prices_Comparison/","title":"Precious Metals Spot Prices Comparison","text":"<p>Data source &amp; cadence: Alpha Vantage (daily), using <code>TIME_SERIES_DAILY</code> with <code>symbolUSD</code> and an <code>FX_DAILY</code> fallback, refreshed each morning with the <code>AV_API_KEY</code> secret. Timestamps below show the latest run (UTC).</p> <pre><code># If needed, install dependencies\n!pip install -q yfinance pandas plotly statsmodels\n</code></pre> <pre><code>import os\nimport time\nimport pandas as pd\nimport requests\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom datetime import datetime, timezone\nfrom IPython.display import Markdown, display\nimport yfinance as yf\n</code></pre> <pre><code>tickers = {\n    \"Gold (XAUUSD)\": \"GC=F\",\n    \"Silver (XAGUSD)\": \"SI=F\",\n    # \"Platinum (XPTUSD)\": \"PL=F\",\n    # \"Palladium (XPDUSD)\": \"PA=F\",\n}\n\nprice_frames = []\nskipped = []\n\nfor label, yahoo_symbol in tickers.items():\n    try:\n        ticker = yf.Ticker(yahoo_symbol)\n        # Download data for the last 5 years using Ticker.history\n        # auto_adjust=True is the default for history(), so no need to specify.\n        # progress=False is not a valid argument for history()\n        data = ticker.history(period=\"5y\", interval=\"1d\")\n        if not data.empty:\n            # Use 'Close' as auto_adjust handles splits/dividends\n            series = data['Close'].rename(label)\n            price_frames.append(series)\n        else:\n            skipped.append(f\"{label} (No data returned from Yahoo Finance for symbol {yahoo_symbol})\")\n    except Exception as e:\n        skipped.append(f\"{label} (Error fetching data from Yahoo Finance: {type(e).__name__} - {e})\")\n    time.sleep(1) # Be nice to Yahoo Finance\n\nif not price_frames:\n    raise ValueError(\"No price data returned from Yahoo Finance for any metal. Skipped: \" + \", \".join(skipped))\n\nprices = pd.concat(price_frames, axis=1).sort_index().ffill().dropna(how=\"all\")\nif prices.empty:\n    raise ValueError(\"Price data empty after cleaning; check Yahoo Finance availability.\")\nprices.index.name = \"Date\"\nprices.tail()\n</code></pre> Gold (XAUUSD) Silver (XAGUSD) Date 2026-01-09 00:00:00-05:00 4490.299805 78.884003 2026-01-12 00:00:00-05:00 4604.299805 84.610001 2026-01-13 00:00:00-05:00 4589.200195 85.876999 2026-01-14 00:00:00-05:00 4626.299805 90.869003 2026-01-15 00:00:00-05:00 4620.500000 92.209999 <pre><code>normalized = prices / prices.iloc[0] * 100\nnormalized.head()\n</code></pre> Gold (XAUUSD) Silver (XAGUSD) Date 2021-01-15 00:00:00-05:00 100.000000 100.000000 2021-01-19 00:00:00-05:00 100.557588 101.828796 2021-01-20 00:00:00-05:00 102.000764 103.661625 2021-01-21 00:00:00-05:00 101.967966 104.016112 2021-01-22 00:00:00-05:00 101.443170 102.815707 <pre><code>pct_change = prices.pct_change().dropna() * 100\npct_change.head()\n</code></pre> Gold (XAUUSD) Silver (XAGUSD) Date 2021-01-19 00:00:00-05:00 0.557588 1.828796 2021-01-20 00:00:00-05:00 1.435174 1.799913 2021-01-21 00:00:00-05:00 -0.032155 0.341965 2021-01-22 00:00:00-05:00 -0.514668 -1.154056 2021-01-25 00:00:00-05:00 -0.043106 -0.270335 <pre><code>fig = px.line(\n    prices.reset_index(),\n    x=\"Date\",\n    y=prices.columns,\n    title=\"Spot Price Comparison (Last 5 Years)\",\n    labels={\"value\": \"USD per troy ounce\", \"Date\": \"Date\"},\n)\nfig.update_layout(legend_title_text=\"Metal\")\nfig.show()\n</code></pre> <p> </p>"},{"location":"Finance/Precious_Metals_Spot_Prices_Comparison/#precious-metals-daily-spot-price-snapshot","title":"Precious Metals: Daily Spot Price Snapshot","text":"<p>A quick daily view of gold, silver, platinum, and palladium with performance, volatility, and a simple 12-month trend projection.</p>"},{"location":"Finance/Precious_Metals_Spot_Prices_Comparison/#precious-metal-spot-price-comparison-5-years","title":"Precious Metal Spot Price Comparison (5 Years)","text":"<p>This notebook compares precious metal spot prices over the last five years and builds a simple forecast using interactive charts.</p> <pre><code>fig = px.line(\n    normalized.reset_index(),\n    x=\"Date\",\n    y=normalized.columns,\n    title=\"Normalized Performance (100 = Start of Period)\",\n    labels={\"value\": \"Index (100 = start)\", \"Date\": \"Date\"},\n)\nfig.update_layout(legend_title_text=\"Metal\")\nfig.show()\n</code></pre> <p> </p> <pre><code>fig = px.line(\n    pct_change.tail(30).reset_index(),\n    x=\"Date\",\n    y=pct_change.columns,\n    title=\"Daily Percent Change (Last 30 Days)\",\n    labels={\"value\": \"Percent change (%)\", \"Date\": \"Date\"},\n)\nfig.update_layout(legend_title_text=\"Metal\")\nfig.show()\n</code></pre> <p> </p> <pre><code>monthly = prices.resample(\"M\").mean().dropna()\nforecast_horizon = 12\n\nforecast_frames = []\nfor metal in monthly.columns:\n    model = ExponentialSmoothing(\n        monthly[metal],\n        trend=\"add\",\n        seasonal=None,\n        initialization_method=\"estimated\",\n    ).fit()\n    forecast = model.forecast(forecast_horizon)\n    forecast.name = metal\n    forecast_frames.append(forecast)\n\nforecast_df = pd.concat(forecast_frames, axis=1)\nforecast_df.tail()\n</code></pre> <pre>\n<code>/tmp/ipython-input-120228698.py:1: FutureWarning:\n\n'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n\n</code>\n</pre> Gold (XAUUSD) Silver (XAGUSD) 2026-09-30 00:00:00-04:00 5756.569793 197.002944 2026-10-31 00:00:00-04:00 5912.852272 211.483924 2026-11-30 00:00:00-05:00 6069.134752 225.964904 2026-12-31 00:00:00-05:00 6225.417231 240.445885 2027-01-31 00:00:00-05:00 6381.699710 254.926865 <pre><code>fig = go.Figure()\nfor metal in monthly.columns:\n    fig.add_trace(\n        go.Scatter(\n            x=monthly.index,\n            y=monthly[metal],\n            name=f\"{metal} (history)\",\n        )\n    )\n    fig.add_trace(\n        go.Scatter(\n            x=forecast_df.index,\n            y=forecast_df[metal],\n            name=f\"{metal} (forecast)\",\n            line=dict(dash=\"dash\"),\n        )\n    )\n\nfig.update_layout(\n    title=\"12-Month Forecast (Holt-Winters, Monthly Avg)\",\n    xaxis_title=\"Date\",\n    yaxis_title=\"USD per troy ounce\",\n    legend_title_text=\"Series\",\n)\nfig.show()\n</code></pre> <p> </p> <pre><code>\n</code></pre> <pre><code>timestamp = datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')\ndisplay(Markdown(f'**Last updated:** {timestamp}'))\n</code></pre> <p>Last updated: 2026-01-15 22:10 UTC</p>"},{"location":"Finance/Precious_Metals_Spot_Prices_Comparison/#where-to-get-spot-price-data","title":"Where to get spot price data","text":"<p>Common sources for spot price data include:</p> <ul> <li>LBMA (London Bullion Market Association): Official daily gold and silver price benchmarks. Useful for authoritative spot pricing.</li> <li>Metals-API: Paid/free tiers with JSON API access for multiple metals (gold, silver, platinum, palladium).</li> <li>Alpha Vantage: Free tier provides precious metals data with API keys and rate limits.</li> <li>Quandl/Nasdaq Data Link: Offers LBMA and other datasets (free and paid).</li> <li>Yahoo Finance: Convenient access for analysis (e.g., <code>XAUUSD=X</code>, <code>XAGUSD=X</code>, <code>XPTUSD=X</code>, <code>XPDUSD=X</code>). While not an official benchmark, it's easy to use for exploratory analysis.</li> </ul> <p>This notebook uses Yahoo Finance spot proxies for convenience.</p>"},{"location":"Finance/Precious_Metals_Spot_Prices_Comparison/#data-download","title":"Data download","text":"<p>Download the last five years of daily closes for each metal.</p>"},{"location":"Finance/Precious_Metals_Spot_Prices_Comparison/#spot-price-history","title":"Spot price history","text":"<p>See how absolute prices have moved over time.</p>"},{"location":"Finance/Precious_Metals_Spot_Prices_Comparison/#performance-since-start","title":"Performance since start","text":"<p>Each series is rebased to 100 on day one to compare relative performance.</p>"},{"location":"Finance/Precious_Metals_Spot_Prices_Comparison/#day-to-day-volatility","title":"Day-to-day volatility","text":"<p>Daily percent change highlights short-term swings and risk.</p>"},{"location":"Finance/Precious_Metals_Spot_Prices_Comparison/#simple-forecast-12-months","title":"Simple Forecast (12 Months)","text":"<p>We use a basic Holt-Winters exponential smoothing model on monthly averages to produce a simple, transparent forecast.</p>"},{"location":"Finance/Precious_Metals_Spot_Prices_Comparison/#12-month-outlook","title":"12-month outlook","text":"<p>Monthly averages feed a simple Holt-Winters model to sketch a near-term trend. Treat this as directional only.</p>"},{"location":"Git/Add_All_FIles_With_Pattern/","title":"Git Add All Files with Specific Extension","text":"<p>Git add all files in subdirectories with a specific extension.</p> <pre><code>git add ./\\*.ipynb\n</code></pre>"},{"location":"Google_Sheets/custom_google_sheets_function/","title":"Mastering Datetime Conversions in Google Sheets: A Custom Function Hack","text":"<p>Tired of manually converting ISO 8601 datetime strings into Google Sheets\u2019 native format? Let\u2019s automate this tedious task with a custom function!</p>"},{"location":"Google_Sheets/custom_google_sheets_function/#understanding-the-problem","title":"Understanding the Problem:","text":"<p>Google Sheets, while a powerful tool, lacks a direct function to convert ISO 8601 datetime strings (e.g., \u201c2023\u201311\u201322T13:37:00Z\u201d) into its native datetime format. This can be a major headache when working with data from APIs or other sources that use this standard format.</p>"},{"location":"Google_Sheets/custom_google_sheets_function/#the-solution-a-custom-function","title":"The Solution: A Custom Function","text":"<p>To streamline this process, we\u2019ll create a custom function using Google Apps Script. This function will parse the ISO 8601 string and return a Google Sheets datetime value, saving you time and effort.</p> <p>Here\u2019s the custom function: <pre><code>/**\n * A custom function to convert an ISO 8601 datetime string to a Google Sheets datetime value.\n * @param {string} isoString The ISO 8601 datetime string to convert.\n * @returns {Date} A Google Sheets datetime value.\n * @customfunction\n */\nfunction ISOSTRTODATE(isoString) {\n  return new Date(isoString);\n}\n</code></pre></p>"},{"location":"Google_Sheets/custom_google_sheets_function/#how-to-use-it","title":"How to Use It:","text":"<p>Create a New Script:</p> <p>In your Google Sheet, go to Extensions &gt; App Script. This will open the script editor. Paste the code into the script editor. Save the script. Use the Function in Your Sheet:</p> <p>In any cell, type the following formula, replacing A2 with the cell containing the ISO 8601 string: <code>=ISOSTRTODATE(A2)</code></p>"},{"location":"Google_Sheets/custom_google_sheets_function/#example","title":"Example:","text":"<p>If cell A2 contains the ISO 8601 string \u201c2023\u201311\u201322T13:37:00Z\u201d, the formula <code>=ISOSTRTODATE(A2)</code> will convert it to a Google Sheets datetime value, displaying it in your local time zone.</p>"},{"location":"Google_Sheets/custom_google_sheets_function/#additional-tips","title":"Additional Tips:","text":"<p>Formatting the Output: You can format the output cell to display the datetime in your desired format. Error Handling: Consider adding error handling to the function to gracefully handle invalid input. Batch Processing: For large datasets, explore Google Apps Script\u2019s batch processing capabilities to optimize performance. Leverage Google Apps Script: Explore other Google Apps Script features to automate tasks, create custom menus, and more. By following these steps and using the <code>ISOSTRTODATE</code> function, you can automate the conversion of ISO 8601 datetime strings to Google Sheets datetime values, making your data analysis and manipulation more efficient and accurate.</p>"},{"location":"Linux/Change_Ownership/","title":"Chnage Ownership","text":"<p>To change the ownership of all the files in current directory.</p> <pre><code>sudo chown -R $USER: .\n</code></pre>"},{"location":"Linux/Check_Group_Membership/","title":"Check Group Membership","text":"<pre><code>id $USER | tr ',' '\\n'\n</code></pre>"},{"location":"Linux/Dictionary_Words/","title":"Unix Words File","text":"<p>Dictionary Words is a standard file on Unix and Unix-like operating systems, and is simply a newline-delimited list of dictionary words. It is used, for instance, by spell-checking programs.</p> <pre><code>cat /usr/share/dict/words\n</code></pre>"},{"location":"Linux/List_files_exceeding_size/","title":"List Files Exceeding Certain Size","text":"<p>List Files Exceeding Certain Size in current (or given) directory</p> <pre><code>find . -maxdepth 1 -type f -size +10M\n</code></pre>"},{"location":"Linux/Random_Password/","title":"Random Password Generator using <code>openssl</code>","text":""},{"location":"Linux/Random_Password/#password-with-hexadecimal-characters","title":"Password with Hexadecimal Characters","text":"<p>Specify number of characters, 32 in this example.</p> <pre><code>openssl rand -hex 32\n</code></pre> <p>Example Output</p> <p>35e29ce60b04a7cafb7790a62805d5ed62ff012cbb51fa2f1bb40cb94a356b9b</p>"},{"location":"Linux/Random_Password/#password-with-base64-characters","title":"Password with Base64 Characters","text":"<p>Specify number of characters, 16 in this example.</p> <pre><code>openssl rand -base64 16\n</code></pre> <p>Example Output</p> <p>buARrzhKpgnDaWI6lAzsRA==</p>"},{"location":"Linux/Sort_Directory_Size/","title":"List Directories Sorted by Size","text":"<p>Sort and list directory sizes in human readable format.</p> <pre><code>du -sh * | sort -rh\n</code></pre> <p>Or use <code>ducks</code></p> <pre><code>du -cksh * | sort -hr | head -n 10\n</code></pre> <p>du: Disk Usage -c: Total -k: Block-size=1K -s: Summarize -h: Human-readable</p>"},{"location":"Linux/Symbolic_Link/","title":"Symbolic link","text":"<p>Symbolic link In computing, a symbolic link (also symlink or soft link) is a term for any file that contains a reference to another file or directory in the form of an absolute or relative path and that affects pathname resolution.</p> <pre><code>readlink /usr/share/dict/words\n</code></pre>"},{"location":"MCP/fastmcp_server_client/","title":"Building a Minimal MCP Server and Client with FastMCP 2.0","text":"<p>The Model Context Protocol (MCP) standardizes how AI assistants discover and safely call external tools. FastMCP 2.0 is a batteries-included Python framework that streamlines both server and client development while remaining fully compatible with the protocol. This note walks through creating a single <code>greet</code> tool that runs inside a FastMCP server and exercising it from a FastMCP client.</p>"},{"location":"MCP/fastmcp_server_client/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+</li> <li><code>pipx</code> (recommended) or <code>pip</code></li> <li>The <code>fastmcp</code> package pinned to the 2.x series for both the server and the client</li> </ul> <p>Install FastMCP globally with <code>pipx</code>:</p> <pre><code>pipx install \"fastmcp==2.*\"\n</code></pre> <p>or within a virtual environment that you manage manually:</p> <pre><code>python -m venv .venv\nsource .venv/bin/activate\npip install \"fastmcp==2.*\"\n</code></pre> <p>FastMCP bundles an async runtime, a schema-first tool decorator, and a modern client that can speak to any MCP-compliant server.</p>"},{"location":"MCP/fastmcp_server_client/#project-layout","title":"Project Layout","text":"<pre><code>fastmcp-demo/\n\u251c\u2500\u2500 client.py\n\u2514\u2500\u2500 server.py\n</code></pre> <p>The <code>server.py</code> script exposes the <code>greet</code> tool, while <code>client.py</code> starts the server as a subprocess and issues requests through the MCP transport.</p>"},{"location":"MCP/fastmcp_server_client/#implementing-the-fastmcp-server","title":"Implementing the FastMCP Server","text":"<p>Create <code>server.py</code> with the following contents:</p> <pre><code>\"\"\"Minimal FastMCP 2.0 server exposing a single greeting tool.\"\"\"\nimport asyncio\nfrom dataclasses import dataclass\n\nfrom fastmcp import FastMCP, tool\n\nmcp = FastMCP(\"fast-greeter\")\n\n\n@dataclass\nclass GreetArgs:\n    \"\"\"Typed input schema for the greet tool.\"\"\"\n\n    name: str\n\n\n@tool(mcp)\nasync def greet(args: GreetArgs) -&gt; str:\n    \"\"\"Return a friendly greeting for the provided name.\"\"\"\n\n    person = args.name.strip() or \"there\"\n    return f\"Hello, {person}!\"\n\n\ndef main() -&gt; None:\n    asyncio.run(mcp.run_stdio())\n\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Key FastMCP 2.0 features illustrated above:</p> <ul> <li><code>FastMCP(\"fast-greeter\")</code> registers the server with a human-readable identifier that will surface in client handshakes.</li> <li><code>@tool(mcp)</code> wires the coroutine into the server, automatically deriving the MCP JSON schema from the <code>dataclass</code> argument.</li> <li><code>mcp.run_stdio()</code> launches the event loop and keeps the server alive on the standard input/output transport expected by most   assistants.</li> </ul>"},{"location":"MCP/fastmcp_server_client/#implementing-the-fastmcp-client","title":"Implementing the FastMCP Client","text":"<p>Create <code>client.py</code> next:</p> <pre><code>\"\"\"Minimal FastMCP 2.0 client that calls the greet tool.\"\"\"\nimport asyncio\nfrom fastmcp import FastMCPClient\n\n\nasync def main() -&gt; None:\n    async with FastMCPClient.from_subprocess([\"python\", \"server.py\"]) as session:\n        tools = await session.list_tools()\n        print(\"Tools:\", tools)\n\n        response = await session.call_tool(\"greet\", {\"name\": \"Ada\"})\n        print(\"Tool response:\", response)\n\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n</code></pre> <p>Highlights:</p> <ul> <li><code>FastMCPClient.from_subprocess()</code> launches the server as a subprocess and negotiates the MCP handshake automatically.</li> <li><code>session.list_tools()</code> returns the tool metadata exposed by the server.</li> <li><code>session.call_tool(\"greet\", ...)</code> invokes the server tool and returns the structured response that the assistant will see.</li> </ul>"},{"location":"MCP/fastmcp_server_client/#running-the-demo","title":"Running the Demo","text":"<ol> <li>Start the server in one terminal tab (or leave it to the client to spawn automatically):</li> </ol> <pre><code>python server.py\n</code></pre> <p>The process waits for MCP requests while streaming logs to the console.</p> <ol> <li>In a second terminal, execute the client:</li> </ol> <pre><code>python client.py\n</code></pre> <p>Expected output resembles the following:</p> <pre><code>Tools: [{'name': 'greet', 'description': 'Return a friendly greeting', ...}]\nTool response: {'type': 'text', 'text': 'Hello, Ada!'}\n</code></pre> <ol> <li>Stop the server with <code>Ctrl+C</code> when finished.</li> </ol>"},{"location":"MCP/fastmcp_server_client/#extending-the-example","title":"Extending the Example","text":"<ul> <li>Add more tools by defining additional <code>@tool(mcp)</code> coroutines and associated dataclasses for their inputs.</li> <li>Switch to transports such as WebSocket or TCP by replacing <code>mcp.run_stdio()</code> with the corresponding <code>fastmcp</code> helper.</li> <li>Use FastMCP's dependency-injection hooks (for example, <code>@tool(mcp, inject=[...])</code>) to share database handles or API clients   across tools.</li> <li>Integrate with the <code>mcp</code> developer CLI (<code>mcp run ...</code>) or other MCP-compliant clients by pointing them at <code>server.py</code>.</li> </ul> <p>FastMCP 2.0 keeps the boilerplate minimal while providing type-safe tooling definitions, making it an excellent starting point for more sophisticated assistants.</p>"},{"location":"MacOS/Find_WiFi_Password/","title":"Find WiFi Password on Mac OS Terminal","text":"<pre><code>security find-generic-password -ga WIFI_NAME | grep \"password:\"\n</code></pre>"},{"location":"MacOS/Flush_DNS_Cache/","title":"Flush DNS Cache on Mac OS X 12 (Sierra) and Later","text":"<pre><code>sudo killall -HUP mDNSResponder; \\\nsudo killall mDNSResponderHelper; \\\nsudo dscacheutil -flushcache\n</code></pre>"},{"location":"Misc/Alaska_RV_Parks_Map/","title":"Alaska RV Parks Map","text":"<pre><code>!pip install polyline geocoder -q\n</code></pre> <pre><code>import pandas as pd\n</code></pre> <pre><code>df_all = pd.read_html('https://www.greatalaskanholidays.com/trip-planning/recommended-campgrounds/')\n</code></pre> <pre><code>len(df_all)\n</code></pre> <pre>\n<code>3</code>\n</pre> <pre><code>df = df_all[0].copy()\n# df.head(5)\n</code></pre> <pre><code>df.columns = ['CampgroundName', 'Sites', 'Website', 'BusinessAddress', 'Email']\ndf = df.fillna('')\ndf\n</code></pre> CampgroundName Sites Website BusinessAddress Email 0 Alaskan  Angler RV Resort 73 https://alaskabestrvpark.com/ 15640  Kingsley Rd, Ninilchik, AK 99639 bakersrvresort@yahoo.com 1 Anchorage  Ship Creek RV Park 147 150  N. Ingra St, Anchorage, AK 99501 dsmith@bestofalaskatravel.com 2 Bayside  RV Park 117 https://www.baysidervcabin.com/ 230  E. Egan St, Valdez, AK 99686 Bayside1@cvinternet.net 3 Bear  Creek RV Park 40 http://bearcreekrv.com/ 33508  Lincoln Ave, Seward, AK 99664 bearcreekrvpk@gci.net 4 Big  Bear RV Park 59 http://www.bigbearrv.net 2010  Church St, Palmer, AK 99645 bigbear@mtaonline.net 5 Cantwell  RV Park Inc. 80 https://cantwellrvpark.wordpress.com/ PO  Box 210 Cantwell, AK 99729 cantwellrvpark@ak.net,  cantwellrvpark@gmail.com 6 Chicken  Gold Camp &amp; Outpost 70 http://chickengold.com 1/4  mile Airport Rd, Chicken Alaska 99732 chickenrvpark@gmail.com 7 Clearwater  Ranch Cabins &amp; RV Park 12 http://clearwaterlodgeak.com 7028  Remington Road, Delta Junction, AK 99737 ewingk@wildak.net 8 Deep  Creek View Campground 45 http://smartcharters.com 16215  Shy Maiden Circle, Ninilchik smartcharters@alaskan.com 9 Denali  Grizzly Bear Resort 24 http://denaligrizzlybear.com Mile  231.1 George Parks Hwy,Denali, AK scott@denaligrizzlybear.com 10 Denali  Rainbow Village &amp; RV Park 58 http://denalirv.com Mile  238.6 Parks Highway denalirainbow@mtaonline.net,  denalirv@yahoo.com 11 Denali  RV Park &amp; Motel 80 http://denalirvparkandmotel.com Mile  245.1 Parks Highway stay@denalirvparkandmotel.com 12 Diamond  M Ranch Resort 93 http://diamondmranch.com 48500  Diamond M Ranch Road ronna@diamondmranch.com,  office@diamondmranch... 13 Driftwood  Inn &amp; RV Park 22 https://thedriftwoodinn.com/ 135  West Bunnell, Homer, AK 99603 driftwoodinn@alaskan.com,  adriennewalli@hotma... 14 Eagles  Rest RV Park &amp; Cabins 244 http://eaglesrestrv.com Eagles  Rest RV Park &amp; Cabins,Valdez, AK laurasaxe@yahoo.com 15 Exit  Glacier RV Park 60 31702  Herman Leirer Rd, Seward, AK sewardrvpark@gmail.com 16 Fox  Run Lodge &amp; RV Park 38 http://foxruncamp.com/ 4466  S. Glenn Highway, Palmer, AK 99645 walkingoutside@mtaonline.net 17 Golden  Nugget RV Park 212 http://www.goldennuggetrvpark.com 4100  DeBarr Road, Anchorage, AK 99508 gnugget@gci.net 18 Grand  View RV Park 28 http://grandviewrv.com Milepost  109.7 Glenn Highway info@grandviewrv.com 19 Grizzly  Lake Campground 22 Mile  53 Tok Cutoff Gakona, AK 99586 grizzlylake1@cvinternet.net 20 Gwin\u2019s  Lodge Historic Roadhouse &amp; Campground 24 http://gwinslodge.com 14865  Sterling Highway, Milepost 52 reservations@gwinslodge.com 21 Happy Valley RV Campground 7 www.rv-alaska.com 21590 Sterling Highway, Ninilchik, AK 99639 happyvalleyrv@gmail.com 22 Heritage  RV Park 107 https://alaskanheritagervpark.com/ 3550  Homer Spit Road Homer, Alaska 99603 heritagervpark@alaska.net 23 Homer  BaycrestKOA RV Park 45 http://www.baycrestrvpark.com 3425  Sterling Highway Homer, AK 99603 homerunoil@alaska.net 24 Homer  Spit Campground 119 https://www.alaskacampgrounds.net/ 4535  Homer Spit Road Homer, Alaska 99603 baycrestrvpark@gmail.com 25 Ice  Art RV Park &amp; Campground 90 https://www.iceartpark.com/ 3570  Phillips Field Road, Fairbanks, AK 99709 ice@gci.net 26 Kenai  RV 24 https://kenairv.net/ 39864  Kalifornsky Beach Road, Kenai, AK 99611 grandma9cindy@gmail.com 27 Klondike  RV Park &amp; Cottages 35 http://www.klondikervpark.com 48665  Funny River Road, Soldotna, AK manager@klondikervpark.com 28 Kyllonen\u2019s  RV Park 24 http://www.kyllonensrvpark.com 74160  Anchor Point Rd, Anchor Point, AK 99556 info@kyllonensrvpark.com 29 Lu-Lu  Belle Glacier Wildlife Cruises RV Park 7 http://www.lulubelletours.com 240  Kokuk Drive, Valdez, AK 99686 info@lulubelletours.com 30 Midnight  Sun RV Park 44 248.5  Park Hwy, Healy, AK 999743 nikki@trivalleygas.com 31 Montana  Creek Campground 66 http://montanacreekcampground.com Mile  96.5 Parks Highway, Willow, Alaska montanacreek@mtaonline.net 32 Northern  Nights Campground &amp; RV Park 32 http://northernnightscampground.com 188  Glenn Hwy, Glennallen, AK 99588 tazlinamarc@yahoo.com 33 Oceanside  RV Park 23 http://oceansiderv.com 14  Front Street, Haines, Alaska greatview@oceansiderv.com 34 Ocean  Shores RV Park 85 http://homeralaskarvpark.com 455  Sterling Highway, Homer, AK 99603 mbarling1965@gmail.com 35 Ranch  HouseRV Resort 38 http://alaskaranchhouselodge.com Mile  173 PO Box 113 Glennallen, AK 99588 ranchhouselodge@gmail.com 36 Riverview  RV Park 160 http://www.riverviewrvpark.net 1316  Badger Rd, North Pole, AK 99705 riverview@gci.net 37 Seagull\u2019s  Roost Campground LLC 6 15136  Grouse Creek Road, Seward, AK 99664 seagullsroostcampgroundllc@gmail.com 38 Slide  Mountain Cabins &amp; RV Park 24 http://www.slidemountaincabins.net Mile  135 Glenn Hwy, Glennallen, AK 99588 slidemountaincabins@gmail.com 39 Sourdough  Campground &amp; Cafe 75 http://sourdoughcampground.com 1  Prospector Way, Tok, AK 99780 sourdoughcampground@outlook.com 40 Glacier  Nalu Campground 32 https://glaciernalu.com/ 10200  Mendenhall Loop Road, Juneau, AK 99801 glaciernalu@gmail.com 41 Talkeetna  Camper Park 33 http://talkeetnacamper.com 22763  S. Talkeetna Spur Rd, Talkeetna, AK 99676 camp@talkeetnacamper.com 42 Tok  RV Village 162 http://tokrv.net Mile  1313.4 Alaska Highway camp@tokrv.net 43 Tolsona  Wilderness Campground 75 http://tolsonacampground.com Mile  173 Glenn Highway, Glennallen, AK 99588 camp@tolsonacampground.com 44 Valdez KOA- Bear Creek Cabins &amp; RV Park 68 http://bearcreekcabinsrvpark.com 3181  Richardson Hwy, Valdez, AK 99686 leickman@hotmail.com <pre><code>import logging\nimport requests\nimport json\nimport polyline\nimport folium\nfrom folium.plugins import MeasureControl\nimport geocoder\n\nfrom functools import lru_cache\n\nlogger = logging.getLogger(__name__)\nDEBUG = True\n\n@lru_cache(maxsize=None)\ndef geocode(location):\n    return _geocode(location)\n\ndef _geocode(location):\n    import geocoder\n    # g = geocoder.osm(location)\n    g = geocoder.arcgis(location)\n    return g.latlng\n\n@lru_cache(maxsize=None)\ndef get_route(olat, olng, dlat, dlng):\n    response = _get_route(olat, olng, dlat, dlng)\n    return response\n\ndef _get_route(olat, olng, dlat, dlng):\n    url = f'http://router.project-osrm.org/route/v1/driving/{olng},{olat};{dlng},{dlat}?alternatives=false&amp;amp;steps=false'\n    # logger.debug(url)\n    response = None\n\n    try:\n        logger.debug(f'====== OSRM: {url}')\n        response = requests.get(url, verify=False)\n    except Exception as ex:\n        raise\n\n    # logger.debug(response.text)\n    if response and response.text:\n        response_dict = json.loads(response.text)\n        #possible = pd.DataFrame([{'Distance': (route['distance'] / 1000) *  0.621371 , route['weight_name']: route['weight']} for route in response_dict['routes']])\n        return response_dict\n    else:\n        return None\n\ndef get_routing_map(origin, destination, zoom=5):\n    orig_latlng = geocode(origin)\n    dest_latlng = geocode(destination)\n\n    resp = get_route(orig_latlng[0], orig_latlng[1], dest_latlng[0], dest_latlng[1])\n\n    decoded = polyline.decode(resp[\"routes\"][0]['geometry'])\n    distance = resp[\"routes\"][0]['distance'] * 0.000621371\n    duration = resp[\"routes\"][0]['duration'] / 60\n\n    map2 = folium.Map(location=(orig_latlng[0], orig_latlng[1]), zoom_start=zoom,\n                                    control_scale=True)\n    # map2.add_child(MeasureControl(\n    #     primary_length_unit='miles',\n    #     secondary_length_unit='meters',\n    #     primary_area_unit='acres',\n    #     secondary_area_unit='sqmeters')\n    # )\n\n    folium.PolyLine(locations=decoded, color=\"blue\").add_to(map2)\n\n    print(f\"{duration} minutes\")\n    print(f\"{distance} miles\")\n\n    return map2\n</code></pre> <pre><code># geocode('15640 Kingsley Rd, Ninilchik, AK 99639   ')\n</code></pre> <pre><code>applied_df = df.apply(lambda row: geocode(row.BusinessAddress), axis='columns', result_type='expand')\ndf = pd.concat([df, applied_df], axis='columns')\n\ndf = df.rename(columns={0:'lat', 1: 'lng'})\n</code></pre> <pre><code># df\n</code></pre> <pre><code>df[pd.isnull(df.lat)]\n</code></pre> CampgroundName Sites Website BusinessAddress Email lat lng 18 Grand  View RV Park 28 http://grandviewrv.com Milepost  109.7 Glenn Highway info@grandviewrv.com NaN NaN 42 Tok  RV Village 162 http://tokrv.net Mile  1313.4 Alaska Highway camp@tokrv.net NaN NaN <pre><code># 61.84590432308156, -147.62491071636222\ndf.loc[df.CampgroundName == 'Grand  View RV Park', ['lat', 'lng']] = [61.84590432308156, -147.62491071636222]\n\n# 63.33500236865673, -142.9639913435805\ndf.loc[df.CampgroundName == 'Tok  RV Village', ['lat', 'lng']] = [63.33500236865673, -142.9639913435805]\n</code></pre> <pre><code>df[pd.isnull(df.lat)]\n</code></pre> CampgroundName Sites Website BusinessAddress Email lat lng <pre><code>anchorage = geocode('Anchorage, Alaska, USA')\n</code></pre> <pre><code># anchorage\n</code></pre> <pre><code>def format_popup(row):\n    format_str=f\"\"\"\n    &lt;b&gt;{row['CampgroundName']}&lt;/b&gt;&lt;br/&gt;\n    {row['BusinessAddress']}&lt;br/&gt;\n&lt;a herf=\"{row['Website']}\"&gt;{row['Website']}&lt;/a&gt;&lt;br/&gt;\n    \"\"\"\n    return format_str\n</code></pre> <pre><code>map2 = folium.Map(location=(anchorage[0], anchorage[1]), zoom_start=5, control_scale=True)\n\n# add marker one by one on the map\nfor idx, row in df.iterrows():\n    if not pd.isnull(row['lat']) and not pd.isnull(row['lng']):\n        folium.Marker(\n            location=[row['lat'], row['lng']],\n            popup=format_popup(row),\n        ).add_to(map2)\n</code></pre> <pre><code>map2\n</code></pre> Make this Notebook Trusted to load map: File -&gt; Trust Notebook <pre><code>df_1 = df_all[1]\n</code></pre> <pre><code>df_1 = df_1[~(df_1['Campground'] == df_1['Winter Open'])].reset_index(drop=True)\n</code></pre> <pre><code>df_1 = df_1.fillna('')\ndf_1.head(5)\n</code></pre> Campground RV Fee # Sites Picnic Sites Toilets Water Trails Fishing Nearest Community Location Winter Open 0 Bird Creek Y $20 27 Y H Y Y Y Girdwood Mi. 101.2 Seward Hwy 1 Bird Creek Overflow Y $20 20 Y H H H Y Girdwood 2 Eagle River* $25 57 Y H H N Y Eagle River Mi. 12.6 Glenn Hwy 3 Eklutna $15 58 H H H Y Y Eklutna Mi. 26.5 Glenn Hwy 4 Big Delta SHP Y $20 25 Y H H Y N Delta Junction Mi. 274.5 Richardson Hwy <pre><code>df_1['Address'] = df_1['Campground'] + ', ' + df_1['Location'] + ', ' + df_1['Nearest Community'] + ', AK, USA'\n</code></pre> <pre><code># df_1.columns\n</code></pre> <pre><code>df = df_1.copy()\napplied_df = df.apply(lambda row: geocode(row.Address), axis='columns', result_type='expand')\ndf = pd.concat([df, applied_df], axis='columns')\n\ndf = df.rename(columns={0:'lat', 1: 'lng'})\ndf\n</code></pre> Campground RV Fee # Sites Picnic Sites Toilets Water Trails Fishing Nearest Community Location Winter Open Address lat lng 0 Bird Creek Y $20 27 Y H Y Y Y Girdwood Mi. 101.2 Seward Hwy Bird Creek, Mi. 101.2 Seward Hwy, Girdwood, AK... 60.933641 -149.156054 1 Bird Creek Overflow Y $20 20 Y H H H Y Girdwood Bird Creek Overflow, , Girdwood, AK, USA 60.941040 -149.172700 2 Eagle River* $25 57 Y H H N Y Eagle River Mi. 12.6 Glenn Hwy Eagle River*, Mi. 12.6 Glenn Hwy, Eagle River,... 61.316480 -149.574140 3 Eklutna $15 58 H H H Y Y Eklutna Mi. 26.5 Glenn Hwy Eklutna, Mi. 26.5 Glenn Hwy, Eklutna, AK, USA 61.455170 -149.366020 4 Big Delta SHP Y $20 25 Y H H Y N Delta Junction Mi. 274.5 Richardson Hwy Big Delta SHP, Mi. 274.5 Richardson Hwy, Delta... 63.793616 -145.750973 5 Birch Lake SRS Y $15 19 Y H Y N Y Fairbanks Mi. 305.5 Richardson Hwy Birch Lake SRS, Mi. 305.5 Richardson Hwy, Fair... 64.820500 -147.700540 6 Red Squirrel Y $15 5 H H Y N Y Fairbanks Mi. 43 Chena Hot Springs Rd Red Squirrel, Mi. 43 Chena Hot Springs Rd, Fai... 64.888235 -147.622911 7 Rosehip Y $15 37 Y H Y Y Y Fairbanks Mi. 27 Chena Hot Springs Rd Rosehip, Mi. 27 Chena Hot Springs Rd, Fairbank... 64.888235 -147.622911 8 Granite Tors Y $15 24 H H Y Y Y Fairbanks Mi. 39 Chena Hot Springs Rd Granite Tors, Mi. 39 Chena Hot Springs Rd, Fai... 64.888235 -147.622911 9 Chena River SRS** Y $15/$30 61 H H H Y Y Fairbanks 3530 Geraghty Ave Chena River SRS**, 3530 Geraghty Ave, Fairbank... 64.837757 -147.806355 10 Clearwater SRS Y $15 16 Y H Y N Y Delta Junction Remington Rd Clearwater SRS, Remington Rd, Delta Junction, ... 64.051740 -145.428960 11 Delta SRS Y $15 24 Y H H Y N Delta Junction Mi. 267 Richardson Hwy Delta SRS, Mi. 267 Richardson Hwy, Delta Junct... 64.053090 -145.736720 12 Eagle Trail SRS Y $20 35 Y H Y Y N Tok Mi. 109.5 Tok Cutoff Eagle Trail SRS, Mi. 109.5 Tok Cutoff, Tok, AK... 63.284304 -143.170090 13 Harding Lake SRA Y $15 81 H H H Y Y Fairbanks Mi. 321.4 Richardson Hwy Harding Lake SRA, Mi. 321.4 Richardson Hwy, Fa... 64.820500 -147.700540 14 Moon Lake SRS Y $20 17 Y H Y N N Tok Mi. 1,332 Alaska Hwy. Moon Lake SRS, Mi. 1,332 Alaska Hwy., Tok, AK,... 63.125472 -142.059442 15 Lost Lake Y $15 11 Y H Y Y Y Delta Junction Mi. 277.8 Richardson Hwy Lost Lake, Mi. 277.8 Richardson Hwy, Delta Jun... 63.793616 -145.750973 16 Quartz Lake Campground Y $15 80 Y H Y Y Y Delta Junction Mi. 277.8 Richardson Hwy Quartz Lake Campground, Mi. 277.8 Richardson H... 63.793616 -145.750973 17 Salcha River SRS Y $15 6 Y H Y N Y Fairbanks Mi. 323 Richardson Hwy Salcha River SRS, Mi. 323 Richardson Hwy, Fair... 64.466050 -146.922480 18 Tok River SRS Y $20 24 Y H Y Y N Tok Mile 1,309 Alaska Hwy Tok River SRS, Mile 1,309 Alaska Hwy, Tok, AK,... 63.125472 -142.059442 19 Whitefish Y $15 25 Y H Y N Y Fairbanks Mi. 11 Elliot Hwy Whitefish, Mi. 11 Elliot Hwy, Fairbanks, AK, USA 64.845250 -147.722100 20 Olnes Pond Y $15 15 Y H Y Y Y Fairbanks Mi. 10.5 Elliot Hwy Olnes Pond, Mi. 10.5 Elliot Hwy, Fairbanks, AK... 64.845250 -147.722100 21 Clam Gulch SRA Y $15 116 Y H H N Y Soldotna Mi. 117 Sterling Hwy Clam Gulch SRA, Mi. 117 Sterling Hwy, Soldotna... 60.466670 -151.090603 22 Crooked Creek SRS Y $15 79 Y H H H Y Kasilof Coho Loop Crooked Creek SRS, Coho Loop, Kasilof, AK, USA 60.270076 -151.270761 23 Deep Creek SRA Y Ninilchik Mi. 138 Sterling Hwy Y Deep Creek SRA, Mi. 138 Sterling Hwy, Ninilchi... 60.040534 -151.675684 24 Johnson Lake SRA Y $15 50 Y H H N Y Kasilof Mi. 110 Sterling Hwy Y Johnson Lake SRA, Mi. 110 Sterling Hwy, Kasilo... 60.321473 -151.259947 25 Ninilchik SRA Y Ninilchik SRA, , , AK, USA 60.052400 -151.667900 26 K\u2019esugi Ken** Y $20/$30 42 Y H Y Y Trapper Creek Mi. 135.4 Parks Hwy K\u2019esugi Ken**, Mi. 135.4 Parks Hwy, Trapper Cr... 62.318930 -150.235580 27 Byers Lake Y $20 73 Y H Y Y Y Trapper Creek Mi. 147 Parks Hwy Byers Lake, Mi. 147 Parks Hwy, Trapper Creek, ... 62.744630 -150.135220 28 Denali View North Y $15 23 H H Y H Trapper Creek Mi. 162.7 Parks Hwy Denali View North, Mi. 162.7 Parks Hwy, Trappe... 62.318930 -150.235580 29 Denali View South Y $15 10 Y H Y Y Trapper Creek Mi. 134.8 Parks Hwy Denali View South, Mi. 134.8 Parks Hwy, Trappe... 62.318930 -150.235580 30 Lower Troublesome Creek Y $10 12 Y Y Y Y Y Trapper Creek Mi. 137.2 Parks Hwy Y Lower Troublesome Creek, Mi. 137.2 Parks Hwy, ... 62.318930 -150.235580 31 Hatcher Pass Y Hatcher Pass, , , AK, USA 61.769720 -149.308890 32 South Rolly Lake Y $20 97 Y H Y Y Y Willow Mi. 6.5 Nancy Lake Pkwy. South Rolly Lake, Mi. 6.5 Nancy Lake Pkwy., Wi... 61.750860 -150.045090 33 Willow Creek Y $15 131 H Y Y Y Y Willow Mi. 70.8 Parks Hwy. Willow Creek, Mi. 70.8 Parks Hwy., Willow, AK,... 61.750860 -150.045090 34 Chilkat SP Y $15 35 Y H Y Y Y Haines Mi. 7 Mud Bay Rd. Chilkat SP, Mi. 7 Mud Bay Rd., Haines, AK, USA 59.228457 -135.445365 35 Chilkoot Lake SRS Y $15 32 Y H Y Y Haines Mi. 10 Lutak Rd Chilkoot Lake SRS, Mi. 10 Lutak Rd, Haines, AK... 59.283838 -135.475300 36 Eagle Beach SRA Y $15 16 H H H H Y Juneau Mi. 29 Glacier Hwy. Y Eagle Beach SRA, Mi. 29 Glacier Hwy., Juneau, ... 58.314719 -134.445896 37 Settler\u2019s Cove SRA Y $15 13 H H H H Y Ketchikan Mi. 18 N. Tongass Hwy. Settler\u2019s Cove SRA, Mi. 18 N. Tongass Hwy., Ke... 55.464094 -131.805873 <pre><code># df[pd.isnull(df.lat)]\n</code></pre> <pre><code>def format_popup(row):\n    format_str=f\"\"\"\n    &lt;b&gt;{row['Campground']}&lt;/b&gt;&lt;br/&gt;\n    {row['Address']}&lt;br/&gt;\n    \"\"\"\n    return format_str\n</code></pre> <pre><code>anchorage = geocode('Anchorage, Alaska, USA')\nmap2 = folium.Map(location=(anchorage[0], anchorage[1]), zoom_start=5, control_scale=True)\n# add marker one by one on the map\nfor idx, row in df.iterrows():\n    if not pd.isnull(row['lat']) and not pd.isnull(row['lng']):\n        folium.Marker(\n            location=[row['lat'], row['lng']],\n            popup=format_popup(row),\n        ).add_to(map2)\n</code></pre> <pre><code>map2\n</code></pre> Make this Notebook Trusted to load map: File -&gt; Trust Notebook"},{"location":"Misc/Alaska_RV_Parks_Map/#recommended-rv-parks-dataset","title":"Recommended RV Parks Dataset","text":"<p>Reference: https://www.greatalaskanholidays.com/trip-planning/recommended-campgrounds/</p>"},{"location":"Misc/Alaska_RV_Parks_Map/#setup","title":"Setup","text":""},{"location":"Misc/Alaska_RV_Parks_Map/#scrape-data","title":"Scrape Data","text":""},{"location":"Misc/Alaska_RV_Parks_Map/#dataframes","title":"DataFrames","text":"<ol> <li>Privately Owned Campgrounds</li> <li>State of Alaska Campgrounds</li> </ol>"},{"location":"Misc/Alaska_RV_Parks_Map/#geocode-private-campgrounds","title":"Geocode Private Campgrounds","text":""},{"location":"Misc/Alaska_RV_Parks_Map/#map-of-privately-owned-campgrounds","title":"Map of Privately Owned Campgrounds","text":""},{"location":"Misc/Alaska_RV_Parks_Map/#state-of-alaska-campgrounds","title":"State of Alaska Campgrounds","text":""},{"location":"Misc/Alaska_RV_Parks_Map/#map-of-state-of-alaska-campgrounds","title":"Map of State of Alaska Campgrounds","text":""},{"location":"Misc/airline_baggage_delay/","title":"Airline Baggage Delay Compensation \u2013 Know Your Rights","text":"<p>If your checked baggage is delayed, the U.S. Department of Transportation (DOT) requires airlines to compensate you for reasonable expenses. Here are the key points:</p>"},{"location":"Misc/airline_baggage_delay/#your-rights-under-dot-rules","title":"\u2705 Your Rights Under DOT Rules","text":"<ul> <li>Reimbursement Required: Airlines must cover reasonable and verifiable expenses you incur while your bag is delayed.</li> <li>No Arbitrary Daily Limits: Airlines cannot impose a fixed daily cap (e.g., \\$50/day).</li> <li>Maximum Liability:  </li> <li>As of March 2025, the maximum liability is \\$4,700 per ticketed passenger for domestic flights.</li> <li>Qualifying Expenses:  </li> <li>Clothing, shoes, toiletries, and other essentials you need because your bag is missing.</li> </ul>"},{"location":"Misc/airline_baggage_delay/#how-to-claim","title":"\u2705 How to Claim","text":"<ol> <li>File a Property Irregularity Report (PIR) at the airport before leaving.</li> <li>Keep All Receipts for purchases made during the delay.</li> <li>Submit a Claim to the airline with receipts attached.</li> <li>Reference DOT Rules if the airline tries to limit reimbursement.</li> </ol>"},{"location":"Misc/airline_baggage_delay/#key-regulation","title":"\u2705 Key Regulation","text":"<p>DOT regulations mandate reimbursement for reasonable expenses during baggage delays, up to the maximum liability limit of \\$4,700 for domestic travel.</p>"},{"location":"Misc/airline_baggage_delay/#pro-tip","title":"\ud83d\udca1 Pro Tip","text":"<p>Always check the latest DOT guidelines before traveling: DOT Consumer Protection</p>"},{"location":"Misc/airline_baggage_delay/#references","title":"\ud83d\udcda References","text":"<ul> <li>Video: Know Your Rights When Airline Delays Your Bag </li> <li>Federal Register: Periodic Revisions to Domestic Baggage Liability Limits </li> <li>Eckert Seamans Aviation Blog \u2013 Revised Liability Limits </li> </ul> <p>Tags: #TravelTips #AirlineRights #DOT #BaggageDelay</p>"},{"location":"PySpark/Install_With_Docker_Compose/","title":"PySpark Cluster Setup with Docker Compose","text":"<p>Download <sup>1</sup> Bitnami docker-compose file.</p> <pre><code>curl -LO https://raw.githubusercontent.com/bitnami/bitnami-docker-spark/master/docker-compose.yml\n</code></pre>"},{"location":"PySpark/Install_With_Docker_Compose/#start-cluster","title":"Start Cluster","text":"<pre><code>docker-compose up\n</code></pre>"},{"location":"PySpark/Install_With_Docker_Compose/#check-spark-dashboard","title":"Check Spark Dashboard","text":"<p>Spark UI</p> <ol> <li> <p>Bitnami Spark docker-compose \u21a9</p> </li> </ol>"},{"location":"PySpark/spark_df_shape/","title":"PySpark custom shape function","text":"<p>Custom <code>df.shape()</code> function for PySpark dataframe.</p> <pre><code>import pyspark\n\ndef spark_shape(self) :\n    return (self.count(), len(self.columns) )\n\npyspark.sql.dataframe.DataFrame.shape = spark_shape\n\ndf.shape()\n</code></pre> <p>Just reminder that <code>.count()</code> could be very slow for large tables. </p>"},{"location":"PyTorch/gradients/","title":"Gradients","text":"<pre><code># PyTorch Gradients\n</code></pre> <p>This section covers the PyTorch autograd implementation of gradient descent. Tools include: * torch.autograd.backward() * torch.autograd.grad()</p> <p>Before continuing in this section, be sure to watch the theory lectures to understand the following concepts: * Error functions (step and sigmoid) * One-hot encoding * Maximum likelihood * Cross entropy (including multi-class cross entropy) * Back propagation (backprop)</p> Additional Resources: PyTorch Notes: Autograd mechanics <pre><code>import torch\n</code></pre> <pre><code>x = torch.tensor(2.0, requires_grad=True)\n</code></pre> <pre><code>y = 2*x**4 + x**3 + 3*x**2 + 5*x + 1\n\nprint(y)\n</code></pre> <pre>\n<code>tensor(63., grad_fn=&lt;AddBackward0&gt;)\n</code>\n</pre> <p>Since $y$ was created as a result of an operation, it has an associated gradient function accessible as y.grad_fn The calculation of $y$ is done as:</p> <p>$\\quad y=2(2)^4+(2)^3+3(2)^2+5(2)+1 = 32+8+12+10+1 = 63$</p> <p>This is the value of $y$ when $x=2$.</p> <pre><code>y.backward()\n</code></pre> <pre><code>print(x.grad)\n</code></pre> <pre>\n<code>tensor(93.)\n</code>\n</pre> <p>Note that x.grad is an attribute of tensor $x$, so we don't use parentheses. The computation is the result of</p> <p>$\\quad y'=8(2)^3+3(2)^2+6(2)+5 = 64+12+12+5 = 93$</p> <p>This is the slope of the polynomial at the point $(2,63)$.</p> <pre><code>x = torch.tensor([[1.,2,3],[3,2,1]], requires_grad=True)\nprint(x)\n</code></pre> <pre>\n<code>tensor([[1., 2., 3.],\n        [3., 2., 1.]], requires_grad=True)\n</code>\n</pre> <pre><code>y = 3*x + 2\nprint(y)\n</code></pre> <pre>\n<code>tensor([[ 5.,  8., 11.],\n        [11.,  8.,  5.]], grad_fn=&lt;AddBackward0&gt;)\n</code>\n</pre> <pre><code>z = 2*y**2\nprint(z)\n</code></pre> <pre>\n<code>tensor([[ 50., 128., 242.],\n        [242., 128.,  50.]], grad_fn=&lt;MulBackward0&gt;)\n</code>\n</pre> <pre><code>out = z.mean()\nprint(out)\n</code></pre> <pre>\n<code>tensor(140., grad_fn=&lt;MeanBackward1&gt;)\n</code>\n</pre> <pre><code>out.backward()\nprint(x.grad)\n</code></pre> <pre>\n<code>tensor([[10., 16., 22.],\n        [22., 16., 10.]])\n</code>\n</pre> <p>You should see a 2x3 matrix. If we call the final out tensor \"$o$\", we can calculate the partial derivative of $o$ with respect to $x_i$ as follows:</p> <p>$o = \\frac {1} {6}\\sum_{i=1}^{6} z_i$</p> <p>$z_i = 2(y_i)^2 = 2(3x_i+2)^2$</p> <p>To solve the derivative of $z_i$ we use the chain rule, where the derivative of $f(g(x)) = f'(g(x))g'(x)$</p> <p>In this case</p> <p>$\\begin{split} f(g(x)) &amp;= 2(g(x))^2, \\quad &amp;f'(g(x)) = 4g(x) \\ g(x) &amp;= 3x+2, &amp;g'(x) = 3 \\ \\frac {dz} {dx} &amp;= 4g(x)\\times 3 &amp;= 12(3x+2) \\end{split}$</p> <p>Therefore,</p> <p>$\\frac{\\partial o}{\\partial x_i} = \\frac{1}{6}\\times 12(3x+2)$</p> <p>$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = 2(3(1)+2) = 10$</p> <p>$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=2} = 2(3(2)+2) = 16$</p> <p>$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=3} = 2(3(3)+2) = 22$</p> A NOTE ABOUT TENSORS AND VARIABLES: Prior to PyTorch v0.4.0 (April 2018) Tensors (torch.Tensor) only held data, and tracking history was reserved for the Variable wrapper (torch.autograd.Variable). Since v0.4.0 tensors and variables have merged, and tracking functionality is now available through the requires_grad=True flag. <p></p>"},{"location":"PyTorch/gradients/#autograd-automatic-differentiation","title":"Autograd - Automatic Differentiation","text":"<p>In previous sections we created tensors and performed a variety of operations on them, but we did nothing to store the sequence of operations, or to apply the derivative of a completed function.</p> <p>In this section we'll introduce the concept of the dynamic computational graph which is comprised of all the Tensor objects in the network, as well as the Functions used to create them. Note that only the input Tensors we create ourselves will not have associated Function objects.</p> <p>The PyTorch autograd package provides automatic differentiation for all operations on Tensors. This is because operations become attributes of the tensors themselves. When a Tensor's .requires_grad attribute is set to True, it starts to track all operations on it. When an operation finishes you can call .backward() and have all the gradients computed automatically. The gradient for a tensor will be accumulated into its .grad attribute.</p> <p>Let's see this in practice.</p>"},{"location":"PyTorch/gradients/#back-propagation-on-one-step","title":"Back-propagation on one step","text":"<p>We'll start by applying a single polynomial function $y = f(x)$ to tensor $x$. Then we'll backprop and print the gradient $\\frac {dy} {dx}$.</p> <p>$\\begin{split}Function:\\quad y &amp;= 2x^4 + x^3 + 3x^2 + 5x + 1 \\ Derivative:\\quad y' &amp;= 8x^3 + 3x^2 + 6x + 5\\end{split}$</p>"},{"location":"PyTorch/gradients/#step-1-perform-standard-imports","title":"Step 1. Perform standard imports","text":""},{"location":"PyTorch/gradients/#step-2-create-a-tensor-with-requires_grad-set-to-true","title":"Step 2. Create a tensor with requires_grad set to True","text":"<p>This sets up computational tracking on the tensor.</p>"},{"location":"PyTorch/gradients/#step-3-define-a-function","title":"Step 3. Define a function","text":""},{"location":"PyTorch/gradients/#step-4-backprop","title":"Step 4. Backprop","text":""},{"location":"PyTorch/gradients/#step-5-display-the-resulting-gradient","title":"Step 5. Display the resulting gradient","text":""},{"location":"PyTorch/gradients/#back-propagation-on-multiple-steps","title":"Back-propagation on multiple steps","text":"<p>Now let's do something more complex, involving layers $y$ and $z$ between $x$ and our output layer $out$.</p>"},{"location":"PyTorch/gradients/#1-create-a-tensor","title":"1. Create a tensor","text":""},{"location":"PyTorch/gradients/#2-create-the-first-layer-with-y-3x2","title":"2. Create the first layer with $y = 3x+2$","text":""},{"location":"PyTorch/gradients/#3-create-the-second-layer-with-z-2y2","title":"3. Create the second layer with $z = 2y^2$","text":""},{"location":"PyTorch/gradients/#4-set-the-output-to-be-the-matrix-mean","title":"4. Set the output to be the matrix mean","text":""},{"location":"PyTorch/gradients/#5-now-perform-back-propagation-to-find-the-gradient-of-x-wrt-out","title":"5. Now perform back-propagation to find the gradient of x w.r.t out","text":"<p>(If you haven't seen it before, w.r.t. is an abbreviation of with respect to)</p>"},{"location":"PyTorch/gradients/#turn-off-tracking","title":"Turn off tracking","text":"<p>There may be times when we don't want or need to track the computational history.</p> <p>You can reset a tensor's requires_grad attribute in-place using <code>.requires_grad_(True)</code> (or False) as needed.</p> <p>When performing evaluations, it's often helpful to wrap a set of operations in <code>with torch.no_grad():</code></p> <p>A less-used method is to run <code>.detach()</code> on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor.</p>"},{"location":"PyTorch/tensor_basics/","title":"Tensor basics","text":"<p>This section covers: * Converting NumPy arrays to PyTorch tensors * Creating tensors from scratch</p> <pre><code>import torch\nimport numpy as np\n</code></pre> <p>Confirm you're using PyTorch version 1.1.0+</p> <pre><code>torch.__version__\n</code></pre> <pre>\n<code>'1.5.0+cu101'</code>\n</pre> <pre><code>arr = np.array([1,2,3,4,5])\nprint(arr)\nprint(arr.dtype)\nprint(type(arr))\n</code></pre> <pre>\n<code>[1 2 3 4 5]\nint64\n&lt;class 'numpy.ndarray'&gt;\n</code>\n</pre> <pre><code>x = torch.from_numpy(arr)\n# Equivalent to x = torch.as_tensor(arr)\n\nprint(x)\n</code></pre> <pre>\n<code>tensor([1, 2, 3, 4, 5])\n</code>\n</pre> <pre><code># Print the type of data held by the tensor\nprint(x.dtype)\n</code></pre> <pre>\n<code>torch.int64\n</code>\n</pre> <pre><code># Print the tensor object type\nprint(type(x))\nprint(x.type()) # this is more specific!\n</code></pre> <pre>\n<code>&lt;class 'torch.Tensor'&gt;\ntorch.LongTensor\n</code>\n</pre> <pre><code>arr2 = np.arange(0.,12.).reshape(4,3)\nprint(arr2)\n</code></pre> <pre>\n<code>[[ 0.  1.  2.]\n [ 3.  4.  5.]\n [ 6.  7.  8.]\n [ 9. 10. 11.]]\n</code>\n</pre> <pre><code>x2 = torch.from_numpy(arr2)\nprint(x2)\nprint(x2.type())\n</code></pre> <pre>\n<code>tensor([[ 0.,  1.,  2.],\n        [ 3.,  4.,  5.],\n        [ 6.,  7.,  8.],\n        [ 9., 10., 11.]], dtype=torch.float64)\ntorch.DoubleTensor\n</code>\n</pre> <p>Here torch.DoubleTensor refers to 64-bit floating point data.</p> <p>torch.from_numpy() torch.as_tensor() torch.tensor()</p> <p>There are a number of different functions available for creating tensors. When using torch.from_numpy() and torch.as_tensor(), the PyTorch tensor and the source NumPy array share the same memory. This means that changes to one affect the other. However, the torch.tensor() function always makes a copy.</p> <pre><code># Using torch.from_numpy(), shares same memory\narr = np.arange(0,5)\nt = torch.from_numpy(arr)\nprint(t)\n</code></pre> <pre>\n<code>tensor([0, 1, 2, 3, 4])\n</code>\n</pre> <pre><code>arr[2]=77\nprint(t)\n</code></pre> <pre>\n<code>tensor([ 0,  1, 77,  3,  4])\n</code>\n</pre> <pre><code># Using torch.tensor(), makes a copy\narr = np.arange(0,5)\nt = torch.tensor(arr)\nprint(t)\n</code></pre> <pre>\n<code>tensor([0, 1, 2, 3, 4])\n</code>\n</pre> <pre><code>arr[2]=77\nprint(t)\n</code></pre> <pre>\n<code>tensor([0, 1, 2, 3, 4])\n</code>\n</pre> <pre><code>data = np.array([1,2,3])\n</code></pre> <pre><code>a = torch.Tensor(data)  # Equivalent to cc = torch.FloatTensor(data)\nprint(a, a.type())\n</code></pre> <pre>\n<code>tensor([1., 2., 3.]) torch.FloatTensor\n</code>\n</pre> <pre><code>b = torch.tensor(data)\nprint(b, b.type())\n</code></pre> <pre>\n<code>tensor([1, 2, 3]) torch.LongTensor\n</code>\n</pre> <pre><code>c = torch.tensor(data, dtype=torch.long)\nprint(c, c.type())\n</code></pre> <pre>\n<code>tensor([1, 2, 3]) torch.LongTensor\n</code>\n</pre> <pre><code>x = torch.empty(4, 3)\nprint(x)\n</code></pre> <pre>\n<code>tensor([[4.4866e-36, 0.0000e+00, 3.3631e-44],\n        [0.0000e+00,        nan, 0.0000e+00],\n        [1.1578e+27, 1.1362e+30, 7.1547e+22],\n        [4.5828e+30, 1.2121e+04, 7.1846e+22]])\n</code>\n</pre> <pre><code>x = torch.zeros(4, 3, dtype=torch.int64)\nprint(x)\n</code></pre> <pre>\n<code>tensor([[0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0],\n        [0, 0, 0]])\n</code>\n</pre> <pre><code>x = torch.arange(0,18,2).reshape(3,3)\nprint(x)\n</code></pre> <pre>\n<code>tensor([[ 0,  2,  4],\n        [ 6,  8, 10],\n        [12, 14, 16]])\n</code>\n</pre> <pre><code>x = torch.linspace(0,18,12).reshape(3,4)\nprint(x)\n</code></pre> <pre>\n<code>tensor([[ 0.0000,  1.6364,  3.2727,  4.9091],\n        [ 6.5455,  8.1818,  9.8182, 11.4545],\n        [13.0909, 14.7273, 16.3636, 18.0000]])\n</code>\n</pre> <pre><code>x = torch.tensor([1, 2, 3, 4])\nprint(x)\nprint(x.dtype)\nprint(x.type())\n</code></pre> <pre>\n<code>tensor([1, 2, 3, 4])\ntorch.int64\ntorch.LongTensor\n</code>\n</pre> <pre><code># Converting type\nx = x.type(torch.int16)\nprint(x.dtype)\nprint(x.type())\n</code></pre> <pre>\n<code>torch.int16\ntorch.ShortTensor\n</code>\n</pre> <p>Alternatively you can set the type by the tensor method used. For a list of tensor types visit https://pytorch.org/docs/stable/tensors.html</p> <pre><code>x = torch.FloatTensor([5,6,7])\nprint(x)\nprint(x.dtype)\nprint(x.type())\n</code></pre> <pre>\n<code>tensor([5., 6., 7.])\ntorch.float32\ntorch.FloatTensor\n</code>\n</pre> <p>You can also pass the dtype in as an argument. For a list of dtypes visit https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.dtype</p> <pre><code>x = torch.tensor([8,9,-3], dtype=torch.int)\nprint(x)\nprint(x.dtype)\nprint(x.type())\n</code></pre> <pre>\n<code>tensor([ 8,  9, -3], dtype=torch.int32)\ntorch.int32\ntorch.IntTensor\n</code>\n</pre> <pre><code>print('Old:', x.type())\n\nx = x.type(torch.int64)\n\nprint('New:', x.type())\n</code></pre> <pre>\n<code>Old: torch.IntTensor\nNew: torch.LongTensor\n</code>\n</pre> <pre><code>x = torch.rand(4, 3)\nprint(x)\n</code></pre> <pre>\n<code>tensor([[0.6682, 0.3914, 0.5425],\n        [0.2000, 0.3056, 0.9103],\n        [0.7039, 0.5021, 0.9170],\n        [0.4305, 0.7270, 0.6577]])\n</code>\n</pre> <pre><code>x = torch.randn(4, 3)\nprint(x)\n</code></pre> <pre>\n<code>tensor([[ 0.4623, -0.2561, -0.5399],\n        [-0.6609, -0.6707,  0.6866],\n        [-0.9742, -0.3833,  0.1253],\n        [ 0.1251, -0.7600, -1.8088]])\n</code>\n</pre> <pre><code>x = torch.randint(0, 5, (4, 3))\nprint(x)\n</code></pre> <pre>\n<code>tensor([[0, 2, 2],\n        [3, 1, 2],\n        [3, 2, 1],\n        [2, 0, 4]])\n</code>\n</pre> <pre><code>x = torch.zeros(2,5)\nprint(x)\n</code></pre> <pre>\n<code>tensor([[0., 0., 0., 0., 0.],\n        [0., 0., 0., 0., 0.]])\n</code>\n</pre> <pre><code>x2 = torch.randn_like(x)\nprint(x2)\n</code></pre> <pre>\n<code>tensor([[-0.5288,  0.5442,  1.8976,  0.5154,  2.7177],\n        [-1.7115, -1.4005,  0.2681, -0.0782, -0.6214]])\n</code>\n</pre> <p>The same syntax can be used with torch.zeros_like(input) torch.ones_like(input)</p> <pre><code>x3 = torch.ones_like(x2)\nprint(x3)\n</code></pre> <pre>\n<code>tensor([[1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.]])\n</code>\n</pre> <pre><code>torch.manual_seed(42)\nx = torch.rand(2, 3)\nprint(x)\n</code></pre> <pre>\n<code>tensor([[0.8823, 0.9150, 0.3829],\n        [0.9593, 0.3904, 0.6009]])\n</code>\n</pre> <pre><code>torch.manual_seed(42)\nx = torch.rand(2, 3)\nprint(x)\n</code></pre> <pre>\n<code>tensor([[0.8823, 0.9150, 0.3829],\n        [0.9593, 0.3904, 0.6009]])\n</code>\n</pre> <pre><code>x.shape\n</code></pre> <pre>\n<code>torch.Size([2, 3])</code>\n</pre> <pre><code>x.size()  # equivalent to x.shape\n</code></pre> <pre>\n<code>torch.Size([2, 3])</code>\n</pre> <pre><code>x.device\n</code></pre> <pre>\n<code>device(type='cpu')</code>\n</pre> <p>PyTorch supports use of multiple devices, harnessing the power of one or more GPUs in addition to the CPU. We won't explore that here, but you should know that operations between tensors can only happen for tensors installed on the same device.</p> <pre><code>x.layout\n</code></pre> <pre>\n<code>torch.strided</code>\n</pre> <p>PyTorch has a class to hold the memory layout option. The default setting of strided will suit our purposes throughout the course.</p> <p></p>"},{"location":"PyTorch/tensor_basics/#pytorch-tensor-basics","title":"PyTorch Tensor Basics","text":""},{"location":"PyTorch/tensor_basics/#perform-standard-imports","title":"Perform standard imports","text":""},{"location":"PyTorch/tensor_basics/#converting-numpy-arrays-to-pytorch-tensors","title":"Converting NumPy arrays to PyTorch tensors","text":"<p>A torch.Tensor is a multi-dimensional matrix containing elements of a single data type. Calculations between tensors can only happen if the tensors share the same dtype. In some cases tensors are used as a replacement for NumPy to use the power of GPUs (more on this later).</p>"},{"location":"PyTorch/tensor_basics/#tensor-datatypes","title":"Tensor Datatypes","text":"TYPENAMEEQUIVALENTTENSOR TYPE 32-bit integer (signed)torch.int32torch.intIntTensor 64-bit integer (signed)torch.int64torch.longLongTensor 16-bit integer (signed)torch.int16torch.shortShortTensor 32-bit floating pointtorch.float32torch.floatFloatTensor 64-bit floating pointtorch.float64torch.doubleDoubleTensor 16-bit floating pointtorch.float16torch.halfHalfTensor 8-bit integer (signed)torch.int8CharTensor 8-bit integer (unsigned)torch.uint8ByteTensor"},{"location":"PyTorch/tensor_basics/#copying-vs-sharing","title":"Copying vs. sharing","text":""},{"location":"PyTorch/tensor_basics/#class-constructors","title":"Class constructors","text":"<p>torch.Tensor() torch.FloatTensor() torch.LongTensor(), etc.</p> <p>There's a subtle difference between using the factory function torch.tensor(data) and the class constructor torch.Tensor(data). The factory function determines the dtype from the incoming data, or from a passed-in dtype argument. The class constructor torch.Tensor()is simply an alias for torch.FloatTensor(data). Consider the following:</p>"},{"location":"PyTorch/tensor_basics/#creating-tensors-from-scratch","title":"Creating tensors from scratch","text":""},{"location":"PyTorch/tensor_basics/#uninitialized-tensors-with-empty","title":"Uninitialized tensors with .empty()","text":"<p>torch.empty() returns an uninitialized tensor. Essentially a block of memory is allocated according to the size of the tensor, and any values already sitting in the block are returned. This is similar to the behavior of numpy.empty().</p>"},{"location":"PyTorch/tensor_basics/#initialized-tensors-with-zeros-and-ones","title":"Initialized tensors with .zeros() and .ones()","text":"<p>torch.zeros(size) torch.ones(size) It's a good idea to pass in the intended dtype.</p>"},{"location":"PyTorch/tensor_basics/#tensors-from-ranges","title":"Tensors from ranges","text":"<p>torch.arange(start,end,step) torch.linspace(start,end,steps) Note that with .arange(), end is exclusive, while with linspace(), end is inclusive.</p>"},{"location":"PyTorch/tensor_basics/#tensors-from-data","title":"Tensors from data","text":"<p>torch.tensor() will choose the dtype based on incoming data:</p>"},{"location":"PyTorch/tensor_basics/#changing-the-dtype-of-existing-tensors","title":"Changing the dtype of existing tensors","text":"<p>Don't be tempted to use x = torch.tensor(x, dtype=torch.type) as it will raise an error about improper use of tensor cloning. Instead, use the tensor .type() method.</p>"},{"location":"PyTorch/tensor_basics/#random-number-tensors","title":"Random number tensors","text":"<p>torch.rand(size) returns random samples from a uniform distribution over [0, 1) torch.randn(size) returns samples from the \"standard normal\" distribution [\u03c3 = 1] \u00a0\u00a0\u00a0\u00a0Unlike rand which is uniform, values closer to zero are more likely to appear. torch.randint(low,high,size) returns random integers from low (inclusive) to high (exclusive)</p>"},{"location":"PyTorch/tensor_basics/#random-number-tensors-that-follow-the-input-size","title":"Random number tensors that follow the input size","text":"<p>torch.rand_like(input) torch.randn_like(input) torch.randint_like(input,low,high) these return random number tensors with the same size as input</p>"},{"location":"PyTorch/tensor_basics/#setting-the-random-seed","title":"Setting the random seed","text":"<p>torch.manual_seed(int) is used to obtain reproducible results</p>"},{"location":"PyTorch/tensor_basics/#tensor-attributes","title":"Tensor attributes","text":"<p>Besides dtype, we can look at other tensor attributes like shape, device and layout</p>"},{"location":"PyTorch/tensor_operations/","title":"Tensor operations","text":"<p>This section covers: * Indexing and slicing * Reshaping tensors (tensor views) * Tensor arithmetic and basic operations * Dot products * Matrix multiplication * Additional, more advanced operations</p> <pre><code>import torch\nimport numpy as np\n</code></pre> <pre><code>x = torch.arange(6).reshape(3,2)\nprint(x)\n</code></pre> <pre>\n<code>tensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n</code>\n</pre> <pre><code># Grabbing the right hand column values\nx[:,1]\n</code></pre> <pre>\n<code>tensor([1, 3, 5])</code>\n</pre> <pre><code># Grabbing the right hand column as a (3,1) slice\nx[:,1:]\n</code></pre> <pre>\n<code>tensor([[1],\n        [3],\n        [5]])</code>\n</pre> <pre><code>x = torch.arange(12)\nprint(x)\n</code></pre> <pre>\n<code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n</code>\n</pre> <pre><code>x.view(2,6)\n</code></pre> <pre>\n<code>tensor([[ 0,  1,  2,  3,  4,  5],\n        [ 6,  7,  8,  9, 10, 11]])</code>\n</pre> <pre><code>x.view(6,2)\n</code></pre> <pre>\n<code>tensor([[ 0,  1],\n        [ 2,  3],\n        [ 4,  5],\n        [ 6,  7],\n        [ 8,  9],\n        [10, 11]])</code>\n</pre> <pre><code># x is unchanged\nx\n</code></pre> <pre>\n<code>tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])</code>\n</pre> <pre><code>z = x.view(2,6)\nx[0]=234\nprint(z)\n</code></pre> <pre>\n<code>tensor([[234,   1,   2,   3,   4,   5],\n        [  6,   7,   8,   9,  10,  11]])\n</code>\n</pre> <pre><code># infer number of columns for given rows\nx.view(2,-1)\n</code></pre> <pre>\n<code>tensor([[234,   1,   2,   3,   4,   5],\n        [  6,   7,   8,   9,  10,  11]])</code>\n</pre> <pre><code># infer number of rows for given columns\nx.view(-1,3)\n</code></pre> <pre>\n<code>tensor([[234,   1,   2],\n        [  3,   4,   5],\n        [  6,   7,   8],\n        [  9,  10,  11]])</code>\n</pre> <pre><code>x.view_as(z)\n</code></pre> <pre>\n<code>tensor([[234,   1,   2,   3,   4,   5],\n        [  6,   7,   8,   9,  10,  11]])</code>\n</pre> <pre><code>a = torch.tensor([1,2,3], dtype=torch.float)\nb = torch.tensor([4,5,6], dtype=torch.float)\nprint(a + b)\n</code></pre> <pre>\n<code>tensor([5., 7., 9.])\n</code>\n</pre> <p>As arguments passed into a torch operation:</p> <pre><code>print(torch.add(a, b))\n</code></pre> <pre>\n<code>tensor([5., 7., 9.])\n</code>\n</pre> <p>With an output tensor passed in as an argument:</p> <pre><code>result = torch.empty(3)\ntorch.add(a, b, out=result)  # equivalent to result=torch.add(a,b)\nprint(result)\n</code></pre> <pre>\n<code>tensor([5., 7., 9.])\n</code>\n</pre> <p>Changing a tensor in-place with _</p> <pre><code>a.add_(b)  # equivalent to a=torch.add(a,b)\nprint(a)\n</code></pre> <pre>\n<code>tensor([5., 7., 9.])\n</code>\n</pre> NOTE: Any operation that changes a tensor in-place is post-fixed with an underscore _.     In the above example: a.add_(b) changed a. Monomial Operations OPERATIONFUNCTIONDESCRIPTION |a|torch.abs(a)absolute value 1/atorch.reciprocal(a)reciprocal $\\sqrt{a}$torch.sqrt(a)square root log(a)torch.log(a)natural log e<sup>a</sup>torch.exp(a)exponential 12.34  ==&gt;  12.torch.trunc(a)truncated integer 12.34  ==&gt;  0.34torch.frac(a)fractional component Trigonometry OPERATIONFUNCTIONDESCRIPTION sin(a)torch.sin(a)sine cos(a)torch.sin(a)cosine tan(a)torch.sin(a)tangent arcsin(a)torch.asin(a)arc sine arccos(a)torch.acos(a)arc cosine arctan(a)torch.atan(a)arc tangent sinh(a)torch.sinh(a)hyperbolic sine cosh(a)torch.cosh(a)hyperbolic cosine tanh(a)torch.tanh(a)hyperbolic tangent Summary Statistics OPERATIONFUNCTIONDESCRIPTION $\\sum a$torch.sum(a)sum $\\bar a$torch.mean(a)mean a<sub>max</sub>torch.max(a)maximum a<sub>min</sub>torch.min(a)minimum torch.max(a,b) returns a tensor of size acontaining the element wise max between a and b NOTE: Most arithmetic operations require float values. Those that do work with integers return integer tensors. For example, torch.div(a,b) performs floor division (truncates the decimal) for integer types, and classic division for floats. <pre><code>a = torch.tensor([1,2,3], dtype=torch.float)\nb = torch.tensor([4,5,6], dtype=torch.float)\nprint(torch.add(a,b).sum())\n</code></pre> <pre>\n<code>tensor(21.)\n</code>\n</pre> <pre><code>a = torch.tensor([1,2,3], dtype=torch.float)\nb = torch.tensor([4,5,6], dtype=torch.float)\nprint(a.mul(b)) # for reference\nprint()\nprint(a.dot(b))\n</code></pre> <pre>\n<code>tensor([ 4., 10., 18.])\n\ntensor(32.)\n</code>\n</pre> NOTE: There's a slight difference between torch.dot() and numpy.dot(). While torch.dot() only accepts 1D arguments and returns a dot product, numpy.dot() also accepts 2D arguments and performs matrix multiplication. We show matrix multiplication below. <pre><code>a = torch.tensor([[0,2,4],[1,3,5]], dtype=torch.float)\nb = torch.tensor([[6,7],[8,9],[10,11]], dtype=torch.float)\n\nprint('a: ',a.size())\nprint('b: ',b.size())\nprint('a x b: ',torch.mm(a,b).size())\n</code></pre> <pre>\n<code>a:  torch.Size([2, 3])\nb:  torch.Size([3, 2])\na x b:  torch.Size([2, 2])\n</code>\n</pre> <pre><code>print(torch.mm(a,b))\n</code></pre> <pre>\n<code>tensor([[56., 62.],\n        [80., 89.]])\n</code>\n</pre> <pre><code>print(a.mm(b))\n</code></pre> <pre>\n<code>tensor([[56., 62.],\n        [80., 89.]])\n</code>\n</pre> <pre><code>print(a @ b)\n</code></pre> <pre>\n<code>tensor([[56., 62.],\n        [80., 89.]])\n</code>\n</pre> <pre><code>t1 = torch.randn(2, 3, 4)\nt2 = torch.randn(4, 5)\n</code></pre> <pre><code>t1\n</code></pre> <pre>\n<code>tensor([[[ 0.0495, -1.2814,  0.4144,  0.3883],\n         [-2.1511,  0.0932,  2.0666,  0.8509],\n         [ 0.4211, -2.1292,  0.9620, -1.6141]],\n\n        [[ 0.6840, -0.7749,  0.7027,  0.0369],\n         [-0.0445,  0.4145, -0.2296,  1.2467],\n         [ 0.2800, -1.7043,  0.2537,  0.1963]]])\n</code>\n</pre> <pre><code>t2\n</code></pre> <pre>\n<code>tensor([[ 1.9903,  0.3279, -0.2475,  0.5449,  0.0568],\n        [-0.5038, -0.0790, -0.1920,  0.1574, -0.2723],\n        [ 0.1912,  0.8469, -1.7464,  1.1971,  2.7874],\n        [-0.8376,  0.5609,  0.8387,  1.5994,  0.0535]])</code>\n</pre> <pre><code>print(torch.matmul(t1, t2).size())\n</code></pre> <pre>\n<code>torch.Size([2, 3, 5])\n</code>\n</pre> <p>However, the same operation raises a RuntimeError with torch.mm():</p> <pre><code>print(torch.mm(t1, t2).size())\n</code></pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n&lt;ipython-input-46-edaac219da2b&gt; in &lt;module&gt;()\n----&gt; 1 print(torch.mm(t1, t2).size())\n\nRuntimeError: matrices expected, got 3D, 2D tensors at /pytorch/aten/src/TH/generic/THTensorMath.cpp:36</pre> <pre><code>x = torch.tensor([2.,5.,8.,14.])\nx.norm()\n</code></pre> <pre>\n<code>tensor(17.)</code>\n</pre> <pre><code>x = torch.ones(3,7)\nx.numel()\n</code></pre> <pre>\n<code>21</code>\n</pre> <p>This can be useful in certain calculations like Mean Squared Error:  def mse(t1, t2): \u00a0\u00a0\u00a0\u00a0diff = t1 - t2     \u00a0\u00a0\u00a0\u00a0return torch.sum(diff * diff) / diff.numel()</p> <p></p>"},{"location":"PyTorch/tensor_operations/#pytorch-tensor-operations","title":"PyTorch Tensor Operations","text":""},{"location":"PyTorch/tensor_operations/#perform-standard-imports","title":"Perform standard imports","text":""},{"location":"PyTorch/tensor_operations/#indexing-and-slicing","title":"Indexing and slicing","text":"<p>Extracting specific values from a tensor works just the same as with NumPy arrays  Image source: http://www.scipy-lectures.org/_images/numpy_indexing.png</p>"},{"location":"PyTorch/tensor_operations/#reshape-tensors-with-view","title":"Reshape tensors with .view()","text":"<p>view() and reshape() do essentially the same thing by returning a reshaped tensor without changing the original tensor in place. There's a good discussion of the differences here.</p>"},{"location":"PyTorch/tensor_operations/#views-reflect-the-most-current-data","title":"Views reflect the most current data","text":""},{"location":"PyTorch/tensor_operations/#views-can-infer-the-correct-size","title":"Views can infer the correct size","text":"<p>By passing in -1 PyTorch will infer the correct value from the given tensor</p>"},{"location":"PyTorch/tensor_operations/#adopt-another-tensors-shape-with-view_as","title":"Adopt another tensor's shape with .view_as()","text":"<p>view_as(input) only works with tensors that have the same number of elements.</p>"},{"location":"PyTorch/tensor_operations/#tensor-arithmetic","title":"Tensor Arithmetic","text":"<p>Adding tensors can be performed a few different ways depending on the desired result.</p> <p>As a simple expression:</p>"},{"location":"PyTorch/tensor_operations/#basic-tensor-operations","title":"Basic Tensor Operations","text":"Arithmetic OPERATIONFUNCTIONDESCRIPTION a + ba.add(b)element wise addition a - ba.sub(b)subtraction a * ba.mul(b)multiplication a / ba.div(b)division a % ba.fmod(b)modulo (remainder after division) a<sup>b</sup>a.pow(b)power"},{"location":"PyTorch/tensor_operations/#use-the-space-below-to-experiment-with-different-operations","title":"Use the space below to experiment with different operations","text":""},{"location":"PyTorch/tensor_operations/#dot-products","title":"Dot products","text":"<p>A dot product is the sum of the products of the corresponding entries of two 1D tensors. If the tensors are both vectors, the dot product is given as:</p> <p>$\\begin{bmatrix} a &amp; b &amp; c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d &amp; e &amp; f \\end{bmatrix} = ad + be + cf$</p> <p>If the tensors include a column vector, then the dot product is the sum of the result of the multiplied matrices. For example: $\\begin{bmatrix} a &amp; b &amp; c \\end{bmatrix} \\;\\cdot\\; \\begin{bmatrix} d \\ e \\ f \\end{bmatrix} = ad + be + cf$ Dot products can be expressed as torch.dot(a,b) or <code>a.dot(b)</code> or <code>b.dot(a)</code></p>"},{"location":"PyTorch/tensor_operations/#matrix-multiplication","title":"Matrix multiplication","text":"<p>2D Matrix multiplication is possible when the number of columns in tensor A matches the number of rows in tensor B. In this case, the product of tensor A with size $(x,y)$ and tensor B with size $(y,z)$ results in a tensor of size $(x,z)$</p>   $\\begin{bmatrix} a &amp; b &amp; c \\\\ d &amp; e &amp; f \\end{bmatrix} \\;\\times\\; \\begin{bmatrix} m &amp; n \\\\ p &amp; q \\\\ r &amp; s \\end{bmatrix} = \\begin{bmatrix} (am+bp+cr) &amp; (an+bq+cs) \\\\ (dm+ep+fr) &amp; (dn+eq+fs) \\end{bmatrix}$ Image source: https://commons.wikimedia.org/wiki/File:Matrix_multiplication_diagram_2.svg <p>Matrix multiplication can be computed using torch.mm(a,b) or <code>a.mm(b)</code> or <code>a @ b</code></p>"},{"location":"PyTorch/tensor_operations/#matrix-multiplication-with-broadcasting","title":"Matrix multiplication with broadcasting","text":"<p>Matrix multiplication that involves broadcasting can be computed using torch.matmul(a,b) or <code>a.matmul(b)</code> or <code>a @ b</code></p>"},{"location":"PyTorch/tensor_operations/#advanced-operations","title":"Advanced operations","text":""},{"location":"PyTorch/tensor_operations/#l2-or-euclidian-norm","title":"L2 or Euclidian Norm","text":"<p>See torch.norm()</p> <p>The Euclidian Norm gives the vector norm of $x$ where $x=(x_1,x_2,...,x_n)$. It is calculated as</p> <p>${\\displaystyle \\left|{\\boldsymbol {x}}\\right|{2}:={\\sqrt {x$}^{2}+\\cdots +x_{n}^{2}}}</p> <p>When applied to a matrix, torch.norm() returns the Frobenius norm by default.</p>"},{"location":"PyTorch/tensor_operations/#number-of-elements","title":"Number of elements","text":"<p>See torch.numel()</p> <p>Returns the number of elements in a tensor.</p>"},{"location":"Python/Basics/ASCII/","title":"ASCII","text":"<p>ASCII abbreviated from American Standard Code for Information Interchange, is a character encoding standard for electronic communication. ASCII codes represent text in computers, telecommunications equipment, and other devices.</p> <p>Print ASCII value of character 'a'.</p> <pre><code>print(ord('a'))\n</code></pre> <pre>\n<code>97\n</code>\n</pre> <p>Print ASCII value of character 'A'.</p> <pre><code>print(ord('A'))\n</code></pre> <pre>\n<code>65\n</code>\n</pre> <p>Print character represented by ACII value 97.</p> <pre><code>print(chr(97))\n</code></pre> <pre>\n<code>a\n</code>\n</pre> <p>Print character represented by ACII value 65.</p> <pre><code>print(chr(65))\n</code></pre> <pre>\n<code>A\n</code>\n</pre> <p></p>"},{"location":"Python/Basics/ASCII/#python-ascii-encoding","title":"Python ASCII Encoding","text":""},{"location":"Python/Basics/ASCII/#ascii-value-of-a-character","title":"ASCII Value of a Character","text":""},{"location":"Python/Basics/ASCII/#character-from-ascii-value","title":"Character from ASCII Value","text":""},{"location":"Python/Basics/ISO_8601/","title":"ISO 8601","text":"<p>ISO 8601 Data elements and interchange formats \u2013 Information interchange \u2013 Representation of dates and times is an international standard covering the exchange of date- and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988.</p> <pre><code>from datetime import datetime, timezone, tzinfo\n</code></pre> <pre><code># UTC time now\nprint(datetime.utcnow().replace(microsecond=0).isoformat())\n</code></pre> <pre>\n<code>2020-05-12T16:16:06\n</code>\n</pre> <pre><code># UTC time now with timezone info\nprint(datetime.now().astimezone(tz=timezone.utc).replace(microsecond=0).isoformat())\n</code></pre> <pre>\n<code>2020-05-12T16:16:06+00:00\n</code>\n</pre> <pre><code># Local time now with timezone info\nprint(datetime.now().astimezone().replace(microsecond=0).isoformat())\n</code></pre> <pre>\n<code>2020-05-12T12:16:06-04:00\n</code>\n</pre> <p></p>"},{"location":"Python/Basics/ISO_8601/#iso-8601-timestamp-in-python","title":"ISO 8601 Timestamp in Python","text":""},{"location":"Python/Basics/ISO_8601/#utc-time","title":"UTC Time","text":""},{"location":"Python/Basics/ISO_8601/#local-time","title":"Local Time","text":""},{"location":"Python/Basics/OrderedDict_LRU_Cache/","title":"OrderedDict LRU Cache","text":"<p>ISO 8601 Data elements and interchange formats \u2013 Information interchange \u2013 Representation of dates and times is an international standard covering the exchange of date- and time-related data. It was issued by the International Organization for Standardization (ISO) and was first published in 1988.</p> <pre><code>from datetime import datetime, timezone, tzinfo\n</code></pre> <pre><code># UTC time now\nprint(datetime.utcnow().replace(microsecond=0).isoformat())\n</code></pre> <pre>\n<code>2020-05-12T16:16:06\n</code>\n</pre> <pre><code># UTC time now with timezone info\nprint(datetime.now().astimezone(tz=timezone.utc).replace(microsecond=0).isoformat())\n</code></pre> <pre>\n<code>2020-05-12T16:16:06+00:00\n</code>\n</pre> <pre><code># Local time now with timezone info\nprint(datetime.now().astimezone().replace(microsecond=0).isoformat())\n</code></pre> <pre>\n<code>2020-05-12T12:16:06-04:00\n</code>\n</pre> <p></p>"},{"location":"Python/Basics/OrderedDict_LRU_Cache/#iso-8601-timestamp-in-python","title":"ISO 8601 Timestamp in Python","text":""},{"location":"Python/Basics/OrderedDict_LRU_Cache/#utc-time","title":"UTC Time","text":""},{"location":"Python/Basics/OrderedDict_LRU_Cache/#local-time","title":"Local Time","text":""},{"location":"Python/Basics/pip_package_size/","title":"Package Sizes Installed with <code>pip</code>","text":"<p>List of <code>pip</code> packages and the sizes.</p>"},{"location":"Python/Basics/pip_package_size/#ascending-order","title":"Ascending Order","text":"<pre><code>pip list --format freeze | \\\nawk -F = {'print $1'} | \\\nxargs pip3 show | \\\ngrep -E 'Location:|Name:' | \\\ncut -d ' ' -f 2 | \\\npaste -d ' ' - - | \\\nawk '{print $2 \"/\" tolower($1)}' | \\\nxargs du -sh 2&gt; /dev/null | \\\nsort -h\n</code></pre>"},{"location":"Python/Basics/pip_package_size/#descending-order","title":"Descending Order","text":"<pre><code>pip list --format freeze | \\\nawk -F = {'print $1'} | \\\nxargs pip3 show | \\\ngrep -E 'Location:|Name:' | \\\ncut -d ' ' -f 2 | \\\npaste -d ' ' - - | \\\nawk '{print $2 \"/\" tolower($1)}' | \\\nxargs du -sh 2&gt; /dev/null | \\\nsort -hr\n</code></pre>"},{"location":"Python/Deep_Learning/Sentiment_analysis/","title":"Sentiment analysis","text":"<pre><code>from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Load pre-trained BERT tokenizer and model\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=4)\n\n# Split the data\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    [d['text'] for d in data], [0, 1, 2, 3], test_size=0.2, random_state=42\n)\n\n# Tokenize the data\ntrain_encodings = tokenizer(train_texts, truncation=True, padding=True)\nval_encodings = tokenizer(val_texts, truncation=True, padding=True)\n\n# Convert to torch tensors\nclass Dataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)\n\ntrain_dataset = Dataset(train_encodings, train_labels)\nval_dataset = Dataset(val_encodings, val_labels)\n\n# Define training arguments\ntraining_args = TrainingArguments(output_dir='./results', num_train_epochs=3, per_device_train_batch_size=2)\n\n# Create Trainer instance\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset)\n\n# Train the model\ntrainer.train()\n\n# Evaluate the model\npredictions = trainer.predict(val_dataset)\npred_labels = predictions.predictions.argmax(axis=1)\n\n# Print classification report\nprint(classification_report(val_labels, pred_labels, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n</code></pre> <pre><code>from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Tokenize the data\ntokenizer = Tokenizer(num_words=1000)\ntokenizer.fit_on_texts([d['text'] for d in data])\n\n# Split the data\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    [d['text'] for d in data], [0, 1, 2, 3], test_size=0.2, random_state=42\n)\n\n# Tokenize and pad the data\ntrain_sequences = tokenizer.texts_to_sequences(train_texts)\nval_sequences = tokenizer.texts_to_sequences(val_texts)\ntrain_padded = pad_sequences(train_sequences, maxlen=50)\nval_padded = pad_sequences(val_sequences, maxlen=50)\n\n# Define the LSTM model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=1000, output_dim=64, input_length=50))\nmodel.add(LSTM(64))\nmodel.add(Dense(4, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(train_padded, train_labels, epochs=5, batch_size=2, validation_data=(val_padded, val_labels))\n\n# Evaluate the model\npred_labels = model.predict(val_padded).argmax(axis=1)\n\n# Print classification report\nprint(classification_report(val_labels, pred_labels, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n</code></pre> <pre><code>from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, SimpleRNN, Dense\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Tokenize the data\ntokenizer = Tokenizer(num_words=1000)\ntokenizer.fit_on_texts([d['text'] for d in data])\n\n# Split the data\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(\n    [d['text'] for d in data], [0, 1, 2, 3], test_size=0.2, random_state=42\n)\n\n# Tokenize and pad the data\ntrain_sequences = tokenizer.texts_to_sequences(train_texts)\nval_sequences = tokenizer.texts_to_sequences(val_texts)\ntrain_padded = pad_sequences(train_sequences, maxlen=50)\nval_padded = pad_sequences(val_sequences, maxlen=50)\n\n# Define the RNN model\nmodel = Sequential()\nmodel.add(Embedding(input_dim=1000, output_dim=64, input_length=50))\nmodel.add(SimpleRNN(64))\nmodel.add(Dense(4, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# Train the model\nmodel.fit(train_padded, train_labels, epochs=5, batch_size=2, validation_data=(val_padded, val_labels))\n\n# Evaluate the model\npred_labels = model.predict(val_padded).argmax(axis=1)\n\n# Print classification report\nprint(classification_report(val_labels, pred_labels, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n</code></pre> <pre><code>data = [\n    {\"text\": \"I loved working here, but I need to move to a new city.\", \"sentiment\": \"positive\"},\n    {\"text\": \"The work environment was toxic and stressful.\", \"sentiment\": \"negative\"},\n    {\"text\": \"It was an okay experience, nothing special.\", \"sentiment\": \"neutral\"},\n    {\"text\": \"I am unsure about my feelings towards this job.\", \"sentiment\": \"ambiguous\"}\n]\n</code></pre> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom gensim.models import Word2Vec\nimport numpy as np\nimport xgboost as xgb\nfrom sklearn.metrics import classification_report\n\n# Convert data to DataFrame\ndf = pd.DataFrame(data)\n\n# Encode labels\nlabel_mapping = {'positive': 0, 'negative': 1, 'neutral': 2, 'ambiguous': 3}\ndf['label'] = df['sentiment'].map(label_mapping)\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n\n# Tokenize text data\nX_train_tokens = [text.split() for text in X_train]\nX_val_tokens = [text.split() for text in X_val]\n\n# Train Word2Vec model\nword2vec_model = Word2Vec(sentences=X_train_tokens, vector_size=100, window=5, min_count=1, workers=4)\n\n# Function to average word vectors for a document\ndef document_vector(doc):\n    doc = [word for word in doc if word in word2vec_model.wv.index_to_key]\n    return np.mean(word2vec_model.wv[doc], axis=0)\n\n# Create feature vectors\nX_train_vec = np.array([document_vector(doc) for doc in X_train_tokens])\nX_val_vec = np.array([document_vector(doc) for doc in X_val_tokens])\n\n# Convert to DMatrix\ndtrain = xgb.DMatrix(X_train_vec, label=y_train)\ndval = xgb.DMatrix(X_val_vec, label=y_val)\n\n# Set parameters\nparams = {\n    'objective': 'multi:softmax',\n    'num_class': 4,\n    'eval_metric': 'mlogloss'\n}\n\n# Train the model\nbst = xgb.train(params, dtrain, num_boost_round=100)\n\n# Predict and evaluate\ny_pred = bst.predict(dval)\nprint(\"XGBoost Classification Report:\")\nprint(classification_report(y_val, y_pred, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n</code></pre>"},{"location":"Python/Deep_Learning/Sentiment_analysis/#sentiment-analysis","title":"Sentiment Analysis","text":"<p>Sentiment analysis is a natural language processing technique used to determine whether data is positive, negative or neutral. Sentiment analysis is often performed on textual data to help businesses monitor brand and product sentiment in customer feedback, and understand customer needs.</p>"},{"location":"Python/Deep_Learning/Sentiment_analysis/#sample-data","title":"Sample Data","text":"<pre><code>data = [\n    {\"text\": \"I loved working here, but I need to move to a new city.\", \"sentiment\": \"positive\"},\n    {\"text\": \"The work environment was toxic and stressful.\", \"sentiment\": \"negative\"},\n    {\"text\": \"It was an okay experience, nothing special.\", \"sentiment\": \"neutral\"},\n    {\"text\": \"I am unsure about my feelings towards this job.\", \"sentiment\": \"ambiguous\"}\n]\n</code></pre>"},{"location":"Python/Deep_Learning/Sentiment_analysis/#bert-model","title":"BERT Model","text":"<ul> <li>Split Data: Split your dataset into training and validation sets.</li> <li>Training: Train the model on the training set.</li> <li>Evaluation: Use the validation set to evaluate the model. Here\u2019s how you can do it with the BERT model:</li> </ul>"},{"location":"Python/Deep_Learning/Sentiment_analysis/#lstm-model","title":"LSTM Model","text":"<ul> <li>Split Data: Split your dataset into training and validation sets.</li> <li>Training: Train the model on the training set.</li> <li>Evaluation: Use the validation set to evaluate the model. Here\u2019s how you can do it with the LSTM model:</li> </ul>"},{"location":"Python/Deep_Learning/Sentiment_analysis/#rnn-model","title":"RNN Model","text":"<ul> <li>Split Data: Split your dataset into training and validation sets.</li> <li>Training: Train the model on the training set.</li> <li>Evaluation: Use the validation set to evaluate the model. Here\u2019s how you can do it with the RNN model:</li> </ul>"},{"location":"Python/Deep_Learning/Sentiment_analysis/#traditional-machine-learning-models-logistic-regression-naive-bayes-support-vector-machines-svm","title":"Traditional Machine Learning Models: Logistic Regression, Naive Bayes, Support Vector Machines (SVM)","text":""},{"location":"Python/Deep_Learning/Sentiment_analysis/#preprocessing","title":"Preprocessing","text":"<p>First, we need to preprocess the text data and convert it into a format suitable for machine learning models.</p> <pre><code>import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Convert data to DataFrame\ndf = pd.DataFrame(data)\n\n# Encode labels\nlabel_mapping = {'positive': 0, 'negative': 1, 'neutral': 2, 'ambiguous': 3}\ndf['label'] = df['sentiment'].map(label_mapping)\n\n# Split data\nX_train, X_val, y_train, y_val = train_test_split(df['text'], df['label'], test_size=0.2, random_state=42)\n\n# Vectorize text data\nvectorizer = TfidfVectorizer(max_features=1000)\nX_train_vec = vectorizer.fit_transform(X_train)\nX_val_vec = vectorizer.transform(X_val)\n</code></pre>"},{"location":"Python/Deep_Learning/Sentiment_analysis/#logistic-regression","title":"Logistic Regression","text":"<p>Logistic regression is a linear model used for binary classification. It can be extended to multi-class classification using the one-vs-rest approach.</p> <pre><code>from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n# Train the model\nlog_reg = LogisticRegression()\nlog_reg.fit(X_train_vec, y_train)\n\n# Predict and evaluate\ny_pred = log_reg.predict(X_val_vec)\nprint(\"Logistic Regression Classification Report:\")\nprint(classification_report(y_val, y_pred, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n</code></pre>"},{"location":"Python/Deep_Learning/Sentiment_analysis/#naive-bayes","title":"Naive Bayes","text":"<p>Naive Bayes is a probabilistic classifier based on Bayes' theorem with strong independence assumptions between features.</p> <pre><code>from sklearn.naive_bayes import MultinomialNB\n\n# Train the model\nnb = MultinomialNB()\nnb.fit(X_train_vec, y_train)\n\n# Predict and evaluate\ny_pred = nb.predict(X_val_vec)\nprint(\"Naive Bayes Classification Report:\")\nprint(classification_report(y_val, y_pred, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n</code></pre>"},{"location":"Python/Deep_Learning/Sentiment_analysis/#support-vector-machine-svm","title":"Support Vector Machine (SVM)","text":"<p>Support Vector Machines (SVM) are supervised learning models used for classification tasks. They find the hyperplane that best separates the classes in the feature space.</p> <pre><code>from sklearn.svm import SVC\n\n# Train the model\nsvm = SVC(kernel='linear')\nsvm.fit(X_train_vec, y_train)\n\n# Predict and evaluate\ny_pred = svm.predict(X_val_vec)\nprint(\"SVM Classification Report:\")\nprint(classification_report(y_val, y_pred, target_names=['positive', 'negative', 'neutral', 'ambiguous']))\n</code></pre>"},{"location":"Python/Deep_Learning/Sentiment_analysis/#xgboost-model-with-word2vec-embeddings","title":"XGBoost Model with Word2Vec Embeddings","text":""},{"location":"Python/Django/Django_Pandas_DataFrame_To_jQuery_DataTable/","title":"Django Pandas DataFrame To jQuery DataTable","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport requests\nfrom django.shortcuts import render\n\ndef datatable_view(request):\n    dates = pd.date_range('2022-01-01', '2022-01-10')\n    df = pd.DataFrame({\n        'date': dates,\n        'sales': np.round(np.random.rand(len(dates))*1000)\n    })\n    # table classes, if needed (ex: Bootstrap)\n    # classes=[\"table-bordered\", \"table-striped\", \"table-hover\"]\n    table_data = df.to_html(table_id=\"table_example\")\n    context = {'table_data': table_data}\n    return render(request, \"app/datatable.html\", context)\n</code></pre> <pre><code>&lt;!DOCTYPE html&gt;\n\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n&lt;meta charset=\"utf-8\"/&gt;\n&lt;meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/&gt;\n&lt;meta content=\"width=device-width, initial-scale=1, shrink-to-fit=no\" name=\"viewport\"/&gt;\n&lt;!-- datatables CSS --&gt;\n&lt;link href=\"https://cdn.datatables.net/1.11.5/css/jquery.dataTables.min.css\" rel=\"stylesheet\" type=\"text/css\"/&gt;\n&lt;/head&gt;\n&lt;body id=\"page-top\"&gt;\n&lt;!-- DataTable content --&gt;\n&lt;div class=\"card-body\"&gt;\n        {{ table_data | safe }}\n    &lt;/div&gt;\n&lt;!-- datatables JS --&gt;\n&lt;script src=\"https://cdn.datatables.net/1.11.5/js/jquery.dataTables.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdn.datatables.net/1.11.5/js/dataTables.bootstrap4.min.js\" type=\"text/javascript\"&gt;&lt;/script&gt;\n&lt;script&gt;\n        {% comment %} Initialize DataTable {% endcomment %}\n        $(document).ready(function() {\n            $('#table_example').DataTable();\n        } );\n    &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre>"},{"location":"Python/Django/Django_Pandas_DataFrame_To_jQuery_DataTable/#django-pandas-dataframe-to-jquery-datatable","title":"Django Pandas DataFrame to jQuery DataTable","text":"<p>DataTables (https://datatables.net/) is a cool jQuery Javacript plugin to show advanced tables, an easy way.</p>"},{"location":"Python/Django/Django_Pandas_DataFrame_To_jQuery_DataTable/#define-the-view","title":"Define the View","text":"<p>Table classe can be set with parameter <code>classes</code>, to be used with Bootstrap for example.</p> <pre><code>df.to_html(table_id=\"table_example\", classes=[\"table-bordered\", \"table-striped\", \"table-hover\"])\n</code></pre>"},{"location":"Python/Django/Django_Pandas_DataFrame_To_jQuery_DataTable/#define-the-template","title":"Define the Template","text":"<p>Define the template <code>app/datatable.html</code>.</p>"},{"location":"Python/Flask/Flask_Chat_Colab/","title":"Flask Chat Colab","text":"<pre><code>%%writefile requirements.txt\nFlask==0.12.2\nflask-socketio\neventlet==0.17.4\ngunicorn==18.0.0\n</code></pre> <pre>\n<code>Overwriting requirements.txt\n</code>\n</pre> <pre><code>%%writefile main.py\nimport os\nimport logging\n\nfrom flask import Flask, render_template\nfrom flask_socketio import SocketIO\n\nsecret = os.urandom(24).hex()\n\napp = Flask(__name__)\napp.logger.info(\"Starting...\")\napp.config['SECRET_KEY'] = secret\napp.logger.critical(\"secret: %s\" % secret)\nsocketio = SocketIO(app)\n\n# render home page\n@app.route('/')\ndef index():\n    return render_template('index.html')\n\n# Simple http endpoint\n@app.route('/hello')\ndef hello():\n    return \"Hello World!\"\n\n# ws callback\ndef message_received(methods=['GET', 'POST']):\n    app.logger.info('message was received!')\n\n# ws event handler\n@socketio.on('flask-chat-event')\ndef handle_flask_chat_event(json, methods=['GET', 'POST']):\n    app.logger.info('received flask-chat-event: ' + str(json))\n    socketio.emit('flask-chat-response', json, callback=message_received)\n\nif __name__ == '__main__':\n    socketio.run(app, debug=True)\n</code></pre> <pre>\n<code>Overwriting main.py\n</code>\n</pre> <pre><code>%mkdir templates -p\n</code></pre> <pre><code>%%writefile templates/index.html\n&lt;!DOCTYPE html&gt;\n\n&lt;html lang=\"en\"&gt;\n&lt;head&gt;\n&lt;meta charset=\"utf-8\"/&gt;\n&lt;meta content=\"IE=edge\" http-equiv=\"X-UA-Compatible\"/&gt;\n&lt;meta content=\"Learn how to create a chat app using Flask\" name=\"description\"/&gt;\n&lt;meta content=\"width=device-width, initial-scale=1.0\" name=\"viewport\"/&gt;\n&lt;title&gt;Flask Chat&lt;/title&gt;\n&lt;!-- Disable tap highlight on IE --&gt;\n&lt;meta content=\"no\" name=\"msapplication-tap-highlight\"/&gt;\n&lt;!-- Web Application Manifest --&gt;\n&lt;link href='data:application/manifest+json,{ \"name\": \"Flask Chat\", \"short_name\": \"Flask Chat\", \"display\": \"standalone\" }' rel=\"manifest\"/&gt;\n&lt;!-- Add to homescreen for Chrome on Android --&gt;\n&lt;meta content=\"yes\" name=\"mobile-web-app-capable\"/&gt;\n&lt;meta content=\"Flask Chat\" name=\"application-name\"/&gt;\n&lt;meta content=\"#303F9F\" name=\"theme-color\"/&gt;\n&lt;!-- Add to homescreen for Safari on iOS --&gt;\n&lt;meta content=\"yes\" name=\"apple-mobile-web-app-capable\"/&gt;\n&lt;meta content=\"black-translucent\" name=\"apple-mobile-web-app-status-bar-style\"/&gt;\n&lt;meta content=\"Flask Chat\" name=\"apple-mobile-web-app-title\"/&gt;\n&lt;meta content=\"#303F9F\" name=\"apple-mobile-web-app-status-bar-style\"/&gt;\n&lt;!-- Tile icon for Win8 --&gt;\n&lt;meta content=\"#3372DF\" name=\"msapplication-TileColor\"/&gt;\n&lt;meta content=\"#303F9F\" name=\"msapplication-navbutton-color\"/&gt;\n&lt;script async=\"\" src=\"https://www.googletagmanager.com/gtag/js?id=UA-135532366-1\"&gt;&lt;/script&gt;\n&lt;script&gt;function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag(\"js\",new Date),gtag(\"config\",\"UA-135532366-1\");&lt;/script&gt;\n&lt;!-- Material Design Lite --&gt;\n&lt;link href=\"https://fonts.googleapis.com/icon?family=Material+Icons\" rel=\"stylesheet\"/&gt;\n&lt;link href=\"https://code.getmdl.io/1.1.3/material.orange-indigo.min.css\" rel=\"stylesheet\"/&gt;\n&lt;script defer=\"\" src=\"https://code.getmdl.io/1.1.3/material.min.js\"&gt;&lt;/script&gt;\n&lt;!-- App Styling --&gt;\n&lt;link href=\"https://fonts.googleapis.com/css?family=Roboto:regular,bold,italic,thin,light,bolditalic,black,medium&amp;amp;lang=en\" rel=\"stylesheet\"/&gt;\n&lt;style&gt;\n    .message-container .spacing {\n      display: table-cell;\n      vertical-align: top;\n    }\n\n    .message-container .message {\n      display: table-cell;\n      width: calc(100% - 40px);\n      padding: 5px 0 5px 10px;\n    }\n\n    .message-container .name {\n      display: inline-block;\n      width: 100%;\n      padding-left: 40px;\n      color: #bbb;\n      font-style: italic;\n      font-size: 12px;\n      box-sizing: border-box;\n    }\n  &lt;/style&gt;\n&lt;/head&gt;\n&lt;body&gt;\n&lt;div class=\"demo-layout mdl-layout mdl-js-layout mdl-layout--fixed-header\"&gt;\n&lt;!-- Header section containing logo --&gt;\n&lt;header class=\"mdl-layout__header mdl-color-text--white mdl-color--light-blue-700\"&gt;\n&lt;div class=\"mdl-cell mdl-cell--12-col mdl-cell--12-col-tablet mdl-grid\"&gt;\n&lt;div class=\"mdl-layout__header-row mdl-cell mdl-cell--12-col mdl-cell--12-col-tablet mdl-cell--12-col-desktop\"&gt;\n&lt;h3&gt;&lt;i class=\"material-icons\"&gt;chat_bubble_outline&lt;/i&gt; Flask Chat&lt;/h3&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/header&gt;\n&lt;main class=\"mdl-layout__content mdl-color--grey-100\"&gt;\n&lt;div class=\"mdl-cell mdl-cell--12-col mdl-grid\" id=\"messages-card-container\"&gt;\n&lt;!-- Messages container --&gt;\n&lt;div class=\"mdl-card mdl-shadow--2dp mdl-cell mdl-cell--12-col mdl-cell--6-col-tablet mdl-cell--6-col-desktop\" id=\"messages-card\"&gt;\n&lt;div class=\"mdl-card__supporting-text mdl-color-text--grey-600\"&gt;\n&lt;div id=\"messages\"&gt;\n&lt;span id=\"message-filler\"&gt;&lt;/span&gt;\n&lt;div class=\"message-container visible\"&gt;\n&lt;div class=\"spacing\"&gt;\n&lt;/div&gt;\n&lt;div class=\"message\"&gt;Welcome!&lt;/div&gt;\n&lt;div class=\"name\"&gt;Shanaka DeSoysa&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;form action=\"POST\" id=\"message-form\"&gt;\n&lt;div class=\"mdl-textfield mdl-js-textfield mdl-textfield--floating-label\"&gt;\n&lt;input class=\"mdl-textfield__input\" id=\"username\" type=\"text\"/&gt;\n&lt;label class=\"mdl-textfield__label\" for=\"username\"&gt;User...&lt;/label&gt;\n&lt;/div&gt;\n&lt;div class=\"mdl-textfield mdl-js-textfield mdl-textfield--floating-label\"&gt;\n&lt;input class=\"mdl-textfield__input\" id=\"message\" type=\"text\"/&gt;\n&lt;label class=\"mdl-textfield__label\" for=\"message\"&gt;Message...&lt;/label&gt;\n&lt;/div&gt;\n&lt;button class=\"mdl-button mdl-js-button mdl-button--raised mdl-js-ripple-effect\" id=\"submit\" type=\"submit\"&gt;\n                Send\n              &lt;/button&gt;\n&lt;/form&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/div&gt;\n&lt;/main&gt;\n&lt;/div&gt;\n&lt;!-- jQuery (necessary for Bootstrap's JavaScript plugins) --&gt;\n&lt;script src=\"https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js\"&gt;&lt;/script&gt;\n&lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/socket.io/1.7.3/socket.io.min.js\"&gt;&lt;/script&gt;\n&lt;script type=\"text/javascript\"&gt;\n    var socket = io.connect('https://' + document.domain + ':' + location.port);\n\n    socket.on('connect', function () {\n      socket.emit('flask-chat-event', {\n        data: 'User Connected'\n      })\n      var form = $('form').on('submit', function (e) {\n        e.preventDefault()\n        let user_name = $('#username').val()\n        let user_input = $('#message').val()\n        socket.emit('flask-chat-event', {\n          user_name: user_name,\n          message: user_input\n        })\n        $('#message').val('').focus()\n      })\n    })\n    socket.on('flask-chat-response', function (msg) {\n      console.log(msg)\n      if (typeof msg.user_name !== 'undefined') {\n        $('#messages').append('&lt;div class=\"message-container visible\"&gt;&lt;div class=\"spacing\"&gt;&lt;/div&gt;&lt;div class=\"message\"&gt;' + msg.message + '&lt;/div&gt;&lt;div class=\"name\"&gt;' + msg.user_name + '&lt;/div&gt;&lt;/div&gt;')\n      }\n    })\n  &lt;/script&gt;\n&lt;/body&gt;\n&lt;/html&gt;\n</code></pre> <pre>\n<code>Overwriting templates/index.html\n</code>\n</pre> <pre><code>get_ipython().system_raw(\n    'pip3 install -r requirements.txt &amp;amp;&amp;amp; python3 main.py &amp;gt; logs.txt 2&amp;gt;&amp;amp;1 &amp;amp;'\n)\n</code></pre> <pre><code>!tail logs.txt\n</code></pre> <pre>\n<code>[2019-06-29 18:04:09,804] CRITICAL in main: secret: 9aba1627141610d3ea12a10e7a54a08d4c595a7cb9496089\n * Restarting with stat\n[2019-06-29 18:04:10,159] CRITICAL in main: secret: 74c1b6b00c53dce5d804e82e823e623c61224d955158072f\n * Debugger is active!\n * Debugger PIN: 127-941-347\n</code>\n</pre> <pre><code># Make sure it's running on local port\nPORT = 5000\n\n!curl http://localhost:{PORT}/hello\n</code></pre> <pre>\n<code>Hello World!</code>\n</pre> <pre><code>!wget --quiet https://bin.equinox.io/c/4VmDzA7iaHb/ngrok-stable-linux-amd64.zip -O ngrok-stable-linux-amd64.zip\n!unzip -q -f ngrok-stable-linux-amd64.zip\n</code></pre> <p>Running <code>ngrok</code>.</p> <pre><code>get_ipython().system_raw(\n    './ngrok http {} &amp;amp;'\n    .format(PORT)\n)\n</code></pre> <pre><code>public_url = !curl -s http://localhost:4040/api/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"\n\nprint(public_url[0])\n</code></pre> <pre>\n<code>http://8ab37579.ngrok.io\n</code>\n</pre> <pre><code>!curl {public_url[0]}/hello\n</code></pre> <pre>\n<code>Hello World!</code>\n</pre> <p>Congratulations! You have successfully built a chat application using Flask and WebSockets. You can share this URL with your friends and chat. You could also open multiple tabs in the browser to test it out. Don't forget to check it out on your mobile.</p> <p>The source-code for the article can be found here.</p> <p></p>"},{"location":"Python/Flask/Flask_Chat_Colab/#chat-app-using-flask-socketio-and-google-colab","title":"Chat App Using Flask, SocketIO and Google Colab","text":"<p>In this short tutorial, you'll learn how to create a simple but elegant chat app and host it on Google Cloud (for free) under 5 minutes .</p> <p>Here's how the App would look like:</p> <p>On Mobile:</p> <p></p> <p>On Browser:</p> <p></p>"},{"location":"Python/Flask/Flask_Chat_Colab/#what-youll-build","title":"What you'll build","text":"<ul> <li>A fully working real-time multi user chat app.</li> <li>Mobile and web friendly Progressive Web App (PWA).</li> <li>Public URL to share with family/friends.</li> <li>Hosted on Google Cloud for free.</li> </ul>"},{"location":"Python/Flask/Flask_Chat_Colab/#what-youll-learn","title":"What you'll learn","text":"<ul> <li>Dveloping a useful Flask web app.</li> <li>Using WebSocket with Flask-SocketIO for bi-directional real-time communication.</li> <li>How to create a public URL with ngrok to share your web app.</li> <li>How to run a web server on Colab.</li> </ul>"},{"location":"Python/Flask/Flask_Chat_Colab/#what-youll-need","title":"What you'll need","text":"<ul> <li>Gmail account to access Google Colaboratory for free.</li> <li>A browser such as Chrome.</li> <li>The sample notebook. Click on the Open in Colab button below to get started.</li> </ul>"},{"location":"Python/Flask/Flask_Chat_Colab/#python-packages","title":"Python Packages","text":"<p>Packages required to run the app. You'll use <code>pip</code> to install these packages.</p>"},{"location":"Python/Flask/Flask_Chat_Colab/#flask-app","title":"Flask App","text":"<p>Here you are defining event handlers to receive WebSocket (<code>ws</code>) messages using the <code>socketio.on</code> decorator and it sends reply messages to the connected client using the <code>send()</code> and <code>emit()</code> functions.</p> <p>Also you are defining a simple <code>http</code> endpoint <code>/hello</code> for testing purpose.</p>"},{"location":"Python/Flask/Flask_Chat_Colab/#html-template","title":"HTML Template","text":"<p>Here you are defineing:</p> <ol> <li>Web UI for the app.</li> <li>JavaScript functions to establish <code>ws</code> connection, send and receive messages with <code>socket.io</code>.</li> </ol>"},{"location":"Python/Flask/Flask_Chat_Colab/#installing-packages-and-running-web-server","title":"Installing Packages and Running Web Server","text":""},{"location":"Python/Flask/Flask_Chat_Colab/#checking-the-log-file","title":"Checking the Log File","text":"<p>You can check the log file with this command.</p>"},{"location":"Python/Flask/Flask_Chat_Colab/#verifying-the-web-server-is-running","title":"Verifying the Web Server is Running","text":"<p>You can do a quick check if the server is up and running with <code>curl</code>.</p>"},{"location":"Python/Flask/Flask_Chat_Colab/#getting-a-shareable-public-url-from-ngrok","title":"Getting a Shareable Public URL from ngrok","text":"<p>Here you are installing <code>ngrok</code> and obtaining a shareable URL.</p>"},{"location":"Python/Flask/Flask_Chat_Colab/#public-url","title":"Public URL","text":""},{"location":"Python/Flask/Flask_Chat_Colab/#verifying-the-public-url-is-accessible","title":"Verifying the Public URL is Accessible","text":""},{"location":"Python/Jupyter/Generate_Jupyter_password/","title":"Generate Jupyter password","text":"<pre><code>from notebook.auth import passwd\npasswd()\n</code></pre> <pre>\n<code>Enter password: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\nVerify password: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n</code>\n</pre> <pre>\n<code>'argon2:$argon2id$v=19$m=10240,t=10,p=8$WRlMB2K+RRj7Q8kFN3CvVQ$WGpUDx2lT97F5S4Z1wuifQ'</code>\n</pre>"},{"location":"Python/Jupyter/Generate_Jupyter_password/#generate-jupyter-password","title":"Generate Jupyter Password","text":""},{"location":"Python/Jupyter/Generate_Jupyter_password/#use-password-in-jupyter-config","title":"Use Password in Jupyter config","text":"<pre><code>{\n  \"NotebookApp\": {\n    \"password\": \"argon2:$argon2id$v=19$m=10240,t=10,p=8$WRlMB2K+RRj7Q8kFN3CvVQ$WGpUDx2lT97F5S4Z1wuifQ\",\n    \"ip\": \"*\",\n    \"port\": 8888,\n    \"notebook_dir\": \"/home/jovyan/work\"\n  }\n}\n</code></pre>"},{"location":"Python/Jupyter/Running_JavaScript_in_Jupyter_Notebook/","title":"Running JavaScript in Jupyter Notebook","text":"<pre><code>import random\nfrom IPython.display import display, Javascript, HTML, clear_output\n\nunique_id = str(random.randint(100000, 999999))\n\ndisplay(Javascript(\n    '''\n    var id = '%(unique_id)s';\n    // Make a new global function with a unique name, to prevent collisions with past\n    // executions of this cell (since JS state is reused).\n    window['render_' + id] = function() {\n        // Put data fetching function here.\n        // $('#' + id).text('Hello at ' + new Date());\n\n        $.getJSON('https://api.db-ip.com/v2/free/self', function(data) {\n          // console.log(JSON.stringify(data, null, 2));\n          $('#' + id).text(JSON.stringify(data, null, 2));\n        });\n\n    }\n    // See if the `HTML` block executed first, and if so trigger the render.\n    if ($('#' + id).length) {\n        window['render_' + id]();\n    }\n    ''' % dict(unique_id=unique_id)\n    # Use % instead of .format since the latter requires {{ and }} escaping.\n))\n\nclear_output()\n\ndisplay(HTML(\n    '''\n    &lt;div id=\"%(unique_id)s\"&gt;&lt;/div&gt;\n&lt;!-- When this script block executes, the &lt;div&gt; is ready for data. --&gt;\n&lt;script type=\"text/javascript\"&gt;\n        var id = '%(unique_id)s';\n        // See if the `Javascript` block executed first, and if so trigger the render.\n        if (window['render_' + id]) {\n            window['render_' + id]();\n        }\n    &lt;/script&gt;\n    ''' % {'unique_id': unique_id}\n))\n</code></pre> <pre><code># Test\n</code></pre> <pre><code>\n</code></pre>"},{"location":"Python/Jupyter/Running_JavaScript_in_Jupyter_Notebook/#running-javascript-in-jupyter-notebook","title":"Running JavaScript in Jupyter Notebook","text":""},{"location":"Python/Misc/CSV_content_to_QR/","title":"CSV to QR code","text":"<p>Here's a Python script that reads a CSV file and generates a QR code from its content:</p> <pre><code>import qrcode\nimport csv\n\n# Function to read CSV file and convert its content to a string\ndef csv_to_string(file_path):\n    with open(file_path, mode='r') as file:\n        csv_reader = csv.reader(file)\n        csv_data = \"\\n\".join([\",\".join(row) for row in csv_reader])\n    return csv_data\n\n# Path to the CSV file\ncsv_file_path = 'data.csv'\n\n# Convert CSV content to string\ncsv_content = csv_to_string(csv_file_path)\n\n# Generate QR code from CSV content\nqr = qrcode.QRCode(\n    version=1,\n    error_correction=qrcode.constants.ERROR_CORRECT_H,\n    box_size=10,\n    border=4,\n)\nqr.add_data(csv_content)\nqr.make(fit=True)\n\n# Create an image from the QR Code instance\nimg = qr.make_image(fill='black', back_color='white')\n\n# Save the QR code as \"csv_qrcode.png\"\nimg.save(\"csv_qrcode.png\")\n\nprint(\"QR code generated and saved as 'csv_qrcode.png'.\")\n</code></pre> <p>This script reads the content of a CSV file, converts it to a string, and then generates a QR code from that string. The QR code is saved as an image file named <code>csv_qrcode.png</code>.</p> <p>When you run this script, make sure the CSV file (<code>data.csv</code>) is in the same directory as your script.</p>"},{"location":"Python/Misc/Woed2Vec_sklearn/","title":"Integrating Word2Vec with Scikit-Learn Pipelines","text":"<p>In this post, we'll explore how to integrate a custom <code>Word2Vec</code> transformer with <code>scikit-learn</code> pipelines. This allows us to leverage the power of Word2Vec embeddings in a machine learning workflow.</p>"},{"location":"Python/Misc/Woed2Vec_sklearn/#introduction","title":"Introduction","text":"<p>Word2Vec is a popular technique for natural language processing (NLP) that transforms words into continuous vector representations. These vectors capture semantic relationships between words, making them useful for various NLP tasks. However, integrating Word2Vec with <code>scikit-learn</code> pipelines requires a custom transformer. Let's walk through the process step-by-step.</p> <pre><code>import pandas as pd\nfrom gensim.models import Word2Vec\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.base import BaseEstimator, TransformerMixin\nimport numpy as np\n\n# Sample data\ncorpus = [\n    \"This is the first document.\",\n    \"This document is the second document.\",\n    \"And this is the third one.\",\n    \"Is this the first document?\",\n]\nlabels = [1, 0, 1, 1]\n\n# Custom Word2VecTransformer\nclass Word2VecTransformer(BaseEstimator, TransformerMixin):\n    def __init__(self, vector_size=100, window=5, min_count=1, workers=4):\n        self.vector_size = vector_size\n        self.window = window\n        self.min_count = min_count\n        self.workers = workers\n\n    def fit(self, X, y=None):\n        tokenized_sentences = [sentence.split() for sentence in X]\n        self.model = Word2Vec(sentences=tokenized_sentences, vector_size=self.vector_size, \n                              window=self.window, min_count=self.min_count, workers=self.workers)\n        return self\n\n    def transform(self, X):\n        tokenized_sentences = [sentence.split() for sentence in X]\n        return np.array([\n            np.mean([self.model.wv[word] for word in sentence if word in self.model.wv] \n                    or [np.zeros(self.vector_size)], axis=0)\n            for sentence in tokenized_sentences\n        ])\n\n# Define the pipeline\npipeline = Pipeline([\n    ('word2vec', Word2VecTransformer(vector_size=100, window=5, min_count=1, workers=4)),\n    ('clf', LogisticRegression())\n])\n\n# Fit the pipeline\npipeline.fit(corpus, labels)\n\n# Make predictions\npredictions = pipeline.predict([\"This is a new document.\"])\nprint(predictions)\n</code></pre>"},{"location":"Python/Misc/Woed2Vec_sklearn/#explanation","title":"Explanation","text":"<ol> <li>Import Libraries: We import necessary libraries for data manipulation (<code>pandas</code>), Word2Vec model (<code>gensim</code>), and creating a pipeline (<code>sklearn</code>).</li> <li>Sample Data: We create a sample corpus and corresponding labels.</li> <li>Custom Transformer: We define a custom <code>Word2VecTransformer</code> class that fits a Word2Vec model and transforms documents into vector representations.</li> <li>Pipeline Definition: We define a pipeline with <code>TfidfVectorizer</code>, our custom <code>Word2VecTransformer</code>, and <code>LogisticRegression</code>.</li> <li>Training and Prediction: We fit the pipeline on the sample data and make predictions on a new document.</li> </ol>"},{"location":"Python/Misc/Woed2Vec_sklearn/#conclusion","title":"Conclusion","text":"<p>By creating a custom <code>Word2Vec</code> transformer, we can seamlessly integrate Word2Vec embeddings into <code>scikit-learn</code> pipelines. This approach allows us to leverage the power of Word2Vec in a structured machine learning workflow.</p>"},{"location":"Python/Misc/draft/","title":"Numba","text":"<p>Numba JIT Compilation:</p> <p>The compute_percentiles function is compiled with Numba for faster percentile calculations. The bootstrap_sampling function is also compiled with Numba to efficiently generate bootstrap indices. Vectorized Sampling:</p> <p>Bootstrap indices are generated in a single step using np.random.randint and applied to the DataFrame. Preallocated Arrays:</p> <p>Arrays for bootstrap_weights, bootstrap_random_variable, and bootstrap_comparison are preallocated for better memory management. DataFrame to NumPy Conversion:</p> <p>The DataFrame is converted to a NumPy array (df.values) for faster indexing during bootstrap sampling. This refactored code should significantly improve performance, especially for large datasets and a high number of bootstrap</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom numba import jit\n\n@jit(nopython=True)\ndef compute_percentiles(data, alpha):\n    \"\"\"Compute lower and upper percentiles.\"\"\"\n    lower = np.percentile(data, alpha * 100 / 2, axis=0)\n    upper = np.percentile(data, 100 - alpha * 100 / 2, axis=0)\n    return lower, upper\n\n@jit(nopython=True)\ndef bootstrap_sampling(df_values, num_bootstrap, num_samples):\n    \"\"\"Perform bootstrap sampling.\"\"\"\n    bootstrap_indices = np.random.randint(0, num_samples, size=(num_bootstrap, num_samples))\n    return bootstrap_indices\n\ndef optimized_bootstrap_relative_weights(df, outcome, drivers, focal=None, num_bootstrap=10000, compare=\"No\", alpha=0.05):\n    def compute_rel_wts(sample):\n        return relativeImp(sample, outcome, drivers)['rawRelaImpt'].values\n\n    def random_variable(sample):\n        return add_random_variable(sample, outcome, drivers)['rawRelaImpt'].values\n\n    def compare_preds(sample):\n        return compare_predictors(sample, outcome, drivers, focal)['weightDiff'].values\n\n    # Convert DataFrame to NumPy array for faster processing\n    df_values = df.values\n    num_samples = len(df)\n\n    # Preallocate arrays for bootstrap results\n    num_drivers = len(drivers)\n    bootstrap_weights = np.zeros((num_bootstrap, num_drivers))\n    bootstrap_random_variable = np.zeros((num_bootstrap, num_drivers))\n    bootstrap_comparison = np.zeros((num_bootstrap, num_drivers)) if compare == \"Yes\" else None\n\n    # Generate bootstrap indices using Numba\n    bootstrap_indices = bootstrap_sampling(df_values, num_bootstrap, num_samples)\n\n    for i in range(num_bootstrap):\n        sample = df.iloc[bootstrap_indices[i]]\n\n        try:\n            bootstrap_weights[i] = compute_rel_wts(sample)\n            bootstrap_random_variable[i] = random_variable(sample)\n\n            if compare == \"Yes\":\n                bootstrap_comparison[i] = compare_preds(sample)\n        except Exception as e:\n            print(f\"Error in bootstrap sample {i}: {e}\")\n\n    # Compute confidence intervals using Numba\n    ci_relative_weights_lower, ci_relative_weights_upper = compute_percentiles(bootstrap_weights, alpha)\n    ci_random_variable_lower, ci_random_variable_upper = compute_percentiles(bootstrap_random_variable, alpha)\n\n    result = {\n        'relative_weights': {\n            'Outcome': outcome,\n            'Drivers': drivers,\n            'ci_lower': ci_relative_weights_lower,\n            'ci_upper': ci_relative_weights_upper,\n            'ci_median': np.median(bootstrap_weights, axis=0),\n        },\n        'random_variable_diff': {\n            'Outcome': outcome,\n            'Drivers': drivers,\n            'ci_lower': ci_random_variable_lower,\n            'ci_upper': ci_random_variable_upper,\n            'ci_median': np.median(bootstrap_random_variable, axis=0),\n        }\n    }\n\n    if compare == \"Yes\":\n        ci_comparison_lower, ci_comparison_upper = compute_percentiles(bootstrap_comparison, alpha)\n        comparisons = [f\"{d}-{focal}\" for d in drivers if d != focal]\n        result['comparison_diff'] = {\n            'comparison': comparisons,\n            'ci_lower': ci_comparison_lower,\n            'ci_upper': ci_comparison_upper,\n            'ci_median': np.median(bootstrap_comparison, axis=0),\n        }\n\n    # Plot histograms\n    plt.figure(figsize=(10, 4 * num_drivers))\n    for i, driver in enumerate(drivers):\n        weights = bootstrap_weights[:, i]\n        median = np.median(weights)\n        plt.subplot(num_drivers, 1, i + 1)\n        plt.hist(weights, bins=50, alpha=0.5, color='blue', label='Relative Weights')\n        plt.axvline(ci_relative_weights_lower[i], color='red', linestyle='--', label='Lower Bound')\n        plt.axvline(ci_relative_weights_upper[i], color='green', linestyle='--', label='Upper Bound')\n        plt.axvline(median, color='purple', linestyle='-', label='Median')\n        plt.title(f\"Distribution of Relative Weights for {driver}\")\n        plt.ylabel('Frequency')\n        plt.legend()\n    plt.show()\n\n    return result\n</code></pre>"},{"location":"Python/Misc/draft/#parallel-processing-with-numba","title":"Parallel Processing with Numba","text":"<pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom numba import jit, prange\n\n@jit(nopython=True)\ndef compute_percentiles(data, alpha):\n    \"\"\"Compute lower and upper percentiles manually.\"\"\"\n    num_samples, num_features = data.shape\n    lower = np.zeros(num_features)\n    upper = np.zeros(num_features)\n\n    for j in range(num_features):\n        sorted_column = np.sort(data[:, j])  # Sort each column manually\n        lower_idx = int(alpha * 0.5 * num_samples)\n        upper_idx = int((1 - alpha * 0.5) * num_samples)\n        lower[j] = sorted_column[lower_idx]\n        upper[j] = sorted_column[upper_idx]\n\n    return lower, upper\n\n@jit(nopython=True, parallel=True)\ndef bootstrap_sampling(data, num_bootstrap):\n    \"\"\"Perform bootstrap sampling and compute results.\"\"\"\n    num_samples, num_features = data.shape\n    bootstrap_weights = np.zeros((num_bootstrap, num_features))\n    for i in prange(num_bootstrap):\n        indices = np.random.randint(0, num_samples, size=num_samples)\n        sample = data[indices]\n        # Compute mean manually along axis 0\n        for j in range(num_features):\n            bootstrap_weights[i, j] = sample[:, j].sum() / num_samples\n    return bootstrap_weights\n\ndef optimized_bootstrap_relative_weights(df, outcome, drivers, num_bootstrap=10000, alpha=0.05):\n    # Convert DataFrame to NumPy array (ensure uniform data type)\n    data = df[drivers].to_numpy(dtype=np.float64)\n\n    # Perform bootstrap sampling using Numba\n    bootstrap_weights = bootstrap_sampling(data, num_bootstrap)\n\n    # Compute confidence intervals\n    ci_lower, ci_upper = compute_percentiles(bootstrap_weights, alpha)\n    ci_median = np.median(bootstrap_weights, axis=0)\n\n    # Prepare result\n    result = {\n        'relative_weights': {\n            'Outcome': outcome,\n            'Drivers': drivers,\n            'ci_lower': ci_lower,\n            'ci_upper': ci_upper,\n            'ci_median': ci_median,\n        }\n    }\n\n    # Plot histograms\n    num_drivers = len(drivers)\n    plt.figure(figsize=(10, 4 * num_drivers))\n    for i, driver in enumerate(drivers):\n        weights = bootstrap_weights[:, i]\n        median = ci_median[i]\n        plt.subplot(num_drivers, 1, i + 1)\n        plt.hist(weights, bins=50, alpha=0.5, color='blue', label='Relative Weights')\n        plt.axvline(ci_lower[i], color='red', linestyle='--', label='Lower Bound')\n        plt.axvline(ci_upper[i], color='green', linestyle='--', label='Upper Bound')\n        plt.axvline(median, color='purple', linestyle='-', label='Median')\n        plt.title(f\"Distribution of Relative Weights for {driver}\")\n        plt.ylabel('Frequency')\n        plt.legend()\n    plt.show()\n\n    return result\n</code></pre>"},{"location":"Python/Misc/draft/#parallel-processing","title":"Parallel Processing:","text":"<p>The ProcessPoolExecutor is used to distribute the bootstrap sampling across multiple CPU cores. Each bootstrap iteration is handled by the bootstrap_worker function, which processes a single sample. Efficient Data Handling:</p> <p>Results from each worker are collected and converted into NumPy arrays for further processing. Error Handling:</p> <p>Errors in individual bootstrap samples are logged, and invalid results are skipped. Confidence Interval Calculation:</p> <p>Percentiles are calculated on the aggregated results after all workers complete their tasks. This approach leverages parallelism to significantly reduce the runtime for large datasets and a high number of bootstrap iterations.</p> <pre><code>import numpy as np\nimport matplotlib.pyplot as plt\nfrom concurrent.futures import ProcessPoolExecutor\n\ndef compute_rel_wts(sample, outcome, drivers):\n    return relativeImp(sample, outcome, drivers)['rawRelaImpt'].values\n\ndef random_variable(sample, outcome, drivers):\n    return add_random_variable(sample, outcome, drivers)['rawRelaImpt'].values\n\ndef compare_preds(sample, outcome, drivers, focal):\n    return compare_predictors(sample, outcome, drivers, focal)['weightDiff'].values\n\ndef bootstrap_worker(df, outcome, drivers, focal, compare):\n    \"\"\"Worker function to process a single bootstrap sample.\"\"\"\n    indices = np.random.choice(df.index, size=len(df), replace=True)\n    sample = df.loc[indices]\n\n    try:\n        relative_weights = compute_rel_wts(sample, outcome, drivers)\n        rand_weights = random_variable(sample, outcome, drivers)\n        comp_weights = compare_preds(sample, outcome, drivers, focal) if compare == \"Yes\" else None\n        return relative_weights, rand_weights, comp_weights\n    except Exception as e:\n        print(f\"Error in bootstrap sample: {e}\")\n        return None, None, None\n\ndef parallel_bootstrap_relative_weights(df, outcome, drivers, focal=None, num_bootstrap=10000, compare=\"No\", alpha=0.05):\n    num_drivers = len(drivers)\n\n    # Preallocate arrays for bootstrap results\n    bootstrap_weights = []\n    bootstrap_random_variable = []\n    bootstrap_comparison = [] if compare == \"Yes\" else None\n\n    # Use ProcessPoolExecutor for parallel processing\n    with ProcessPoolExecutor() as executor:\n        futures = [\n            executor.submit(bootstrap_worker, df, outcome, drivers, focal, compare)\n            for _ in range(num_bootstrap)\n        ]\n\n        for future in futures:\n            relative_weights, rand_weights, comp_weights = future.result()\n            if relative_weights is not None:\n                bootstrap_weights.append(relative_weights)\n                bootstrap_random_variable.append(rand_weights)\n                if compare == \"Yes\":\n                    bootstrap_comparison.append(comp_weights)\n\n    # Convert results to NumPy arrays\n    bootstrap_weights = np.array(bootstrap_weights)\n    bootstrap_random_variable = np.array(bootstrap_random_variable)\n    if compare == \"Yes\":\n        bootstrap_comparison = np.array(bootstrap_comparison)\n\n    # Compute confidence intervals\n    ci_relative_weights = np.percentile(bootstrap_weights, [alpha * 100 / 2, 100 - alpha * 100 / 2], axis=0)\n    ci_random_variable = np.percentile(bootstrap_random_variable, [alpha * 100 / 2, 100 - alpha * 100 / 2], axis=0)\n\n    result = {\n        'relative_weights': {\n            'Outcome': outcome,\n            'Drivers': drivers,\n            'ci_lower': ci_relative_weights[0],\n            'ci_upper': ci_relative_weights[1],\n            'ci_median': np.median(bootstrap_weights, axis=0),\n        },\n        'random_variable_diff': {\n            'Outcome': outcome,\n            'Drivers': drivers,\n            'ci_lower': ci_random_variable[0],\n            'ci_upper': ci_random_variable[1],\n            'ci_median': np.median(bootstrap_random_variable, axis=0),\n        }\n    }\n\n    if compare == \"Yes\":\n        ci_comparison = np.percentile(bootstrap_comparison, [alpha * 100 / 2, 100 - alpha * 100 / 2], axis=0)\n        comparisons = [f\"{d}-{focal}\" for d in drivers if d != focal]\n        result['comparison_diff'] = {\n            'comparison': comparisons,\n            'ci_lower': ci_comparison[0],\n            'ci_upper': ci_comparison[1],\n            'ci_median': np.median(bootstrap_comparison, axis=0),\n        }\n\n    # Plot histograms\n    plt.figure(figsize=(10, 4 * num_drivers))\n    for i, driver in enumerate(drivers):\n        weights = bootstrap_weights[:, i]\n        median = np.median(weights)\n        plt.subplot(num_drivers, 1, i + 1)\n        plt.hist(weights, bins=50, alpha=0.5, color='blue', label='Relative Weights')\n        plt.axvline(ci_relative_weights[0][i], color='red', linestyle='--', label='Lower Bound')\n        plt.axvline(ci_relative_weights[1][i], color='green', linestyle='--', label='Upper Bound')\n        plt.axvline(median, color='purple', linestyle='-', label='Median')\n        plt.title(f\"Distribution of Relative Weights for {driver}\")\n        plt.ylabel('Frequency')\n        plt.legend()\n    plt.show()\n\n    return result\n</code></pre>"},{"location":"Python/Misc/draft/#new","title":"new","text":"<pre><code>import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom numba import jit, prange\nfrom multiprocessing import Pool\n\n@jit(nopython=True)\ndef compute_rel_wts(sample, outcome, drivers):\n    return relativeImp(sample, outcome, drivers)['rawRelaImpt'].values\n\n@jit(nopython=True)\ndef random_variable(sample, outcome, drivers):\n    return add_random_variable(sample, outcome, drivers)['rawRelaImpt'].values\n\n@jit(nopython=True)\ndef compare_preds(sample, outcome, drivers, focal):\n    return compare_predictors(sample, outcome, drivers, focal)['weightDiff'].values\n\ndef bootstrap_sample(df, outcome, drivers, focal, compare):\n    indices = np.random.choice(df.index, size=len(df), replace=True)\n    sample = df.loc[indices]\n\n    try:\n        relative_weights = compute_rel_wts(sample, outcome, drivers)\n        rand_weights = random_variable(sample, outcome, drivers)\n        comp_weights = compare_preds(sample, outcome, drivers, focal) if compare == \"Yes\" else None\n        return relative_weights, rand_weights, comp_weights\n    except Exception as e:\n        print(f\"Error in bootstrap sample: {e}\")\n        return None, None, None\n\ndef bootstrap_relative_weights(df, outcome, drivers, focal=None, num_bootstrap=10000, compare=\"No\", alpha=0.05):\n    with Pool() as pool:\n        results = pool.starmap(bootstrap_sample, [(df, outcome, drivers, focal, compare) for _ in range(num_bootstrap)])\n\n    bootstrap_weights = [res[0] for res in results if res[0] is not None]\n    bootstrap_random_variable = [res[1] for res in results if res[1] is not None]\n    bootstrap_comparision = [res[2] for res in results if res[2] is not None]\n\n    ci_relative_weights = np.percentile(bootstrap_weights, [alpha * 100 / 2, 100 - alpha * 100 / 2], axis=0)\n    ci_random_variable = np.percentile(bootstrap_random_variable, [alpha * 100 / 2, 100 - alpha * 100 / 2], axis=0)\n\n    result = {\n        'relative_weights': {\n            'Outcome': outcome,\n            'Drivers': drivers,\n            'ci_lower': ci_relative_weights[0],\n            'ci_upper': ci_relative_weights[1],\n            'ci_median': np.median(bootstrap_weights, axis=0),\n        },\n        'random_variable_diff': {\n            'Outcome': outcome,\n            'Drivers': drivers,\n            'ci_lower': ci_random_variable[0],\n            'ci_upper': ci_random_variable[1],\n            'ci_median': np.median(bootstrap_random_variable, axis=0)\n        }\n    }\n\n    if compare == \"Yes\":\n        ci_comparision = np.percentile(bootstrap_comparision, [alpha * 100 / 2, 100 - alpha * 100 / 2], axis=0)\n        comparisions = [f\"{d}-{focal}\" for d in drivers if d != focal]\n        result['comparision_diff'] = {\n            'comparision': comparisions,\n            'ci_lower': ci_comparision[0],\n            'ci_upper': ci_comparision[1],\n            'ci_median': np.median(bootstrap_comparision)\n        }\n\n    num_drivers = len(drivers)\n    plt.figure(figsize=(10, 4 * num_drivers))\n\n    for i, driver in enumerate(drivers):\n        weights = [rw[i] for rw in bootstrap_weights]\n        median = np.median(weights)\n        plt.subplot(num_drivers, 1, i + 1)\n        plt.hist(weights, bins=50, alpha=0.5, color='blue', label='Relative Weights')\n        plt.axvline(ci_relative_weights[0][i], color='red', linestyle='--', label='Lower Bound')\n        plt.axvline(ci_relative_weights[1][i], color='green', linestyle='--', label='Upper Bound')\n        plt.axvline(median, color='purple', linestyle='-', label='Median')\n        plt.title(f\"Distribution of Relative Weights for {driver}\")\n        plt.ylabel('Frequency')\n        plt.legend()\n    plt.show()\n\n    return result\n</code></pre>"},{"location":"Python/Misc/draft/#numba-new","title":"Numba New","text":"<pre><code>from numba import jit, njit, prange\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n@njit\ndef compute_rel_wts_numba(corrXX, corrXY):\n    w_corrXX, v_corrXX = np.linalg.eig(corrXX)\n    diag = np.diag(np.sqrt(w_corrXX))\n    coef_xz = v_corrXX @ diag @ v_corrXX.T\n    coef_yz = np.linalg.inv(coef_xz) @ corrXY\n    rsquare = np.sum(coef_yz**2)\n    rawWeights = (coef_xz**2) @ (coef_yz**2)\n    normWeights = (rawWeights / rsquare) * 100\n    return rawWeights\n\n@njit(parallel=True)\ndef bootstrap_loop(df_values, num_bootstrap, outcome_idx, driver_indices):\n    n_samples = df_values.shape[0]\n    n_drivers = len(driver_indices)\n    bootstrap_weights = np.zeros((num_bootstrap, n_drivers))\n\n    for b in prange(num_bootstrap):\n        indices = np.random.choice(n_samples, size=n_samples, replace=True)\n        sample = df_values[indices]\n        corrALL = np.corrcoef(sample.T)\n        corrXX = corrALL[1:, 1:]\n        corrXY = corrALL[1:, 0]\n        bootstrap_weights[b] = compute_rel_wts_numba(corrXX, corrXY)\n\n    return bootstrap_weights\n\ndef bootstarp_relative_weights(df, outcome, drivers, num_bootstrap=10000, alpha=0.05):\n    # Prepare data\n    all_columns = [outcome] + drivers\n    df_numeric = df[all_columns].apply(pd.to_numeric, errors='coerce').dropna()\n    df_values = df_numeric.values\n    outcome_idx = 0\n    driver_indices = list(range(1, len(all_columns)))\n\n    # Run bootstrap loop\n    bootstrap_weights = bootstrap_loop(df_values, num_bootstrap, outcome_idx, driver_indices)\n\n    # Compute confidence intervals\n    ci_level = (1 - alpha) * 100\n    ci_relative_weights = np.percentile(bootstrap_weights, [alpha * 100 / 2, 100 - alpha * 100 / 2], axis=0)\n\n    result = {\n        'relative_weights': {\n            'Outcome': outcome,\n            'Drivers': drivers,\n            'ci_lower': ci_relative_weights[0],\n            'ci_upper': ci_relative_weights[1],\n            'ci_median': np.median(bootstrap_weights, axis=0),\n        }\n    }\n\n    # Plot results\n    num_drivers = len(drivers)\n    plt.figure(figsize=(10, 4 * num_drivers))\n\n    for i, driver in enumerate(drivers):\n        weights = bootstrap_weights[:, i]\n        median = np.median(weights)\n        plt.subplot(num_drivers, 1, i + 1)\n        plt.hist(weights, bins=50, alpha=0.5, color='blue', label='Relative Weights')\n        plt.axvline(ci_relative_weights[0][i], color='red', linestyle='--', label='Lower Bound')\n        plt.axvline(ci_relative_weights[1][i], color='green', linestyle='--', label='Upper Bound')\n        plt.axvline(median, color='purple', linestyle='-', label='Median')\n        plt.title(f\"Distribution of Relative Weights for {driver}\")\n        plt.ylabel('Frequency')\n        plt.legend()\n    plt.show()\n\n    return result\n</code></pre>"},{"location":"Python/Misc/draft_ml/","title":"Draft ML","text":"<p>To classify survey comments into predefined topics, especially when allowing up to three classes per comment, you can follow these steps:</p> <ol> <li>Data Preparation:</li> <li>Collect and Clean Data: Gather all survey comments and clean the text by removing any irrelevant information (e.g., special characters, stop words).</li> <li> <p>Label Data: If you have a labeled dataset, use it to train your model. If not, you might need to manually label a subset of comments to create a training set.</p> </li> <li> <p>Feature Extraction:</p> </li> <li> <p>Text Vectorization: Convert text data into numerical format using techniques like TF-IDF (Term Frequency-Inverse Document Frequency) or word embeddings (e.g., Word2Vec, GloVe, BERT).</p> </li> <li> <p>Model Selection:</p> </li> <li>Choose a Classification Algorithm: Use machine learning models such as Logistic Regression, Support Vector Machines (SVM), or more advanced models like BERT for text classification.</li> <li> <p>Multi-label Classification: Since each comment can belong to multiple topics, use multi-label classification techniques. Algorithms like Binary Relevance, Classifier Chains, or neural networks designed for multi-label tasks can be effective.</p> </li> <li> <p>Training the Model:</p> </li> <li>Split Data: Divide your data into training and testing sets.</li> <li> <p>Train the Model: Use your training data to train the chosen model. Ensure you use appropriate evaluation metrics like F1-score, Precision, and Recall for multi-label classification.</p> </li> <li> <p>Classification:</p> </li> <li> <p>Predict Topics: For each new survey comment, use the trained model to predict the relevant topics. Ensure the model can assign up to three topics per comment.</p> </li> <li> <p>Post-processing:</p> </li> <li>Thresholding: Apply a threshold to the model's output probabilities to decide which topics to assign to each comment.</li> <li> <p>Evaluation: Continuously evaluate the model's performance on new data and adjust as necessary.</p> </li> <li> <p>Automation and Integration:</p> </li> <li>Automate the Process: Integrate the model into your survey analysis pipeline to automatically classify new comments.</li> <li>Feedback Loop: Use feedback from the classified comments to retrain and improve the model over time.</li> </ol> <p>By following these steps, you can effectively classify survey comments into predefined topics, allowing for a more nuanced analysis of employee experience. If you need more detailed guidance on any specific step, feel free to ask!</p> <p>Here's an example of how you can use Python with the <code>scikit-learn</code> library to classify survey comments into predefined topics, allowing up to three topics per comment. This example uses a multi-label classification approach with a <code>OneVsRestClassifier</code> and <code>LogisticRegression</code>.</p> <pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\n\n# Sample data\ndata = {\n    'comment': [\n        \"I love the flexible work hours.\",\n        \"The management is supportive.\",\n        \"Great work-life balance.\",\n        \"Need better communication from the top.\",\n        \"More opportunities for growth would be nice.\"\n    ],\n    'topics': [\n        [\"Work Hours\", \"Work-Life Balance\"],\n        [\"Management\"],\n        [\"Work-Life Balance\"],\n        [\"Communication\"],\n        [\"Career Growth\"]\n    ]\n}\n\n# Convert to DataFrame\ndf = pd.DataFrame(data)\n\n# Binarize the labels\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer()\ny = mlb.fit_transform(df['topics'])\n\n# Vectorize the comments\nvectorizer = TfidfVectorizer()\nX = vectorizer.fit_transform(df['comment'])\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the model\nmodel = OneVsRestClassifier(LogisticRegression())\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred, target_names=mlb.classes_))\n\n# Example prediction\nnew_comments = [\"I appreciate the career development programs.\"]\nX_new = vectorizer.transform(new_comments)\npredictions = model.predict(X_new)\npredicted_topics = mlb.inverse_transform(predictions)\nprint(predicted_topics)\n</code></pre> <p>This code does the following: 1. Data Preparation: Creates a sample dataset with comments and their associated topics. 2. Label Binarization: Converts the list of topics into a binary format suitable for multi-label classification. 3. Text Vectorization: Uses TF-IDF to convert text comments into numerical features. 4. Model Training: Trains a <code>OneVsRestClassifier</code> with <code>LogisticRegression</code> on the training data. 5. Evaluation: Evaluates the model's performance on the test set and prints a classification report. 6. Prediction: Demonstrates how to predict topics for new comments.</p> <p>You can expand this code to include your full dataset and adjust the model parameters as needed. If you have any specific requirements or need further customization, let me know!</p> <p>To test if the same topic structure works on another kind of survey and to include an \"Other\" topic for themes not covered, you can follow these steps:</p> <ol> <li>Data Collection and Preparation:</li> <li>Collect New Survey Data: Gather responses from the new survey.</li> <li> <p>Clean and Preprocess: Clean the text data similarly to how you did for the original survey.</p> </li> <li> <p>Feature Extraction:</p> </li> <li> <p>Vectorize New Survey Comments: Use the same vectorization method (e.g., TF-IDF) and the same vectorizer instance to transform the new survey comments into numerical features.</p> </li> <li> <p>Model Application:</p> </li> <li>Load Pre-trained Model: Use the model trained on the original survey data.</li> <li> <p>Predict Topics: Apply the model to the new survey data to predict topics.</p> </li> <li> <p>Evaluation:</p> </li> <li>Manual Review: Manually review a sample of the predictions to see if the topics make sense in the context of the new survey.</li> <li> <p>Quantitative Metrics: If you have labeled data for the new survey, calculate metrics like Precision, Recall, and F1-score to evaluate the model's performance.</p> </li> <li> <p>Incorporate \"Other\" Topic:</p> </li> <li>Thresholding: Set a probability threshold for topic assignment. If no topic exceeds the threshold, classify the comment as \"Other\".</li> <li>Manual Labeling: Manually review comments classified as \"Other\" to identify any new themes that might need to be added to the predefined topics.</li> </ol> <p>Here's an example of how you can modify the code to include an \"Other\" topic:</p> <pre><code>import numpy as np\n\n# Function to predict topics with an \"Other\" category\ndef predict_with_other(model, vectorizer, mlb, comments, threshold=0.2):\n    X_new = vectorizer.transform(comments)\n    probabilities = model.predict_proba(X_new)\n    predictions = (probabilities &gt;= threshold).astype(int)\n\n    # If no topic exceeds the threshold, classify as \"Other\"\n    for i in range(predictions.shape[0]):\n        if not np.any(predictions[i]):\n            predictions[i] = np.zeros(predictions.shape[1])\n            predictions[i, -1] = 1  # Assuming the last column is \"Other\"\n\n    return mlb.inverse_transform(predictions)\n\n# Example usage\nnew_comments = [\"I appreciate the career development programs.\", \"The cafeteria food needs improvement.\"]\npredicted_topics = predict_with_other(model, vectorizer, mlb, new_comments)\nprint(predicted_topics)\n</code></pre> <p>In this code: 1. Thresholding: The <code>predict_with_other</code> function applies a threshold to the predicted probabilities. If no topic exceeds the threshold, the comment is classified as \"Other\". 2. Manual Review: You can manually review comments classified as \"Other\" to identify any new themes.</p> <p>By following these steps, you can test the applicability of your topic structure on a new survey and ensure that any new themes are captured effectively. If you need further customization or have specific requirements, feel free to ask!</p> <p>Here's a Python code snippet that classifies survey comments into predefined topics, including \"No Topic\" and \"Other Topic,\" and assigns up to 3 classes per survey comment:</p> <pre><code>from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport numpy as np\n\n# Predefined topics\ntopics = [\n    \"Work Environment\",\n    \"Compensation\",\n    \"Management\",\n    \"Work-Life Balance\",\n    \"Career Growth\",\n    \"Company Culture\",\n    \"Training and Development\",\n    \"Job Security\",\n    \"Benefits\",\n    \"Communication\",\n    \"No Topic\",\n    \"Other Topic\"\n]\n\n# Sample survey comments\nsurvey_comments = [\n    \"I am happy with the work environment and the management is supportive.\",\n    \"The compensation is not competitive and there are no benefits.\",\n    \"I feel there is no work-life balance and the company culture is toxic.\",\n    \"There are no opportunities for career growth and job security is a concern.\",\n    \"The training and development programs are excellent.\",\n]\n\n# Vectorize the topics and survey comments\nvectorizer = TfidfVectorizer()\ntopic_vectors = vectorizer.fit_transform(topics)\ncomment_vectors = vectorizer.transform(survey_comments)\n\n# Function to classify survey comments to topics\ndef classify_comments(comments, topic_vectors, n_classes=3):\n    classifications = []\n\n    for comment_vector in comments:\n        # Calculate cosine similarity between comment and topics\n        similarities = cosine_similarity(comment_vector, topic_vectors).flatten()\n\n        # Get indices of top n_classes topics\n        top_indices = similarities.argsort()[-n_classes:][::-1]\n\n        # Get the corresponding topics\n        top_topics = [topics[i] for i in top_indices]\n\n        classifications.append(top_topics)\n\n    return classifications\n\n# Classify the survey comments\nclassified_comments = classify_comments(comment_vectors, topic_vectors)\n\n# Print the results\nfor i, comment in enumerate(survey_comments):\n    print(f\"Survey Comment: {comment}\")\n    print(f\"Classified Topics: {classified_comments[i]}\")\n    print()\n</code></pre> <p>This code uses TF-IDF vectorization to convert the survey comments and predefined topics into vectors. It then calculates the cosine similarity between each comment and the topics, selecting the top 3 most similar topics for each comment. You can adjust the <code>n_classes</code> parameter to classify a different number of topics per comment.</p> <p>The code classifies a survey comment to \"Other Topic\" based on the cosine similarity between the comment and the predefined topics. If a comment doesn't closely match any of the specific topics, it might have a higher similarity score with \"Other Topic.\"</p> <p>Here's a more detailed explanation:</p> <ol> <li> <p>Vectorization: The <code>TfidfVectorizer</code> converts both the predefined topics and the survey comments into numerical vectors based on the importance of words.</p> </li> <li> <p>Cosine Similarity: For each survey comment, the code calculates the cosine similarity between the comment vector and each topic vector. This measures how similar the comment is to each topic.</p> </li> <li> <p>Top Matches: The code then identifies the top 3 topics with the highest similarity scores for each comment.</p> </li> </ol> <p>If a comment doesn't strongly match any specific topic, the similarity score for \"Other Topic\" might be among the highest, leading to its classification under \"Other Topic.\"</p> <p>For example, if a comment is about something not covered by the predefined topics, it might not have high similarity scores with any of them, but it could still have a relatively higher score with \"Other Topic\" due to the general nature of this category.</p> <p>Would you like to see an example with a specific comment?</p> <p>Got it! To validate if the existing topics will cover a new survey, you can follow these steps:</p> <ol> <li> <p>Analyze Topic Coverage: Check how well the predefined topics match the new survey comments. This involves calculating the similarity between the new comments and the predefined topics.</p> </li> <li> <p>Identify Uncovered Areas: Determine if there are significant portions of the new survey comments that don't fit well into any of the predefined topics. This can be done by looking at the similarity scores and identifying comments with low scores across all topics.</p> </li> <li> <p>Adjust Topics if Necessary: If you find that many comments don't fit well into the existing topics, you may need to adjust your predefined topics or add new ones.</p> </li> </ol> <p>Here's a step-by-step approach with code:</p>"},{"location":"Python/Misc/draft_ml/#step-1-vectorize-new-survey-comments","title":"Step 1: Vectorize New Survey Comments","text":"<pre><code># New survey comments for testing\nnew_survey_comments = [\n    \"The office environment is very comfortable and the management is approachable.\",\n    \"I am not satisfied with the salary and the benefits provided.\",\n    \"There is a lack of communication and the work-life balance is poor.\",\n    \"I appreciate the career growth opportunities and the training programs.\",\n    \"The company culture is inclusive and supportive.\"\n]\n\n# Preprocess and vectorize the new survey comments\nnew_comment_vectors = vectorizer.transform(new_survey_comments)\n</code></pre>"},{"location":"Python/Misc/draft_ml/#step-2-calculate-similarity-scores","title":"Step 2: Calculate Similarity Scores","text":"<pre><code># Calculate similarity scores for new survey comments\ndef calculate_similarity_scores(comments, topic_vectors):\n    similarity_scores = []\n\n    for comment_vector in comments:\n        # Calculate cosine similarity between comment and topics\n        similarities = cosine_similarity(comment_vector, topic_vectors).flatten()\n        similarity_scores.append(similarities)\n\n    return similarity_scores\n\n# Get similarity scores for new survey comments\nsimilarity_scores = calculate_similarity_scores(new_comment_vectors, topic_vectors)\n</code></pre>"},{"location":"Python/Misc/draft_ml/#step-3-analyze-coverage","title":"Step 3: Analyze Coverage","text":"<pre><code># Analyze coverage of predefined topics\ndef analyze_coverage(similarity_scores, threshold=0.2):\n    uncovered_comments = []\n\n    for i, scores in enumerate(similarity_scores):\n        # Check if all similarity scores are below the threshold\n        if all(score &lt; threshold for score in scores):\n            uncovered_comments.append(i)\n\n    return uncovered_comments\n\n# Identify comments that are not well covered by predefined topics\nuncovered_comments = analyze_coverage(similarity_scores)\n\n# Print results\nfor i in uncovered_comments:\n    print(f\"Uncovered Comment: {new_survey_comments[i]}\")\n</code></pre>"},{"location":"Python/Misc/draft_ml/#step-4-adjust-topics-if-necessary","title":"Step 4: Adjust Topics if Necessary","text":"<p>If you find that several comments are not well covered by the existing topics, you might need to:</p> <ul> <li>Add New Topics: Introduce new topics that better capture the themes in the new survey comments.</li> <li>Refine Existing Topics: Adjust the definitions or scope of existing topics to better match the new data.</li> </ul> <p>By following these steps, you can validate whether your existing topics are sufficient for the new survey and make necessary adjustments to improve coverage.</p>"},{"location":"Python/Misc/draft_ml/#step-1-generate-seed-keywords-and-descriptions","title":"Step 1: Generate Seed Keywords and Descriptions","text":"<ol> <li>Work Environment</li> <li>Seed Keywords: office, workspace, environment, conditions, safety</li> <li> <p>Description: Refers to the physical and psychological conditions of the workplace, including safety, comfort, and overall atmosphere.</p> </li> <li> <p>Compensation and Benefits</p> </li> <li>Seed Keywords: salary, benefits, pay, compensation, perks</li> <li> <p>Description: Concerns the financial and non-financial rewards provided to employees, such as salary, bonuses, health insurance, and other perks.</p> </li> <li> <p>Management and Leadership</p> </li> <li>Seed Keywords: management, leadership, supervisor, boss, direction</li> <li> <p>Description: Involves the effectiveness, behavior, and style of managers and leaders within the organization.</p> </li> <li> <p>Career Growth and Development</p> </li> <li>Seed Keywords: career, growth, development, promotion, advancement</li> <li> <p>Description: Focuses on opportunities for professional development, career advancement, and skill enhancement.</p> </li> <li> <p>Work-Life Balance</p> </li> <li>Seed Keywords: work-life, balance, flexibility, hours, personal time</li> <li> <p>Description: Pertains to the balance between work responsibilities and personal life, including flexible working hours and remote work options.</p> </li> <li> <p>Company Culture</p> </li> <li>Seed Keywords: culture, values, mission, environment, inclusivity</li> <li> <p>Description: Relates to the company's values, mission, and overall cultural environment, including inclusivity and diversity.</p> </li> <li> <p>Job Satisfaction</p> </li> <li>Seed Keywords: satisfaction, happiness, job, role, contentment</li> <li> <p>Description: Measures the level of contentment and fulfillment employees feel in their job roles.</p> </li> <li> <p>Communication</p> </li> <li>Seed Keywords: communication, feedback, information, clarity, updates</li> <li> <p>Description: Involves the effectiveness and clarity of communication within the organization, including feedback mechanisms.</p> </li> <li> <p>Training and Development</p> </li> <li>Seed Keywords: training, development, learning, skills, education</li> <li> <p>Description: Concerns the availability and quality of training programs and opportunities for skill development.</p> </li> <li> <p>Job Security</p> <ul> <li>Seed Keywords: security, stability, job, position, layoffs</li> <li>Description: Relates to the stability and security of employees' job positions and the organization's future outlook.</li> </ul> </li> </ol>"},{"location":"Python/Misc/draft_ml/#step-2-create-and-evaluate-the-model","title":"Step 2: Create and Evaluate the Model","text":"<pre><code>import pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nimport numpy as np\n\n# Define topics with their seed keywords and descriptions\ntopics = {\n    \"Work Environment\": [\"office\", \"workspace\", \"environment\", \"conditions\", \"safety\"],\n    \"Compensation and Benefits\": [\"salary\", \"benefits\", \"pay\", \"compensation\", \"perks\"],\n    \"Management and Leadership\": [\"management\", \"leadership\", \"supervisor\", \"boss\", \"direction\"],\n    \"Career Growth and Development\": [\"career\", \"growth\", \"development\", \"promotion\", \"advancement\"],\n    \"Work-Life Balance\": [\"work-life\", \"balance\", \"flexibility\", \"hours\", \"personal time\"],\n    \"Company Culture\": [\"culture\", \"values\", \"mission\", \"environment\", \"inclusivity\"],\n    \"Job Satisfaction\": [\"satisfaction\", \"happiness\", \"job\", \"role\", \"contentment\"],\n    \"Communication\": [\"communication\", \"feedback\", \"information\", \"clarity\", \"updates\"],\n    \"Training and Development\": [\"training\", \"development\", \"learning\", \"skills\", \"education\"],\n    \"Job Security\": [\"security\", \"stability\", \"job\", \"position\", \"layoffs\"],\n    \"Other Topic\": []\n}\n\n# Sample survey comments and their topics\ndata = {\n    'comment': [\n        \"I love the flexible work hours.\",\n        \"The management is supportive.\",\n        \"Great work-life balance.\",\n        \"Need better communication from the top.\",\n        \"More opportunities for growth would be nice.\"\n    ],\n    'topics': [\n        [\"Work-Life Balance\"],\n        [\"Management and Leadership\"],\n        [\"Work-Life Balance\"],\n        [\"Communication\"],\n        [\"Career Growth and Development\"]\n    ]\n}\n\n# Convert to DataFrame\ndf = pd.DataFrame(data)\n\n# Binarize the labels\nfrom sklearn.preprocessing import MultiLabelBinarizer\nmlb = MultiLabelBinarizer(classes=list(topics.keys()))\ny = mlb.fit_transform(df['topics'])\n\n# Vectorize the comments and seed keywords\nvectorizer = TfidfVectorizer()\nall_text = list(df['comment']) + [kw for kws in topics.values() for kw in kws]\nvectorizer.fit(all_text)\nX = vectorizer.transform(df['comment'])\n\n# Split the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Train the model\nmodel = OneVsRestClassifier(LogisticRegression(max_iter=1000))\nmodel.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint(classification_report(y_test, y_pred, target_names=mlb.classes_))\n\n# Function to predict topics with an \"Other\" category\ndef predict_with_other(model, vectorizer, mlb, comments, threshold=0.2):\n    X_new = vectorizer.transform(comments)\n    probabilities = model.predict_proba(X_new)\n    predictions = (probabilities &gt;= threshold).astype(int)\n\n    # If no topic exceeds the threshold, classify as \"Other\"\n    for i in range(predictions.shape[0]):\n        if not np.any(predictions[i]):\n            predictions[i] = np.zeros(predictions.shape[1])\n            predictions[i, -1] = 1  # Assuming the last column is \"Other Topic\"\n\n    return mlb.inverse_transform(predictions)\n\n# Example prediction\nnew_comments = [\"I appreciate the career development programs.\", \"The cafeteria food needs improvement.\"]\npredicted_topics = predict_with_other(model, vectorizer, mlb, new_comments)\nprint(predicted_topics)\n</code></pre>"},{"location":"Python/Misc/draft_ml/#step-3-test-the-model-on-new-survey-data","title":"Step 3: Test the Model on New Survey Data","text":"<pre><code># New survey comments for testing\nnew_survey_comments = [\n    \"The office environment is very comfortable and the management is approachable.\",\n    \"I am not satisfied with the salary and the benefits provided.\",\n    \"There is a lack of communication and the work-life balance is poor.\",\n    \"I appreciate the career growth opportunities and the training programs.\",\n    \"The company culture is inclusive and supportive.\"\n]\n\n# Preprocess and vectorize the new survey comments\nnew_comment_vectors = vectorizer.transform(new_survey_comments)\n\n# Calculate similarity scores for new survey comments\ndef calculate_similarity_scores(comments, topic_vectors):\n    similarity_scores = []\n\n    for comment_vector in comments:\n        similarities = cosine_similarity(comment_vector, topic_vectors).flatten()\n        similarity_scores.append(similarities)\n\n    return similarity_scores\n\n# Get similarity scores for new survey comments\nsimilarity_scores = calculate_similarity_scores(new_comment_vectors, topic_vectors)\n\n# Analyze coverage of predefined topics\ndef analyze_coverage(similarity_scores, threshold=0.2):\n    uncovered_comments = []\n\n    for i, scores in enumerate(similarity_scores):\n        if all(score &lt; threshold for score in scores):\n            uncovered_comments.append(i)\n\n    return uncovered_comments\n\n# Identify comments that are not well covered by predefined topics\nuncovered_comments = analyze_coverage(similarity_scores)\n\n# Print results\nfor i in uncovered_comments:\n    print(f\"Uncovered Comment: {new_survey_comments[i]}\")\n</code></pre>"},{"location":"Python/Misc/draft_ml/#step-4-explain-criteria-for-using-the-model-on-new-data","title":"Step 4: Explain Criteria for Using the Model on New Data","text":"<p>To determine if the model can be used on new data: - Topic Coverage: Ensure most new comments are well-covered by predefined topics. - Performance Metrics: Check precision, recall, and F1-score on new data. - Manual Review: Manually verify the relevance of predicted topics. - Feedback Loop: Continuously improve the model based on feedback from new data.</p> <p>Each row includes seed keywords and a brief description that outlines the essence of each topic.</p>"},{"location":"Python/Misc/draft_ml/#enhanced-table-for-topic-modeling","title":"Enhanced Table for Topic Modeling:","text":"<p>Here is the continuation of the enhanced table after \"Regulatory Requirements\":</p> Primary Topic Sub-Topic Keywords Description Career Career Advancement promotion, raise, growth, leadership role, skills development Opportunities and pathways for employees to advance in their roles and take on higher responsibilities within the organization. Career Career Mobility internal transfer, relocation, role change, job rotation The ability of employees to move between roles, departments, or locations within the organization. Career Career Opportunities potential, open roles, promotion chance, development plan Access to roles and opportunities for employees to progress professionally and achieve their career goals. Career Training and Professional Development learning, upskilling, courses, certifications, mentorship Programs and initiatives to enhance employees' skills, knowledge, and competencies through structured learning. Company Direction &amp; Strategy Business Decisions strategic planning, management decisions, leadership choices Decisions made at the organizational level that influence the company\u2019s direction and goals. Company Direction &amp; Strategy Company Direction vision, mission, long-term goals, organizational strategy The overall strategy and roadmap for achieving the company's mission and vision. Company Direction &amp; Strategy Vision future outlook, long-term goals, inspiration, mission statement The organization\u2019s aspirational goals and its commitment to achieving them. Company Direction &amp; Strategy Senior Leadership executives, C-suite, decision-makers, leadership support The effectiveness and approachability of senior leaders in guiding the company and supporting employees. Diversity, Equity, &amp; Inclusion Diversity, Equity, &amp; Inclusion equal opportunity, representation, fairness, belonging, inclusive culture Efforts to foster an inclusive environment where employees feel valued regardless of their background. Employee Benefits Employee Benefits perks, benefits, wellness programs, incentives The non-salary compensation provided to employees, such as insurance, wellness programs, and additional perks. Employee Benefits Paid Time Off vacation, holidays, PTO, sick leave Time off provided to employees as part of their compensation package. Employee Benefits Pay and Compensation salary, bonus, remuneration, incentive structure The monetary and non-monetary rewards employees receive for their work. Employee Benefits Sick Leave health leave, illness absence, PTO for illness Policies and provisions for employees to take leave for health reasons. Employee Experience and Engagement Communication and Transparency open communication, honesty, clarity, feedback channels The level of openness and clarity in communication between employees and leadership. Employee Experience and Engagement Employee Voice feedback, suggestions, employee opinion, participation Platforms and opportunities for employees to express their views and provide input. Employee Experience and Engagement Low Morale disengagement, dissatisfaction, unhappiness, lack of motivation Signs of low employee satisfaction, which can lead to decreased productivity and retention. Employee Experience and Engagement Negative Work Experience workplace issues, conflicts, unfair treatment Instances where employees have had adverse experiences in the workplace. Employee Experience and Engagement Org Culture values, team dynamics, workplace culture The prevailing attitudes, values, and practices that define the work environment. Employee Experience and Engagement Positive Work Experience teamwork, recognition, collaboration, achievement Instances where employees have had fulfilling and supportive experiences at work. Employee Experience and Engagement Intent to Leave resignation, turnover, quitting, job change Indicators that employees are considering leaving the organization. Employee Experience and Engagement Sense of Value recognition, appreciation, importance The extent to which employees feel valued and recognized for their contributions. Employee Experience and Engagement Team Culture collaboration, teamwork, support, group dynamics The environment and behaviors within teams that impact collaboration and effectiveness. Job Security Job Stability job safety, long-term employment, economic stability The perceived and actual assurance of continued employment. Life Event Internship apprenticeship, training program, temporary position Early-career opportunities for learning and gaining professional experience. Life Event Retirement pension, end of career, retirement benefits Transitioning out of the workforce and the associated support and benefits. Manager or Supervisor Manager Effectiveness leadership skills, decision-making, team support, coaching The ability of managers to effectively lead, support, and guide their teams. No Topic Insufficient Information unclear, incomplete, vague responses Feedback or survey responses that lack enough context or clarity for analysis. Other Topic No Topic Match unrelated, irrelevant, off-topic Feedback or responses that do not align with predefined topics. Performance Management Customer Service client satisfaction, service delivery, client support Employees\u2019 feedback on their roles in delivering excellent customer service. Performance Management Job Expectations performance goals, role clarity, task expectations Clarity and alignment between employees\u2019 roles and organizational goals. Performance Management Recognition awards, rewards, employee of the month Systems and practices for acknowledging employee contributions and achievements. Performance Management Performance Reviews appraisal, feedback, evaluations The process and effectiveness of reviewing and providing feedback on employee performance. Policies &amp; Procedures Non-Compliance policy violation, misconduct, rule-breaking Instances where employees or processes do not align with company policies. Policies &amp; Procedures Red Tape bureaucracy, slow processes, inefficiency Excessive procedures or regulations that impede efficiency and progress. Policies &amp; Procedures Regulatory Requirements compliance, laws, industry standards Adherence to external and internal legal or regulatory requirements. Policies &amp; Procedures Process Changes new policies, updated procedures, workflow changes Modifications to existing processes and their impact on employees and operations. Policies &amp; Procedures Risk Management risk mitigation, safety protocols, hazard prevention Strategies and protocols implemented to manage and mitigate risks within the organization. Policies &amp; Procedures Wells Fargo Policies company policies, guidelines, compliance standards Specific guidelines and policies unique to Wells Fargo or any other employer in context. Talent Acquisition Internal Hiring internal mobility, internal promotion, internal transfers Opportunities for existing employees to fill open positions within the company. Talent Acquisition Onboarding new hire, orientation, training for new employees The process of integrating new hires into the organization, including training and orientation. Talent Acquisition Recruitment Process hiring, interviews, candidate selection The practices and processes involved in attracting and selecting candidates for open roles. Technology Need for Improvement system upgrade, software issues, technology enhancement Areas where technological tools or systems require enhancement to better support employee workflows. Technology Technology Process Improvement automation, digital transformation, streamlined tools Initiatives to improve the efficiency and functionality of technology processes within the organization. Technology Technology IT infrastructure, technical tools, systems General employee feedback on technology use, accessibility, and effectiveness in the workplace. Unfair Work Environment Discrimination bias, inequality, racism, unfair treatment Instances of unfair or unequal treatment based on personal characteristics, such as race, gender, or age. Unfair Work Environment Favoritism unequal opportunity, preferential treatment, nepotism Concerns related to favoritism or biased treatment within teams or departments. Unfair Work Environment Retaliation workplace retaliation, unfair repercussions Situations where employees feel punished for raising concerns or complaints. Unfair Work Environment Toxic Work Environment hostility, workplace conflict, toxic behavior Feedback on negative or harmful work environments characterized by lack of support, bullying, or other issues. Unfair Work Environment Unethical Behavior policy violations, workplace misconduct, ethical breaches Concerns about actions or decisions that violate ethical or professional standards. Wellbeing Medical Related medical leave, health issues, doctor appointments Health-related issues that affect employees\u2019 ability to work, including access to medical support. Wellbeing Mental Health stress, anxiety, depression, burnout Feedback on how workplace practices and policies impact employees\u2019 mental health and wellbeing. Wellbeing Psychological Safety safe space, open dialogue, fear of retaliation The extent to which employees feel safe to speak openly and take risks without fear of negative consequences. Wellbeing Workplace Safety safety measures, hazard prevention, accidents Concerns about physical safety in the workplace, including protocols for preventing accidents and ensuring employee well-being. Work Balance Overworked long hours, excessive workload, unmanageable tasks Instances where employees feel overwhelmed due to excessive work demands. Work Balance Understaffed lack of resources, manpower shortage, team overextension Issues stemming from insufficient staffing to meet workload demands. Work Balance Burn-out exhaustion, mental fatigue, stress Chronic stress and physical or emotional exhaustion from work demands. Work Balance Work Life Balance personal time, family time, flexible hours Feedback on the ability to balance work responsibilities with personal and family needs. Work Location Strategy Location Strategy relocation, hybrid model, office location Policies or strategies related to work locations, including remote work, hybrid work, or office relocations. Work Location Strategy Relocation moving, job transfer, change in location Employee feedback on the impact of relocation policies on their personal and professional lives. Work Location Strategy Working From Home remote work, virtual workplace, home office setup Feedback on the challenges and benefits of working remotely or in hybrid setups."},{"location":"Python/OSRM/Open_Source_Routing_Machine/","title":"Open Source Routing Machine","text":"<pre><code>!pip install polyline geocoder -q\n</code></pre> <pre><code>import logging\nimport requests\nimport json\nimport polyline\nimport folium\nfrom folium.plugins import MeasureControl\nimport geocoder\n\nfrom functools import lru_cache\n\nlogger = logging.getLogger(__name__)\nDEBUG = True\n\n@lru_cache(maxsize=None)\ndef geocode(location):\n    return _geocode(location)\n\ndef _geocode(location):\n    import geocoder\n    g = geocoder.osm(location)\n    return g.latlng\n\n@lru_cache(maxsize=None)\ndef get_route(olat, olng, dlat, dlng):\n    response = _get_route(olat, olng, dlat, dlng)\n    return response\n\ndef _get_route(olat, olng, dlat, dlng):\n    url = f'http://router.project-osrm.org/route/v1/driving/{olng},{olat};{dlng},{dlat}?alternatives=false&amp;amp;steps=false'\n    # logger.debug(url)\n    response = None\n\n    try:\n        logger.debug(f'====== OSRM: {url}')\n        response = requests.get(url, verify=False)\n    except Exception as ex:\n        raise\n\n    # logger.debug(response.text)\n    if response and response.text:\n        response_dict = json.loads(response.text)\n        #possible = pd.DataFrame([{'Distance': (route['distance'] / 1000) *  0.621371 , route['weight_name']: route['weight']} for route in response_dict['routes']])\n        return response_dict\n    else:\n        return None\n\ndef get_routing_map(origin, destination, zoom=5):\n    orig_latlng = geocode(origin)\n    dest_latlng = geocode(destination)\n\n    resp = get_route(orig_latlng[0], orig_latlng[1], dest_latlng[0], dest_latlng[1])\n\n    decoded = polyline.decode(resp[\"routes\"][0]['geometry'])\n    distance = resp[\"routes\"][0]['distance'] * 0.000621371\n    duration = resp[\"routes\"][0]['duration'] / 60\n\n    map2 = folium.Map(location=(orig_latlng[0], orig_latlng[1]), zoom_start=zoom,\n                                    control_scale=True)\n    # map2.add_child(MeasureControl(\n    #     primary_length_unit='miles',\n    #     secondary_length_unit='meters',\n    #     primary_area_unit='acres',\n    #     secondary_area_unit='sqmeters')\n    # )\n\n    folium.PolyLine(locations=decoded, color=\"blue\").add_to(map2)\n\n    print(f\"{duration} minutes\")\n    print(f\"{distance} miles\")\n\n    return map2\n</code></pre> <pre><code>map = get_routing_map('Plymouth, MN', 'San Diego, CA', zoom=5)\nmap\n</code></pre> <pre>\n<code>2086.615 minutes\n1935.3935121279 miles\n</code>\n</pre> Make this Notebook Trusted to load map: File -&gt; Trust Notebook <pre><code>map = get_routing_map('Ragama, Sri Lanka', 'Kandy, Sri Lanka', zoom=6)\nmap\n</code></pre> <pre>\n<code>123.43 minutes\n64.9220226849 miles\n</code>\n</pre> Make this Notebook Trusted to load map: File -&gt; Trust Notebook <pre><code>import geocoder\ng = geocoder.arcgis('Redlands, CA')\ng.json\n</code></pre> <pre>\n<code>{'address': 'Redlands, California',\n 'bbox': {'northeast': [34.12838000000007, -117.10958999999995],\n  'southwest': [33.98238000000007, -117.25558999999994]},\n 'confidence': 2,\n 'lat': 34.05538000000007,\n 'lng': -117.18258999999995,\n 'ok': True,\n 'quality': 'Locality',\n 'raw': {'extent': {'xmax': -117.10958999999995,\n   'xmin': -117.25558999999994,\n   'ymax': 34.12838000000007,\n   'ymin': 33.98238000000007},\n  'feature': {'attributes': {'Addr_Type': 'Locality', 'Score': 100},\n   'geometry': {'x': -117.18258999999995, 'y': 34.05538000000007}},\n  'name': 'Redlands, California'},\n 'score': 100,\n 'status': 'OK'}</code>\n</pre>"},{"location":"Python/OSRM/Open_Source_Routing_Machine/#open-source-routing-machine","title":"Open Source Routing Machine","text":"<p>The Open Source Routing Machine (OSRM) is an open-source router designed for use with data from the OpenStreetMap project.</p> <p>Project OSRM</p>"},{"location":"Python/Pandas/Display_DataFrames_side_by_side/","title":"Display DataFrames side by side","text":"<pre><code>import pandas as pd\nimport numpy as np\nfrom IPython.display import display_html\n</code></pre> <pre><code>def display_side_by_side(*args):\n    html_str = ''\n    for df in args:\n        html_str += df.to_html()\n    display_html(\n        html_str.replace('table','table style=\"display:inline\"'), \n        raw=True\n    )\n</code></pre> <pre><code>df1 = pd.DataFrame({\n    'date': pd.date_range('2021-01-01', '2021-01-07'),\n    'sales': np.random.rand(7)*1000\n})\ndisplay(df1)\n</code></pre> date sales 0 2021-01-01 803.253221 1 2021-01-02 607.459442 2 2021-01-03 185.055407 3 2021-01-04 467.982851 4 2021-01-05 839.568587 5 2021-01-06 183.600942 6 2021-01-07 445.642795 <pre><code>df2 = pd.DataFrame({\n    'date': pd.date_range('2021-02-01', '2021-02-07'),\n    'sales': np.random.rand(7)*1000\n})\ndisplay(df2)\n</code></pre> date sales 0 2021-02-01 797.061620 1 2021-02-02 922.325911 2 2021-02-03 339.492579 3 2021-02-04 951.633029 4 2021-02-05 361.748705 5 2021-02-06 323.575210 6 2021-02-07 761.153420 <pre><code>display_side_by_side(df1, df2)\n</code></pre> date sales 0 2021-01-01 803.253221 1 2021-01-02 607.459442 2 2021-01-03 185.055407 3 2021-01-04 467.982851 4 2021-01-05 839.568587 5 2021-01-06 183.600942 6 2021-01-07 445.642795 date sales 0 2021-02-01 797.061620 1 2021-02-02 922.325911 2 2021-02-03 339.492579 3 2021-02-04 951.633029 4 2021-02-05 361.748705 5 2021-02-06 323.575210 6 2021-02-07 761.153420"},{"location":"Python/Pandas/Display_DataFrames_side_by_side/#display-pandas-dataframes-side-by-side","title":"Display Pandas DataFrames Side by Side","text":""},{"location":"Python/Pandas/Display_DataFrames_side_by_side/#html-function-for-displaying-side-by-side","title":"HTML Function for Displaying Side by Side","text":""},{"location":"Python/Pandas/Display_DataFrames_side_by_side/#create-test-dataframes","title":"Create Test DataFrames","text":""},{"location":"Python/Pandas/Display_DataFrames_side_by_side/#display-side-by-side","title":"Display Side by Side","text":""},{"location":"Python/Pandas/Groupby/","title":"Groupby","text":"<p>The groupby method in pandas allows you to group rows of data together and call aggregate functions.</p> <pre><code>import pandas as pd\n# Create dataframe\ndata = {'Company':['GOOG','GOOG','MSFT','MSFT','FB','FB'],\n       'Person':['Sam','Charlie','Amy','Vanessa','Carl','Sarah'],\n       'Sales':[200,120,340,124,243,350]}\n</code></pre> <pre><code>df = pd.DataFrame(data)\n</code></pre> <pre><code>df\n</code></pre> Company Person Sales 0 GOOG Sam 200 1 GOOG Charlie 120 2 MSFT Amy 340 3 MSFT Vanessa 124 4 FB Carl 243 5 FB Sarah 350 <p>Now you can use the .groupby() method to group rows together based off of a column name.For instance let's group based off of Company. This will create a DataFrameGroupBy object:</p> <pre><code>df.groupby('Company')\n</code></pre> <pre>\n<code>&lt;pandas.core.groupby.generic.DataFrameGroupBy object at 0x7f1336acded0&gt;</code>\n</pre> <p>You can save this object as a new variable:</p> <pre><code>by_comp = df.groupby(\"Company\")\n</code></pre> <p>And then call aggregate methods off the object:</p> <pre><code>by_comp.mean()\n</code></pre> Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 <pre><code>df.groupby('Company').mean()\n</code></pre> Sales Company FB 296.5 GOOG 160.0 MSFT 232.0 <p>More examples of aggregate methods:</p> <pre><code>by_comp.std()\n</code></pre> Sales Company FB 75.660426 GOOG 56.568542 MSFT 152.735065 <pre><code>by_comp.min()\n</code></pre> Person Sales Company FB Carl 243 GOOG Charlie 120 MSFT Amy 124 <pre><code>by_comp.max()\n</code></pre> Person Sales Company FB Sarah 350 GOOG Sam 200 MSFT Vanessa 340 <pre><code>by_comp.count()\n</code></pre> Person Sales Company FB 2 2 GOOG 2 2 MSFT 2 2 <pre><code>by_comp.describe()\n</code></pre> Sales count mean std min 25% 50% 75% max Company FB 2.0 296.5 75.660426 243.0 269.75 296.5 323.25 350.0 GOOG 2.0 160.0 56.568542 120.0 140.00 160.0 180.00 200.0 MSFT 2.0 232.0 152.735065 124.0 178.00 232.0 286.00 340.0 <pre><code>by_comp.describe().transpose()\n</code></pre> Company FB GOOG MSFT Sales count 2.000000 2.000000 2.000000 mean 296.500000 160.000000 232.000000 std 75.660426 56.568542 152.735065 min 243.000000 120.000000 124.000000 25% 269.750000 140.000000 178.000000 50% 296.500000 160.000000 232.000000 75% 323.250000 180.000000 286.000000 max 350.000000 200.000000 340.000000 <pre><code>by_comp.describe().transpose()['GOOG']\n</code></pre> <pre>\n<code>Sales  count      2.000000\n       mean     160.000000\n       std       56.568542\n       min      120.000000\n       25%      140.000000\n       50%      160.000000\n       75%      180.000000\n       max      200.000000\nName: GOOG, dtype: float64</code>\n</pre> <p></p>"},{"location":"Python/Pandas/Groupby/#pandas-groupby","title":"Pandas GroupBy","text":""},{"location":"Python/Pandas/Handle_Null_Values_With_Pandas/","title":"Handle Null Values With Pandas","text":"<pre><code>import seaborn as sns\n\ntitanic = sns.load_dataset('titanic')\ntitanic.head()\n</code></pre> survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone 0 0 3 male 22.0 1 0 7.2500 S Third man True NaN Southampton no False 1 1 1 female 38.0 1 0 71.2833 C First woman False C Cherbourg yes False 2 1 3 female 26.0 0 0 7.9250 S Third woman False NaN Southampton yes True 3 1 1 female 35.0 1 0 53.1000 S First woman False C Southampton yes False 4 0 3 male 35.0 0 0 8.0500 S Third man True NaN Southampton no True <pre><code>titanic.isnull().sum()\n</code></pre> <pre>\n<code>survived         0\npclass           0\nsex              0\nage            177\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64</code>\n</pre> <pre><code>titanic[titanic.isnull().any(axis=1)]\n</code></pre> survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone 0 0 3 male 22.0 1 0 7.2500 S Third man True NaN Southampton no False 2 1 3 female 26.0 0 0 7.9250 S Third woman False NaN Southampton yes True 4 0 3 male 35.0 0 0 8.0500 S Third man True NaN Southampton no True 5 0 3 male NaN 0 0 8.4583 Q Third man True NaN Queenstown no True 7 0 3 male 2.0 3 1 21.0750 S Third child False NaN Southampton no False ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 884 0 3 male 25.0 0 0 7.0500 S Third man True NaN Southampton no True 885 0 3 female 39.0 0 5 29.1250 Q Third woman False NaN Queenstown no False 886 0 2 male 27.0 0 0 13.0000 S Second man True NaN Southampton no True 888 0 3 female NaN 1 2 23.4500 S Third woman False NaN Southampton no False 890 0 3 male 32.0 0 0 7.7500 Q Third man True NaN Queenstown no True <p>709 rows \u00d7 15 columns</p> <pre><code>titanic.age = titanic.age.fillna(titanic.age.mean())\ntitanic\n</code></pre> survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone 0 0 3 male 22.000000 1 0 7.2500 S Third man True NaN Southampton no False 1 1 1 female 38.000000 1 0 71.2833 C First woman False C Cherbourg yes False 2 1 3 female 26.000000 0 0 7.9250 S Third woman False NaN Southampton yes True 3 1 1 female 35.000000 1 0 53.1000 S First woman False C Southampton yes False 4 0 3 male 35.000000 0 0 8.0500 S Third man True NaN Southampton no True ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 886 0 2 male 27.000000 0 0 13.0000 S Second man True NaN Southampton no True 887 1 1 female 19.000000 0 0 30.0000 S First woman False B Southampton yes True 888 0 3 female 29.699118 1 2 23.4500 S Third woman False NaN Southampton no False 889 1 1 male 26.000000 0 0 30.0000 C First man True C Cherbourg yes True 890 0 3 male 32.000000 0 0 7.7500 Q Third man True NaN Queenstown no True <p>891 rows \u00d7 15 columns</p> <pre><code>titanic.isnull().sum()\n</code></pre> <pre>\n<code>survived         0\npclass           0\nsex              0\nage              0\nsibsp            0\nparch            0\nfare             0\nembarked         2\nclass            0\nwho              0\nadult_male       0\ndeck           688\nembark_town      2\nalive            0\nalone            0\ndtype: int64</code>\n</pre> <pre><code>titanic = titanic.dropna(how='any', axis=0)\ntitanic.isnull().sum()\n</code></pre> <pre>\n<code>survived       0\npclass         0\nsex            0\nage            0\nsibsp          0\nparch          0\nfare           0\nembarked       0\nclass          0\nwho            0\nadult_male     0\ndeck           0\nembark_town    0\nalive          0\nalone          0\ndtype: int64</code>\n</pre> <pre><code>titanic\n</code></pre> survived pclass sex age sibsp parch fare embarked class who adult_male deck embark_town alive alone 1 1 1 female 38.0 1 0 71.2833 C First woman False C Cherbourg yes False 3 1 1 female 35.0 1 0 53.1000 S First woman False C Southampton yes False 6 0 1 male 54.0 0 0 51.8625 S First man True E Southampton no True 10 1 3 female 4.0 1 1 16.7000 S Third child False G Southampton yes False 11 1 1 female 58.0 0 0 26.5500 S First woman False C Southampton yes True ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... ... 871 1 1 female 47.0 1 1 52.5542 S First woman False D Southampton yes False 872 0 1 male 33.0 0 0 5.0000 S First man True B Southampton no True 879 1 1 female 56.0 0 1 83.1583 C First woman False C Cherbourg yes False 887 1 1 female 19.0 0 0 30.0000 S First woman False B Southampton yes True 889 1 1 male 26.0 0 0 30.0000 C First man True C Cherbourg yes True <p>201 rows \u00d7 15 columns</p>"},{"location":"Python/Pandas/Handle_Null_Values_With_Pandas/#handle-nulls","title":"Handle Nulls","text":""},{"location":"Python/Pandas/Handle_Null_Values_With_Pandas/#load-titanic-dataset","title":"Load Titanic Dataset","text":""},{"location":"Python/Pandas/Handle_Null_Values_With_Pandas/#count-nulls-for-each-column","title":"Count Nulls For Each Column","text":""},{"location":"Python/Pandas/Handle_Null_Values_With_Pandas/#visualize-rows-with-nulls","title":"Visualize Rows With Nulls","text":""},{"location":"Python/Pandas/Handle_Null_Values_With_Pandas/#impute-replacement-values","title":"Impute Replacement Values","text":"<p>Use <code>mean</code> to replace numeric values. </p>"},{"location":"Python/Pandas/Handle_Null_Values_With_Pandas/#drop-rows-with-nulls","title":"Drop Rows with Nulls","text":""},{"location":"Python/Pandas/MonthBegin_and_MonthEnd/","title":"MonthBegin and MonthEnd","text":"<pre><code>import pandas as pd\nfrom pandas.tseries.offsets import MonthBegin, MonthEnd\n</code></pre> <pre><code>todays_date = '2021-02-07'\nmonth_end_date = pd.to_datetime(todays_date) + MonthEnd(1)\nprint(month_end_date)\n</code></pre> <pre>\n<code>2021-02-28 00:00:00\n</code>\n</pre> <pre><code>month_start_date = pd.to_datetime(todays_date) + MonthBegin(-1)\nprint(month_start_date)\n</code></pre> <pre>\n<code>2021-02-01 00:00:00\n</code>\n</pre> <pre><code>next_month_start_date = pd.to_datetime(todays_date) + MonthBegin(1)\nprint(next_month_start_date)\n</code></pre> <pre>\n<code>2021-03-01 00:00:00\n</code>\n</pre>"},{"location":"Python/Pandas/MonthBegin_and_MonthEnd/#pandas-timeseries-monthbegin-and-monthend","title":"Pandas TimeSeries MonthBegin and MonthEnd","text":""},{"location":"Python/Pandas/MonthBegin_and_MonthEnd/#monthend","title":"MonthEnd","text":"<p>Helpful when figuring out whether month ends on 28th, 29th, 30th, 31st.</p>"},{"location":"Python/Pandas/MonthBegin_and_MonthEnd/#monthbegin","title":"MonthBegin","text":"<p>Helpful when figuring out when current/previous/next MonthBegin.</p>"},{"location":"Python/Pandas/OpenAI_DataFrame_Accessor/","title":"Pandas Accessor for OpenAI Responses","text":"<p>Create a lightweight helper so <code>df.ai.generate_response()</code> can call the OpenAI API for each row in a DataFrame. This pattern is handy when you need to enrich data (summaries, classifications, translations) without scattering API plumbing throughout your notebooks.</p>"},{"location":"Python/Pandas/OpenAI_DataFrame_Accessor/#1-install-and-configure-openai","title":"1. Install and configure OpenAI","text":"<pre><code>pip install openai pandas\nexport OPENAI_API_KEY=\"sk-...\"  # make sure the key is available to the process\n</code></pre> <p>If you prefer to avoid environment variables, you can pass the key explicitly when the OpenAI client is created, but storing it in <code>OPENAI_API_KEY</code> keeps credentials out of your code.</p>"},{"location":"Python/Pandas/OpenAI_DataFrame_Accessor/#2-register-the-accessor","title":"2. Register the accessor","text":"<p>Save the snippet below as <code>ai_accessor.py</code> (or drop it into a utilities module that is imported before first use). It registers a custom accessor named <code>ai</code> on every DataFrame.</p> <pre><code>import time\nfrom typing import Optional\n\nimport pandas as pd\nfrom openai import OpenAI\n\nclient = OpenAI()  # picks up OPENAI_API_KEY automatically\n\n\n@pd.api.extensions.register_dataframe_accessor(\"ai\")\nclass OpenAIFrameAccessor:\n    \"\"\"Adds AI helper methods to DataFrames.\"\"\"\n\n    def __init__(self, pandas_obj: pd.DataFrame):\n        self._obj = pandas_obj\n\n    def generate_response(\n        self,\n        prompt_column: str,\n        model: str = \"gpt-4o-mini\",\n        output_column: str = \"ai_response\",\n        system_prompt: Optional[str] = \"You are a helpful assistant.\",\n        temperature: float = 0.2,\n        overwrite: bool = False,\n        sleep: float = 0.0,\n        max_retries: int = 3,\n        **chat_kwargs,\n    ) -&gt; pd.DataFrame:\n        \"\"\"\n        Call OpenAI for rows with missing output and return the mutated DataFrame.\n\n        Parameters\n        ----------\n        prompt_column: str\n            Column containing the user prompt for OpenAI.\n        model: str\n            Chat model to call.\n        output_column: str\n            Column where responses are stored. Created when missing.\n        system_prompt: Optional[str]\n            Optional system message prepended to each request.\n        temperature: float\n            Passed through to the chat completion API.\n        overwrite: bool\n            Force re-generation even if an answer already exists.\n        sleep: float\n            Optional delay (seconds) between calls for rate-limit headroom.\n        max_retries: int\n            Number of retry attempts before recording the error text.\n        chat_kwargs:\n            Forwarded verbatim to `client.chat.completions.create`.\n        \"\"\"\n        df = self._obj\n\n        if prompt_column not in df.columns:\n            raise KeyError(f\"Column '{prompt_column}' is missing.\")\n\n        if output_column not in df.columns or overwrite:\n            df[output_column] = pd.NA\n\n        pending = df[df[output_column].isna()]\n\n        for idx, prompt in pending[prompt_column].items():\n            messages = [{\"role\": \"user\", \"content\": str(prompt)}]\n            if system_prompt:\n                messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n\n            for attempt in range(1, max_retries + 1):\n                try:\n                    completion = client.chat.completions.create(\n                        model=model,\n                        messages=messages,\n                        temperature=temperature,\n                        **chat_kwargs,\n                    )\n                    df.at[idx, output_column] = (\n                        completion.choices[0].message.content.strip()\n                    )\n                    break\n                except Exception as exc:\n                    if attempt == max_retries:\n                        df.at[idx, output_column] = f\"ERROR: {exc}\"\n                    else:\n                        time.sleep(min(2 ** attempt, 10))\n\n            if sleep:\n                time.sleep(sleep)\n\n        return df\n</code></pre>"},{"location":"Python/Pandas/OpenAI_DataFrame_Accessor/#3-use-the-accessor","title":"3. Use the accessor","text":"<pre><code>import pandas as pd\nfrom ai_accessor import OpenAIFrameAccessor  # noqa: F401 - registers the accessor\n\ndf = pd.DataFrame(\n    {\n        \"id\": [1, 2, 3],\n        \"text\": [\n            \"Explain what a DataFrame accessor does.\",\n            \"Summarize pandas' groupby operation in one sentence.\",\n            \"List three use cases for joining DataFrames.\",\n        ],\n    }\n)\n\ndf.ai.generate_response(\n    prompt_column=\"text\",\n    output_column=\"ai_summary\",\n    model=\"gpt-4o-mini\",\n    temperature=0.0,\n)\n\nprint(df[[\"id\", \"ai_summary\"]])\n</code></pre> <p>The accessor only calls OpenAI for rows where <code>ai_summary</code> is missing, so you can resume partially completed jobs or override responses by setting <code>overwrite=True</code>.</p>"},{"location":"Python/Pandas/OpenAI_DataFrame_Accessor/#4-extra-ideas","title":"4. Extra ideas","text":"<ul> <li>Pass <code>response_format={\"type\": \"json_object\"}</code> through <code>chat_kwargs</code> to coerce structured output that can be parsed into new columns.</li> <li>Move API calls into an async worker or queue if you need true concurrency; the accessor keeps things simple and sequential to avoid rate surprises.</li> <li>Cache results on disk (e.g., Parquet or SQLite) between runs to stay within usage quotas.</li> </ul> <p>This approach keeps all OpenAI plumbing in one place while matching pandas' fluent style, making it easy to iterate on prompt engineering directly from your notebooks.</p>"},{"location":"Python/Pandas/Pandas_Value_Counts/","title":"Pandas Value Counts","text":"<pre><code>import pandas as pd\n</code></pre> <pre><code>data = {\n    'name': ['Jason', 'Molly', 'Tina', 'Jake', 'Amy'], \n    'year': [2012, 2012, 2013, 2014, 2014], \n    'reports': [4, 24, 31, 2, 3]\n    }\ndf = pd.DataFrame(data, index = ['Cochice', 'Pima', 'Santa Cruz', 'Maricopa', 'Yuma'])\ndf\n</code></pre> name year reports Cochice Jason 2012 4 Pima Molly 2012 24 Santa Cruz Tina 2013 31 Maricopa Jake 2014 2 Yuma Amy 2014 3 <pre><code>df.year.value_counts()\n</code></pre> <pre>\n<code>2014    2\n2012    2\n2013    1\nName: year, dtype: int64</code>\n</pre>"},{"location":"Python/Pandas/Pandas_Value_Counts/#pandas-value-counts","title":"Pandas Value Counts","text":""},{"location":"Python/Pandas/Pandas_Value_Counts/#create-an-example-dataframe","title":"Create an Example Dataframe","text":""},{"location":"Python/Pandas/Pandas_Value_Counts/#value-counts","title":"Value Counts","text":""},{"location":"Python/Pandas/Quandl_Timeseries/","title":"Quandl Timeseries","text":"<pre><code>!pip install quandl -q\n</code></pre> <pre><code>import pandas as pd\nimport quandl\nimport matplotlib.pyplot as plt\n%matplotlib inline\n</code></pre> <pre><code>import getpass\n\nKEY = getpass.getpass()\n</code></pre> <pre><code>df = quandl.get(\n    \"FRED/DEXCHUS\", \n    start_date='2014-01-01', \n    end_date='2020-10-01',\n    api_key=KEY\n).rename(columns={'Value':'DEXCHUS'})\ndf.index.name = 'ds'\n</code></pre> <pre><code>df\n</code></pre> DEXCHUS ds 2014-01-02 6.0504 2014-01-03 6.0505 2014-01-06 6.0524 2014-01-07 6.0507 2014-01-08 6.0510 ... ... 2020-09-25 6.8220 2020-09-28 6.8106 2020-09-29 6.8150 2020-09-30 6.7896 2020-10-01 6.7898 <p>1690 rows \u00d7 1 columns</p> <pre><code>df.plot(figsize=(16, 10));\n</code></pre> <pre><code>pd.date_range(\n    start = df.index.min(), \n    end = df.index.max()\n).difference(df.index)\n</code></pre> <pre>\n<code>DatetimeIndex(['2014-01-04', '2014-01-05', '2014-01-11', '2014-01-12',\n               '2014-01-18', '2014-01-19', '2014-01-20', '2014-01-25',\n               '2014-01-26', '2014-02-01',\n               ...\n               '2020-08-30', '2020-09-05', '2020-09-06', '2020-09-07',\n               '2020-09-12', '2020-09-13', '2020-09-19', '2020-09-20',\n               '2020-09-26', '2020-09-27'],\n              dtype='datetime64[ns]', length=775, freq=None)</code>\n</pre> <pre><code>df_ffill = df.reindex(pd.date_range(start=df.index[0], end=df.index[-1])).ffill()\n\n# Check missing values filled '2014-01-04', '2014-01-05'\ndisplay(df_ffill.loc['2014-01-02':'2014-01-06', :])\ndf_ffill.loc['2014-01-02':'2014-01-06', :].plot(figsize=(16, 10));\n</code></pre> DEXCHUS 2014-01-02 6.0504 2014-01-03 6.0505 2014-01-04 6.0505 2014-01-05 6.0505 2014-01-06 6.0524 <pre><code>df_bfill = df.reindex(pd.date_range(start=df.index[0], end=df.index[-1])).bfill()\n\n# Check missing values filled '2014-01-04', '2014-01-05'\ndisplay(df_bfill.loc['2014-01-02':'2014-01-06', :])\ndf_bfill.loc['2014-01-02':'2014-01-06', :].plot(figsize=(16, 10));\n</code></pre> DEXCHUS 2014-01-02 6.0504 2014-01-03 6.0505 2014-01-04 6.0524 2014-01-05 6.0524 2014-01-06 6.0524 <pre><code>df_interpolate = df.resample('D').interpolate(method='time')\n\n# Check missing values filled '2014-01-04', '2014-01-05'\ndisplay(df_interpolate.loc['2014-01-02':'2014-01-06', :])\ndf_interpolate.loc['2014-01-02':'2014-01-06', :].plot(figsize=(16, 10));\n</code></pre> DEXCHUS ds 2014-01-02 6.050400 2014-01-03 6.050500 2014-01-04 6.051133 2014-01-05 6.051767 2014-01-06 6.052400"},{"location":"Python/Pandas/Quandl_Timeseries/#download-quandl-data","title":"Download Quandl Data","text":""},{"location":"Python/Pandas/Quandl_Timeseries/#install-quandl","title":"Install Quandl","text":""},{"location":"Python/Pandas/Quandl_Timeseries/#get-data-and-save-in-pandas-dataframe","title":"Get Data and Save in Pandas DataFrame","text":"<p>If run into rate limits, register at Quandl and provide <code>api_key</code> in the get call.</p>"},{"location":"Python/Pandas/Quandl_Timeseries/#plot-time-series-data","title":"Plot Time Series Data","text":""},{"location":"Python/Pandas/Quandl_Timeseries/#find-missing-data","title":"Find Missing Data","text":""},{"location":"Python/Pandas/Quandl_Timeseries/#fill-missing-data-with-ffill","title":"Fill Missing Data with <code>ffill()</code>","text":""},{"location":"Python/Pandas/Quandl_Timeseries/#fill-missing-data-with-bfill","title":"Fill Missing Data with <code>bfill()</code>","text":""},{"location":"Python/Pandas/Quandl_Timeseries/#fill-missing-data-with-interpolation","title":"Fill Missing Data with Interpolation","text":"<p>Pandas Documentation</p>"},{"location":"Python/streamlit/employee_listening_bubble_network/","title":"Employee Survey Bubble Network (Streamlit)","text":"<p>A replicable \"employee listening\" visualization that turns qualitative survey comments into a bubble network. Each node represents a taxonomy topic, node size encodes comment volume, color tracks average sentiment, and weighted edges highlight co-occurring themes.</p>"},{"location":"Python/streamlit/employee_listening_bubble_network/#included-assets","title":"Included Assets","text":"<ul> <li><code>streamlit_app.py</code> \u2013 Streamlit + NetworkX app for editing data, tuning layout controls, and rendering the network in real time.</li> <li><code>topics_sample.csv</code> \u2013 Topic catalogue with volumes, sentiment scores, and high-level themes.</li> <li><code>edges_sample.csv</code> \u2013 Pairwise topic co-occurrence weights used to draw edges.</li> <li><code>employee_topics_bubble_graph.png</code> \u2013 Example graph output for quick reference in decks.</li> </ul> <p>All files live beside this note so they can be referenced directly by MkDocs or other doc tooling.</p>"},{"location":"Python/streamlit/employee_listening_bubble_network/#sample-data-files","title":"Sample Data Files","text":""},{"location":"Python/streamlit/employee_listening_bubble_network/#topics_samplecsv","title":"topics_sample.csv","text":"<pre><code>topic,volume,sentiment,theme\nCareer Growth,820,0.1,Development\nPay &amp; Benefits,990,-0.35,Rewards\nWork-Life Balance,620,-0.05,Wellbeing\nLeadership &amp; Direction,720,-0.1,Strategy\nCompany Strategy,450,0.05,Strategy\nRecognition,510,0.2,Culture\nManager Effectiveness,760,-0.15,Leadership\nLearning &amp; Development,580,0.3,Development\nBelonging &amp; Inclusion,390,0.25,Culture\nProductivity &amp; Efficiency,640,0.05,Operations\nTeamwork &amp; Collaboration,700,0.15,Culture\nCustomer Focus,300,0.35,Operations\n</code></pre>"},{"location":"Python/streamlit/employee_listening_bubble_network/#edges_samplecsv","title":"edges_sample.csv","text":"<pre><code>source,target,weight\nCareer Growth,Learning &amp; Development,210\nCareer Growth,Manager Effectiveness,180\nPay &amp; Benefits,Recognition,130\nPay &amp; Benefits,Work-Life Balance,160\nWork-Life Balance,Manager Effectiveness,190\nLeadership &amp; Direction,Company Strategy,220\nLeadership &amp; Direction,Teamwork &amp; Collaboration,95\nTeamwork &amp; Collaboration,Belonging &amp; Inclusion,140\nProductivity &amp; Efficiency,Teamwork &amp; Collaboration,110\nProductivity &amp; Efficiency,Customer Focus,90\nRecognition,Belonging &amp; Inclusion,75\nManager Effectiveness,Recognition,85\n</code></pre>"},{"location":"Python/streamlit/employee_listening_bubble_network/#streamlit-application","title":"Streamlit Application","text":"<pre><code>import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport networkx as nx\nfrom matplotlib.colors import LinearSegmentedColormap, Normalize\nfrom matplotlib.cm import ScalarMappable\n\nimport streamlit as st\n\nst.set_page_config(page_title=\"Employee Topic Bubble Network\", layout=\"wide\")\nst.title(\"Employee Survey \u2013 Topic Bubble Network\")\n\nst.markdown(\"Upload or edit the CSVs below to change taxonomy, volumes, sentiments, and co-occurrence strengths.\")\n\n# Load data\ntopics_df = pd.read_csv(\"topics_sample.csv\")\nedges_df = pd.read_csv(\"edges_sample.csv\")\n\n# Sidebar controls\nst.sidebar.header(\"Controls\")\nmin_area = st.sidebar.slider(\"Min bubble area\", 200, 3000, 800, 50)\nmax_area = st.sidebar.slider(\"Max bubble area\", 3000, 15000, 8000, 50)\nmin_w = st.sidebar.slider(\"Min edge width\", 0.1, 5.0, 0.5, 0.1)\nmax_w = st.sidebar.slider(\"Max edge width\", 2.0, 20.0, 8.0, 0.5)\nseed = st.sidebar.number_input(\"Layout seed\", value=42, step=1)\n\n# Enable editing in-place (Streamlit's data editor)\nst.subheader(\"Topics\")\ntopics_df = st.data_editor(topics_df, num_rows=\"dynamic\", use_container_width=True)\nst.subheader(\"Edges\")\nedges_df = st.data_editor(edges_df, num_rows=\"dynamic\", use_container_width=True)\n\n# Build graph\nG = nx.Graph()\nfor _, r in topics_df.iterrows():\n    G.add_node(r[\"topic\"], volume=float(r[\"volume\"]), sentiment=float(r[\"sentiment\"]), theme=str(r.get(\"theme\", \"\")))\nfor _, r in edges_df.iterrows():\n    if r[\"source\"] in G.nodes and r[\"target\"] in G.nodes:\n        G.add_edge(r[\"source\"], r[\"target\"], weight=float(r[\"weight\"]))\n\npos = nx.spring_layout(G, seed=int(seed), k=None)\n\n# Scale encodings\nvolumes = np.array([G.nodes[n][\"volume\"] for n in G.nodes])\nvmin, vmax = volumes.min() if len(volumes) else 0, volumes.max() if len(volumes) else 1\nareas = min_area + (volumes - vmin) / (vmax - vmin + 1e-9) * (max_area - min_area)\n\nsentiments = np.array([G.nodes[n][\"sentiment\"] for n in G.nodes])\ncolors_neg = (0.80, 0.13, 0.13)\ncolors_neu = (0.70, 0.70, 0.70)\ncolors_pos = (0.10, 0.60, 0.20)\ncmap = LinearSegmentedColormap.from_list(\"sentiment_map\", [colors_neg, colors_neu, colors_pos])\nnorm = Normalize(vmin=-1, vmax=1)\n\nedge_weights = np.array([G.edges[e][\"weight\"] for e in G.edges]) if len(G.edges) else np.array([0.0])\new_min, ew_max = edge_weights.min(), edge_weights.max()\nedge_widths = min_w + (edge_weights - ew_min) / (ew_max - ew_min + 1e-9) * (max_w - min_w)\n\n# Plot\nfig = plt.figure(figsize=(12, 9))\n\n# Edges\nif len(G.edges) &gt; 0:\n    for (edge, w) in zip(G.edges, edge_widths):\n        n1, n2 = edge\n        x1, y1 = pos[n1]\n        x2, y2 = pos[n2]\n        plt.plot([x1, x2], [y1, y2], linewidth=w, alpha=0.35)\n\n# Nodes\nnode_xy = np.array([pos[n] for n in G.nodes])\nplt.scatter(node_xy[:, 0], node_xy[:, 1], s=areas, c=sentiments, cmap=cmap, norm=norm, alpha=0.95, edgecolors=\"white\", linewidths=1.5)\n\n# Labels\nmin_area_local, max_area_local = min_area, max_area\nfor (n, (x, y), a) in zip(G.nodes, node_xy, areas):\n    fs = 6 + (a - min_area_local) / (max_area_local - min_area_local + 1e-9) * 8\n    label = n\n    if len(label) &gt; 18 and \" \" in label:\n        parts = label.split(\" \")\n        mid = len(parts) // 2\n        label = \" \".join(parts[:mid]) + \"\\n\" + \" \".join(parts[mid:])\n    plt.text(x, y, label, ha=\"center\", va=\"center\", fontsize=fs, weight=\"bold\")\n\nplt.title(\"Employee Survey Comments \u2013 Topic Bubble Network\")\nplt.axis(\"off\")\n\nsm = ScalarMappable(cmap=cmap, norm=norm)\nsm.set_array([])\ncb = plt.colorbar(sm, fraction=0.035, pad=0.02)\ncb.set_label(\"Sentiment (\u22121 to +1)\")\n\nst.pyplot(fig)\n\n# Download updated CSVs\nst.download_button(\"Download topics CSV\", data=topics_df.to_csv(index=False), file_name=\"topics_updated.csv\", mime=\"text/csv\")\nst.download_button(\"Download edges CSV\", data=edges_df.to_csv(index=False), file_name=\"edges_updated.csv\", mime=\"text/csv\")\n</code></pre>"},{"location":"Python/streamlit/employee_listening_bubble_network/#running-locally","title":"Running Locally","text":"<pre><code>pip install streamlit pandas numpy matplotlib networkx\nstreamlit run streamlit_app.py\n</code></pre> <p>Adjust bubble/edge scaling in the sidebar while editing the CSV tables inline, then use the download buttons to export cleaned datasets.</p>"},{"location":"Python/streamlit/employee_listening_bubble_network/#sample-bubble-graph","title":"Sample Bubble Graph","text":""}]}